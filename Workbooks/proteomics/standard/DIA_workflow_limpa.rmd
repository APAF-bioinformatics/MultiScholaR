---
title: "DIANN Analysis for xyz"
version: "1.0"
author: "Your_fancy_self"
output:
  html_document:
    code_folding: true
    self_contained: true
    toc: false
    warning: false
    message: false
---
# Initial R environment setup  
## Checks your R environment for the required packages to run MultiScholaR, and installs them if they are not.

### Further Reading
#### Understanding Package Management in R
- [CRAN (The Comprehensive R Archive Network)](https://cran.r-project.org/) - The main repository for R packages
- [Bioconductor](https://bioconductor.org/) - Repository specialized in bioinformatics packages
- [R Package Installation Guide](https://www.datacamp.com/community/tutorials/r-packages-guide) - A comprehensive guide to installing packages in R
- [devtools Documentation](https://devtools.r-lib.org/) - Learn about the devtools package for installing from GitHub

#### Important Packages Used in this Workflow
- [clusterProfiler](https://bioconductor.org/packages/release/bioc/html/clusterProfiler.html) - Used for gene ontology and pathway enrichment analysis
- [GO.db](https://bioconductor.org/packages/release/data/annotation/html/GO.db.html) - Gene Ontology database annotations
- [UniProt.ws](https://bioconductor.org/packages/release/bioc/html/UniProt.ws.html) - Interface to the UniProt web services
- [mixOmics](https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html) - Methods for multivariate analysis of biological data

#### About MultiScholaR
- [MultiScholaR GitHub](https://github.com/APAF-BIOINFORMATICS/MultiScholaR) - The main repository for the package
- [APAF Bioinformatics](https://www.mq.edu.au/research/research-centres-groups-and-facilities/facilities/macquarie-analytical-and-fabrication-facility/australian-proteome-analysis-facility) - Australian Proteome Analysis Facility bioinformatics resources

This function checks for and installs all the required packages for the MultiScholaR workflow. It installs packages from CRAN, Bioconductor, and GitHub as needed. The function is designed to make setup easy for users who may not be familiar with R package management.

```{r}
# Standalone test script to fetch GitHub repo info without requiring a token.
# This mimics the logic in your helper function but allows unauthenticated requests
# for the public MultiScholaR repo.

# Install 'gh' if not already installed
if (!requireNamespace("gh", quietly = TRUE)) {
  install.packages("gh")
}
library(gh)

# Function to fetch git info (without token check)
fetch_git_info <- function() {
  tryCatch({
    # Make the API call (works for public repos without token)
    branch_info <- gh::gh("/repos/APAF-BIOINFORMATICS/MultiScholaR/branches/main")
    
    list(
      commit_sha = branch_info$commit$sha,
      branch = "main",
      repo = "MultiScholaR",
      timestamp = branch_info$commit$commit$author$date
    )
  }, error = function(e) {
    # Print error for debugging (e.g., rate limit or network issues)
    message("Error fetching GitHub info: ", e$message)
    list(
      commit_sha = NA_character_,
      branch = NA_character_,
      repo = NA_character_,
      timestamp = NA_character_
    )
  })
}

# Run the fetch and print the output
git_info <- fetch_git_info()

cat("Git Information:\n")
cat("---------------\n")
cat("Repository: ", ifelse(is.na(git_info$repo), "N/A", git_info$repo), "\n")
cat("Branch: ", ifelse(is.na(git_info$branch), "N/A", git_info$branch), "\n")
cat("Commit: ", ifelse(is.na(git_info$commit_sha), "N/A", git_info$commit_sha), "\n")
cat("Git Timestamp: ", ifelse(is.na(git_info$timestamp), "N/A", git_info$timestamp), "\n")
```

```{r}
# Standalone test script to query local installation details of the MultiScholaR package.
# Extracts git commit, repo, branch/ref, and other metadata if available (e.g., from devtools/remotes installation).

# Ensure required packages are loaded (base R utils is sufficient, but we'll check for installation)
if (!requireNamespace("utils", quietly = TRUE)) {
  stop("This script requires the 'utils' package (part of base R).")
}

# Function to fetch local package info
fetch_local_package_info <- function(pkg_name = "MultiScholaR") {
  tryCatch({
    # Get the package DESCRIPTION as a list
    desc <- utils::packageDescription(pkg_name)
    
    # Extract relevant fields (these are added by devtools/remotes for GitHub installs)
    repo <- desc$GithubRepo %||% desc$RemoteRepo %||% NA_character_
    username <- desc$GithubUsername %||% desc$RemoteUsername %||% NA_character_
    branch <- desc$GithubRef %||% desc$RemoteRef %||% NA_character_
    commit_sha <- desc$GithubSHA1 %||% desc$RemoteSha %||% NA_character_
    install_date <- desc$Packaged %||% desc$Built %||% desc$Date %||% NA_character_  # Fallback to any date-like field
    
    # If repo and username are available, construct full repo name
    full_repo <- if (!is.na(repo) && !is.na(username)) {
      paste(username, repo, sep = "/")
    } else {
      repo
    }
    
    list(
      repo = full_repo,
      branch = branch,
      commit_sha = commit_sha,
      timestamp = install_date
    )
  }, error = function(e) {
    # Print error for debugging (e.g., package not installed)
    message("Error fetching local package info: ", e$message)
    message("Note: Ensure 'MultiScholaR' is installed, ideally via devtools::install_github() for git metadata.")
    list(
      repo = NA_character_,
      branch = NA_character_,
      commit_sha = NA_character_,
      timestamp = NA_character_
    )
  })
}

# Run the fetch and print the output
pkg_info <- fetch_local_package_info()

cat("Local Package Information (MultiScholaR):\n")
cat("----------------------------------------\n")
cat("Repository: ", ifelse(is.na(pkg_info$repo), "N/A", pkg_info$repo), "\n")
cat("Branch/Ref: ", ifelse(is.na(pkg_info$branch), "N/A", pkg_info$branch), "\n")
cat("Commit: ", ifelse(is.na(pkg_info$commit_sha), "N/A", pkg_info$commit_sha), "\n")
cat("Install Timestamp: ", ifelse(is.na(pkg_info$timestamp), "N/A", pkg_info$timestamp), "\n")
```

## IF THIS IS YOUR FIRST INSTALL, THIS WILL TAKE SOME TIME AS THERE ARE A NUMBER OF DEPENDENCIES TO INSTALL :)
## GOOD POINT TO GRAB A COFFEE!
```{r 1 MultiScholaR FIRST INSTALL, message=TRUE, warning=TRUE}
#install.packages("devtools")
rm(list = c("loadDependencies", "setupDirectories", "RunApplet"))
#remove.packages("MultiScholaR")
#install.packages("devtools")
devtools::load_all("C:/Users/willk/OneDrive - Macquarie University/Projects/APAF Software/MultiScholaR")
loadDependencies()
#MultiScholaRapp()
```



# START HERE if you already have MultiScholaR installed

### Further Reading
#### R Library Management
- [Introduction to R Libraries](https://www.datacamp.com/community/tutorials/r-packages-guide) - Understanding how R libraries work
- [R Package Documentation](https://www.rdocumentation.org/) - Search for documentation of R packages
- [The R Packages Book](https://r-pkgs.org/) - In-depth explanation of R packages by Hadley Wickham and Jennifer Bryan

#### Dependency Management in R
- [Understanding R Package Dependencies](https://www.r-bloggers.com/2023/05/dependency-management/) - Best practices for managing dependencies
- [Packrat and renv](https://rstudio.github.io/renv/articles/renv.html) - Tools for reproducible environments in R

#### Reproducible Research
- [Reproducible Research with R](https://cran.r-project.org/web/views/ReproducibleResearch.html) - CRAN task view on reproducible research
- [R Markdown](https://rmarkdown.rstudio.com/) - Framework for reproducible reports in R

When you already have MultiScholaR installed (and you don't want to check for updates), you can start from this point. The code below loads the package and all its dependencies to prepare your environment for analysis.
```{r 2 Load MultiScholaR}
library(MultiScholaR)
loadDependencies()
```

# Set up your environment and project directory

### Further Reading
#### Project Organization & File Management
- [Project Management in R](https://swcarpentry.github.io/r-novice-gapminder/02-project-intro.html) - Best practices for organizing project files
- [here package](https://here.r-lib.org/) - A popular R package for project-relative file paths
- [File Organization in R Projects](https://www.tidyverse.org/blog/2017/12/workflow-vs-script/) - Workflow vs script-based approaches
- [renv for Reproducible Environments](https://rstudio.github.io/renv/) - Package for project isolation and reproducibility

#### Data Organization Principles
- [Tidy Data Principles](https://www.jstatsoft.org/article/view/v059i10) - Hadley Wickham's paper on tidy data structure
- [Good enough practices in scientific computing](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510) - Practical recommendations for data management

#### Proteomics Project Structure
- [Guidelines for Reporting Mass Spectrometry Data](https://www.mcponline.org/article/S1535-9476(20)31288-3/fulltext) - Standards for proteomics project organization
- [ProteomeXchange](http://www.proteomexchange.org/) - Repository standards for proteomics data

This section establishes the directory structure for your project. Setting up a consistent directory structure is crucial for managing the various files generated throughout the analysis, including input data, intermediate results, and final outputs. MultiScholaR creates a standardized structure to help organize your work.
```{r 3 Project Environment Management}
# Directory Management
## Set up the project directory structure
## This section sets up the project directory structure for MultiScholaR
## Directory management can be challenging, particularly when managing objects
## across multiple chunks within a single R Markdown document.
experiment_label <- "TESTING"
omic_type <- "proteomics" # Set this to the type of analysis you are doing eg "proteomics", "metabolomics", "transcriptomics"
# Setup for the central pillars of molecular biology
project_dirs <- setupDirectories(
    #omic_types = "metabolomics"
    # Or: 
    omic_types = c("proteomics", "metabolomics", "transcriptomics", "lipidomics", "integration"),
    label = experiment_label,
    force = FALSE # Set to TRUE to skip prompts if dirs exist
)
```

# At this step, please copy your data, fasta file and other data necessary into 
# the appropriate directories

### Further Reading
#### Proteomics Data Formats
- [DIA-NN Documentation](https://github.com/vdemichev/DiaNN) - The software that generates the report.tsv input file
- [FASTA Format Description](https://www.ncbi.nlm.nih.gov/genbank/fastaformat/) - Explanation of the FASTA file format
- [UniProt Database](https://www.uniprot.org/) - Source for protein sequence databases and FASTA files
- [Proteomics Standards Initiative](https://www.psidev.info/) - Standards for proteomics data formats

#### Data Preprocessing in Proteomics
- [mzTab Format](https://www.psidev.info/mztab) - Standard format for reporting MS proteomics results
- [Introduction to Mass Spectrometry Data Analysis](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/lcms-dataprocessing/tutorial.html) - Overview of MS data processing
- [DIA Data Analysis Overview](https://www.sciencedirect.com/science/article/pii/S1535947624000902) - Review of DIA analysis methods

#### Configuration Files in R
- [INI Files in R](https://cran.r-project.org/web/packages/ini/index.html) - Working with INI configuration files
- [configr Package](https://cran.r-project.org/web/packages/configr/vignettes/configr.html) - Advanced configuration management

#### Finding Taxonomy IDs
- [NCBI Taxonomy Browser](https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi) - Look up taxonomic IDs for different organisms
- [UniProt Taxonomy](https://www.uniprot.org/taxonomy) - Alternative source for organism taxonomy information

#### UniProt ID Mapping
- [UniProt ID Mapping](https://www.uniprot.org/id-mapping) - UniProt ID mapping service

This section handles importing your DIA-NN results and FASTA file, and setting up essential parameters like taxonomy ID that will be used throughout the analysis. Make sure to download your organism's FASTA file from UniProt and find its correct taxonomy ID before proceeding.
```{r 4 Data Management}
## Input Parameters for Quality Control
## Parameters in this section are experiment-specific. Their default parameters
## are intended as a guide only - every source of variance is different just as
## every set of proteins going through a mass spectrometer is different!
## One size does not fit all and you *will* most likely need to fine tune these
## to get the most out of your data.
config_list <- readConfigFile(file = file.path(project_dirs$proteomics$base_dir, "config.ini"))

# Annotation Management
## Please download the organism fasta file from UniProt. If UniProt is not
## available, the program will extract the relevant identifiers from the fasta
## provided and attempt to match them to user supplied UniProt / UniParc
## conversions
## Please set the name of your fasta file here in the root directory if you
## already have it
#if (!require("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")

#BiocManager::install("limpa")
#library("limpa")
#install.packages("arrow")
#library("arrow")
data_tbl_parquet_filt <- limpa::readDIANN(file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$data_dir, "report.parquet"), format="parquet", q.columns = c("Q.Value","Lib.Q.Value","Lib.PG.Q.Value"), q.cutoffs = 0.01)

str(data_tbl_parquet_filt)

df_intensities_wide <- as.data.frame(data_tbl_parquet_filt$E) |>
  rownames_to_column(var = "PrecursorID") # Naming it 'PrecursorID' for clarity

df_intensities_long <- df_intensities_wide %>%
  pivot_longer(
    cols = -PrecursorID, # Select all columns except PrecursorID (i.e., all sample columns)
    names_to = "Run",    # New column for the sample/run names (e.g., "031716 iMN_5 ...")
    values_to = "Log2Intensity" # New column for the log2 intensity values
  )
data_genes <- as.data.frame(data_tbl_parquet_filt$genes) |>
  rownames_to_column(var = "PrecursorID") # Naming it 'PrecursorID' for clarity

final_integrated_df_wide_join <- data_genes %>%
  left_join(df_intensities_wide, by = "PrecursorID")

str(data_tbl)
str(final_integrated_df_wide_join)
```

```{r}
# Convert limpa EList object to standard DIA-NN format
formatDIANN <- function(data_tbl_parquet_filt) {
  library(dplyr)
  library(tidyr)
  
  # Extract intensity matrix and gene annotations
  intensity_matrix <- data_tbl_parquet_filt$E
  gene_info <- data_tbl_parquet_filt$genes
  
  # Convert intensity matrix to data frame with precursor IDs
  intensity_df <- as.data.frame(intensity_matrix) %>%
    rownames_to_column(var = "Precursor.Id")
  
  # Convert to long format
  intensity_long <- intensity_df %>%
    pivot_longer(
      cols = -Precursor.Id,
      names_to = "Run",
      values_to = "Log2Intensity"
    ) %>%
    filter(!is.na(Log2Intensity))  # Remove missing values
  
  # Join with gene information
  data_tbl_converted <- intensity_long %>%
    left_join(gene_info, by = "Precursor.Id") %>%
    mutate(
      # Basic required columns
      File.Name = paste0(Run, ".raw"),
      Protein.Ids = Protein.Group,  # Use Protein.Group as Protein.Ids
      
      # Extract sequence and charge from Precursor.Id
      Stripped.Sequence = gsub("\\d+$", "", Precursor.Id),  # Remove charge number
      Modified.Sequence = Stripped.Sequence,  # Assume no modifications shown
      Precursor.Charge = as.numeric(gsub(".*?(\\d+)$", "\\1", Precursor.Id)),  # Extract charge
      
      # Convert log2 intensities back to linear scale
      Precursor.Quantity = 2^Log2Intensity,
      Precursor.Normalised = 2^Log2Intensity,
      
      # Set reasonable defaults for quality metrics (since data was pre-filtered)
      Q.Value = 0.001,
      PEP = 0.001,
      Global.Q.Value = 0.001,
      Protein.Q.Value = 0.01,
      PG.Q.Value = 0.001,
      Global.PG.Q.Value = 0.001,
      GG.Q.Value = 0.001,
      Translated.Q.Value = 0,
      Lib.Q.Value = 0.001,
      Lib.PG.Q.Value = 0.001,
      
      # Protein-level quantities (same as precursor for now)
      PG.Quantity = Precursor.Quantity,
      PG.Normalised = Precursor.Normalised,
      PG.MaxLFQ = Precursor.Normalised,
      Genes.Quantity = Precursor.Quantity,
      Genes.Normalised = Precursor.Normalised,
      Genes.MaxLFQ = Precursor.Normalised,
      Genes.MaxLFQ.Unique = Precursor.Normalised,
      
      # Quality and technical columns (set defaults)
      Quantity.Quality = 1.0,
      RT = NA_real_,
      RT.Start = NA_real_,
      RT.Stop = NA_real_,
      iRT = NA_real_,
      Predicted.RT = NA_real_,
      Predicted.iRT = NA_real_,
      First.Protein.Description = "",
      Ms1.Profile.Corr = NA_real_,
      Ms1.Area = NA_real_,
      Ms1.Normalised = NA_real_,
      Normalisation.Factor = 1.0,
      Evidence = 1.0,
      Spectrum.Similarity = NA_real_,
      Averagine = NA_real_,
      Mass.Evidence = NA_real_,
      CScore = 1.0,
      Fragment.Quant.Raw = "",
      Fragment.Correlations = "",
      MS2.Scan = NA_real_,
      IM = 0,
      iIM = 0,
      Predicted.IM = 0,
      Predicted.iIM = 0
    ) %>%
    # Select columns in typical DIA-NN order
    dplyr::select(File.Name, Run, Protein.Group, Protein.Ids, Protein.Names, Genes,
           PG.Quantity, PG.Normalised, PG.MaxLFQ,
           Genes.Quantity, Genes.Normalised, Genes.MaxLFQ, Genes.MaxLFQ.Unique,
           Modified.Sequence, Stripped.Sequence, Precursor.Id, Precursor.Charge,
           Q.Value, PEP, Global.Q.Value, Protein.Q.Value, PG.Q.Value,
           Global.PG.Q.Value, GG.Q.Value, Translated.Q.Value, Proteotypic,
           Precursor.Quantity, Precursor.Normalised, Quantity.Quality,
           RT, RT.Start, RT.Stop, iRT, Predicted.RT, Predicted.iRT,
           First.Protein.Description, Lib.Q.Value, Lib.PG.Q.Value,
           Ms1.Profile.Corr, Ms1.Area, Ms1.Normalised, Normalisation.Factor,
           Evidence, Spectrum.Similarity, Averagine, Mass.Evidence, CScore,
           Fragment.Quant.Raw, Fragment.Correlations, MS2.Scan,
           IM, iIM, Predicted.IM, Predicted.iIM)
  
  return(data_tbl_converted)
}

# Convert your EList to DIA-NN format
data_tbl <- formatDIANN(data_tbl_parquet_filt)

# Check the result
str(data_tbl)
```

```{r}
#DIANN_filename <- "report.tsv" ## copy to /data/proteomics in your project directory
fasta_filename <- "CanonicalIsoform uniprotkb_taxonomy_id_9606_AND_reviewed_2025_05_06.fasta" ## copy to /data/UniProt in your project directory
uniprot_search_results <- NULL ## copy to /data/UniProt in your project directory eg "idmapping.tsv"
uniparc_search_results <- NULL ## copy to /data/UniProt in your project directory eg "idmapping.tsv"
## Please supply your organism's taxon ID here
taxon_id <- 9606 # You can find this at the links above.
## Please supply your organism's name here
organism_name <- "Homo sapiens" # If you don't know this by now, you have bigger problems than this workflow!

#data_tbl <- vroom::vroom(file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$data_dir, DIANN_filename))
fasta_file_path <-file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$uniprot_annotation_dir, fasta_filename)

config_list[["globalParameters"]][["fasta_file"]] <- fasta_filename
config_list[["globalParameters"]][["peptides_input_file"]] <- DIANN_filename

# Load search results if files exist
if (!is.null(uniprot_search_results) && !is.null(uniparc_search_results)) {
  uniprot_search_results <- vroom::vroom(
    file.path(project_dirs$proteomics$uniprot_annotation_dir, uniprot_search_results)
  )
  uniparc_search_results <- vroom::vroom(
    file.path(project_dirs$proteomics$uniprot_annotation_dir, uniparc_search_results)
  )
  }
```

```{r}
## Input Parameters for Quality Control
## Parameters in this section are experiment-specific. Their default parameters
## are intended as a guide only - every source of variance is different just as
## every set of proteins going through a mass spectrometer is different!
## One size does not fit all and you *will* most likely need to fine tune these
## to get the most out of your data.
config_list <- readConfigFile(file = file.path(project_dirs$proteomics$base_dir, "config.ini"))

# Annotation Management
## Please download the organism fasta file from UniProt. If UniProt is not
## available, the program will extract the relevant identifiers from the fasta
## provided and attempt to match them to user supplied UniProt / UniParc
## conversions
## Please set the name of your fasta file here in the root directory if you
## already have it
DIANN_filename <- "cotton_report.tsv" ## copy to /data/proteomics in your project directory
fasta_filename <- "uniprotkb_proteome_UP000189702_2024_10_29.fasta" ## copy to /data/UniProt in your project directory
uniprot_search_results <- NULL ## copy to /data/UniProt in your project directory eg "idmapping.tsv"
uniparc_search_results <- NULL ## copy to /data/UniProt in your project directory eg "idmapping.tsv"
## Please supply your organism's taxon ID here
taxon_id <- 3635 # You can find this at the links above.
## Please supply your organism's name here
organism_name <- "Gossypium hirsutum (Upland cotton)" # If you don't know this by now, you have bigger problems than this workflow!

data_tbl <- vroom::vroom(file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$data_dir, DIANN_filename))
fasta_file_path <-file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$uniprot_annotation_dir, fasta_filename)

config_list[["globalParameters"]][["fasta_file"]] <- fasta_filename
config_list[["globalParameters"]][["peptides_input_file"]] <- DIANN_filename

# Load search results if files exist
if (!is.null(uniprot_search_results) && !is.null(uniparc_search_results)) {
  uniprot_search_results <- vroom::vroom(
    file.path(project_dirs$proteomics$uniprot_annotation_dir, uniprot_search_results)
  )
  uniparc_search_results <- vroom::vroom(
    file.path(project_dirs$proteomics$uniprot_annotation_dir, uniparc_search_results)
  )
  }
```

```{r 7 Protein ID Conversion and Uniprot Lookup}
fasta_meta_file <- "parsed_fasta_data.rds"
aa_seq_tbl_final <- processFastaFile(
  fasta_file_path,
  uniprot_search_results,
  uniparc_search_results,
  fasta_meta_file,
  organism_name
)
data_tbl <- updateProteinIDs(data_tbl, aa_seq_tbl_final)
uniprot_dat_cln <- getUniprotAnnotationsFull(
            data_tbl = data_tbl,
            protein_id_column = "Protein.Ids",
            cache_dir = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$uniprot_annotation_dir),
            taxon_id = taxon_id
          )
```
# Set your design matrix (for the first time)

### Further Reading
#### Experimental Design in Proteomics
- [Experimental Design for Mass Spectrometry](https://www.nature.com/articles/s41596-021-00566-6) - Guidelines for designing MS experiments
- [Statistical Considerations in Proteomics](https://link.springer.com/protocol/10.1007/978-1-60761-987-1_16) - Statistical aspects of proteomics experimental design
- [DIA Experimental Design](https://pubs.rsc.org/en/content/articlehtml/2021/mo/d0mo00072h) - Special considerations for DIA-MS studies

#### Design Matrices in R
- [Design Matrices Explained]( https://www.r-bloggers.com/2023/11/understanding-matrices-in-r-programming/) - Understanding R design matrices
- [R for Data Science: Model Basics](https://r4ds.had.co.nz/model-basics.html) - Explanation of model matrices in R
- [limma User Guide](https://bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf) - Details on design matrices for differential expression

#### Sample Organization 
- [Batch Effects in Omics Data](https://www.nature.com/articles/nrg2825) - Understanding and addressing batch effects


This section helps you create a design matrix that links your samples to experimental conditions. The design matrix is a fundamental component that defines how your samples relate to each other and which comparisons will be possible in your analysis. MultiScholaR provides an interactive applet to help you build this matrix easily.
```{r 5 Design Matrix Setup} 
# if (exists("design_matrix", envir = .GlobalEnv)) {
#   print("Design matrix already set :) No need to run app again!")
# } else {
#       RunApplet(applet_type = "designMatrix"
#                 , omic_type = omic_type
#                 , experiment_label = experiment_label
#                 , force = TRUE)
# }
# Comment in if you wish to run manually
RunApplet(applet_type = "designMatrix"
, omic_type = "proteomics"
, experiment_label = experiment_label
, force = TRUE)
```

# If you have the design matrix stored from a previous run, you can read it in here, otherwise skip

### Further Reading
#### Data Import in R
- [Data Import with R](https://r4ds.had.co.nz/data-import.html) - Comprehensive guide to importing data
- [readr Package](https://readr.tidyverse.org/) - Modern data import functions
- [vroom Package](https://vroom.r-lib.org/) - Fast reading of delimited files

#### File Path Management
- [File Paths in R](https://www.r4epi.com/file-paths/) - Best practices for handling file paths
- [Using file.path() Function](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/file.path) - Platform-independent path construction

#### Working with Tables in R
- [R Data Import/Export](https://cran.r-project.org/doc/manuals/r-release/R-data.html) - Official R documentation on data import
- [Data Table Formats](https://www.datacamp.com/community/tutorials/r-data-import-tutorial) - Overview of various table formats in R

This section reads in the created design matrix / cleaned data matrix rather than creating one from scratch. This is useful if you're re-running the analysis or if you've prepared your design matrix in a different way, or if you are bringing your own versions of these.

```{r 6 Design Matrix Read In}
design_matrix <- read.table(
  file = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$source_dir, "design_matrix.tab"),
  sep = "\t",
  header = TRUE,
  stringsAsFactors = FALSE
)

data_cln <- vroom::vroom(
  file = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$source_dir, "data_cln.tab"),
  delim = "\t",
  col_types = cols(.default = col_guess()),
  na = c("", "NA")
)
```

# Convert the protein identifiers to Uniprot or Uniparc accessions if those annotations are available 
## Otherwise makes use of pre-supplied uniprot fasta annotations

### Further Reading
#### Protein Identifiers and Databases
- [UniProt Knowledgebase](https://www.uniprot.org/help/uniprotkb) - Comprehensive resource for protein sequence and annotation data
- [UniParc](https://www.uniprot.org/help/uniparc) - UniProt Archive for non-redundant sequence database
- [Protein Accession Numbers Explained](https://www.uniprot.org/help/accession_numbers) - Understanding different protein accession formats

#### Protein Sequence Analysis
- [Sequence Analysis Tools](https://www.ebi.ac.uk/Tools/sss/) - Tools for sequence similarity searches
- [Bioconductor for Proteomics](https://bioconductor.org/packages/release/BiocViews.html#___Proteomics) - R/Bioconductor tools for proteomics
- [Introduction to Biostrings](https://bioconductor.org/packages/release/bioc/vignettes/Biostrings/inst/doc/BiostringsQuickOverview.pdf) - Working with biological sequences in R

#### FASTA File Processing
- [seqinr Package](https://cran.r-project.org/web/packages/seqinr/index.html) - Biological sequences retrieval and analysis
- [Parsing FASTA Files in R](https://bioinformaticschool.com/fasta-files-bioinformatics-guide/) - Different approaches to reading FASTA files
- [rentrez Package](https://cran.r-project.org/web/packages/rentrez/vignettes/rentrez_tutorial.html) - Accessing NCBI resources programmatically

This section converts protein identifiers in your dataset to standardized UniProt or UniParc accessions, which are essential for accurate protein identification and downstream functional analysis. This step ensures consistent annotation and allows for better integration with protein databases and pathways.




## Create the PeptideQuantitativeData object
### This section initializes a PeptideQuantitativeData object with peptide-level 
### quantitative data and experimental design information.
### It specifies the column names for various data attributes and sets up the 
### design matrix for the experiment.

### Further Reading
#### S4 Objects in R
- [Introduction to S4 Objects](https://bioconductor.org/help/course-materials/2017/Zurich/S4-classes-and-methods.html) - Understanding S4 object-oriented programming in R
- [S4 Classes in Bioconductor](https://bioconductor.org/packages/release/bioc/vignettes/S4Vectors/inst/doc/S4QuickOverview.pdf) - Overview of S4 vectors in Bioconductor
- [Methods and Classes in R](https://adv-r.hadley.nz/s4.html) - Advanced R guide to S4 system

#### Proteomics Data Structures
- [MSnbase: MS Data Structures](https://www.bioconductor.org/packages/release/bioc/vignettes/MSnbase/inst/doc/MSnbase-development.html) - Data structures for MS proteomics
- [Tidy Proteomics](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-023-05360-7) - Modern approaches to proteomics data in R
- [ProteomicsAnnotationHubData](https://github.com/lgatto/ProteomicsAnnotationHubData) - Resources for proteomics annotation

#### Peptide Quantification in DIA
- [MaxLFQ Algorithm](https://www.sciencedirect.com/science/article/pii/S1535947620333107) - Label-free quantification in proteomics
- [Proteotypic Peptides](https://pubmed.ncbi.nlm.nih.gov/27975286/) - Understanding proteotypic peptides in proteomics

This section creates a structured S4 object to hold your peptide data with all the necessary metadata. By organizing your data in this object, ProteomeScholaR can easily access and manipulate the data in subsequent analysis steps, while maintaining the relationship between peptides, proteins, and experimental conditions.

```{r 8 Peptide Data S4 Object Creation & Annotation API Lookup}
peptide_data <- new(
  "PeptideQuantitativeData"

  # Protein vs Sample quantitative data
  ,
  peptide_data = data_cln,
  protein_id_column = "Protein.Ids",
  peptide_sequence_column = "Stripped.Sequence",
  q_value_column = "Q.Value",
  global_q_value_column = "Global.Q.Value",
  proteotypic_peptide_sequence_column = "Proteotypic",
  raw_quantity_column = "Precursor.Quantity",
  norm_quantity_column = "Precursor.Normalised",
  is_logged_data = FALSE

  # Design Matrix Information
  , design_matrix = design_matrix,
  sample_id = "Run",
  group_id = "group",
  technical_replicate_id = "replicates",
  args = config_list
)



```

# Raw Data QC

### Further Reading
#### Quality Control in Proteomics
- [QC in Proteomics Analysis](https://pubs.acs.org/doi/10.1021/acs.jproteome.4c00363) - Comprehensive overview of QC in proteomics
- [Quality Metrics for LC-MS](https://www.sciencedirect.com/science/article/pii/S1535947620337956) - Metrics for evaluating LC-MS performance

#### Data Visualization for QC
- [ggplot2 Documentation](https://ggplot2.tidyverse.org/) - Tool used for visualization in this workflow
- [Data Visualization in R](https://socviz.co/) - Comprehensive guide to data visualization
- [Interactive Visualizations with plotly](https://plotly.com/r/) - Creating interactive plots for data exploration

#### Filtering Strategies
- [Proteomics Data Filtering]( https://www.r-bloggers.com/2018/08/proteomics-data-analysis-2-3-data-filtering-and-missing-value-imputation/) - Approaches to filtering proteomics data
- [DIA Filtering Considerations](https://www.nature.com/articles/s41592-022-01639-4) - Special considerations for DIA data filtering

#### QC Metrics Tracking
- [Longitudinal QC Monitoring](https://pubmed.ncbi.nlm.nih.gov/28483925/) - Tracking metrics across experiments

This section begins the quality control process by examining your raw data and establishes a baseline for subsequent filtering steps. Each `updateProteinFiltering()` call throughout the workflow creates a checkpoint that allows you to track the effects of each filtering step on your dataset. This systematic approach lets you monitor how many peptides and proteins remain after each filter, helping you understand the impact of quality thresholds and optimize your data processing strategy. The visualization tools create plots showing the effect of each filtering step, providing documentation for your methods section and helping identify potential issues in your analysis pipeline.

```{r 9 Raw Data QC}
updateProteinFiltering( # Please note that these images don't render properly in RStudio, please see the publication_graphs_dir for the images
  data = data_cln,
  step_name = "1_Raw Data",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Filter peptides based on q-value and proteotypic peptide match
### NB should be left to default unless you have specific experimental needs

### Further Reading
#### False Discovery Rate and q-values
- [Understanding q-values and FDR](https://totallab.com/resources/p-values-fdr-q-values/) - Statistical basis for q-values in proteomics
- [FDR Control in Proteomics](https://www.bioinfor.com/fdr-tutorial/) - Methods for controlling false discovery rate
- [Target-Decoy Strategy](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2922680/) - The basis for FDR estimation in proteomics

#### Peptide-to-Protein Inference
- [Protein Inference Challenges](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3118629/) - Challenges in mapping peptides to proteins
- [Inference Algorithms](https://www.mcponline.org/article/S1535-9476(20)33526-9/fulltext) - Methods for peptide-to-protein mapping
- [Shared Peptides](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5891066/) - Handling shared peptides across proteins

This step filters your dataset based on the statistical confidence of peptide identifications (q-value) and ensures that only peptides uniquely mapping to a specific protein (proteotypic) are retained. This is crucial for reliable protein quantification, as it reduces false positives and ambiguous peptide assignments that could lead to inaccurate protein abundance estimates. The `updateProteinFiltering()` function at the end visualizes the effect of this filtering step compared to the raw data.

```{r 10 Filter Peptides on q Value and Proteotypic Peptide Match}
search_srl_quant_cln <- srlQvalueProteotypicPeptideClean(theObject = peptide_data)
qval_protein_count <- search_srl_quant_cln@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after q-value and proteotypic filtering:", qval_protein_count))

# Example: Interactively change q-value thresholds and re-run this chunk
# peptide_data <- updateConfigParameter(
#   theObject      = peptide_data, # Use the input object from the previous step
#   function_name  = "srlQvalueProteotypicPeptideClean",
#   parameter_name = "qvalue_threshold",               # Effect: Lower = stricter peptide ID confidence
#   new_value      = NEW_VALUE_HERE
# )
# peptide_data <- updateConfigParameter(
#   theObject      = peptide_data,
#   function_name  = "srlQvalueProteotypicPeptideClean",
#   parameter_name = "global_qvalue_threshold",        # Effect: Lower = stricter protein group ID confidence
#   new_value      = NEW_VALUE_HERE
# )
# peptide_data <- updateConfigParameter(
#   theObject      = peptide_data,
#   function_name  = "srlQvalueProteotypicPeptideClean",
#   parameter_name = "choose_only_proteotypic_peptide",# Effect: 1 = Keep only unique peptides, 0 = Keep shared peptides
#   new_value      = NEW_VALUE_HERE
# )
# search_srl_quant_cln <- srlQvalueProteotypicPeptideClean(theObject = peptide_data) # Re-run after update

updateProteinFiltering(
  data = search_srl_quant_cln@peptide_data,
  step_name = "2_qval Filtered",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Roll-up of precursor ions to peptide level intensity value quantitation

### Further Reading
#### Precursor-to-Peptide Aggregation
- [Precursor Ion Quantification](https://pmc.ncbi.nlm.nih.gov/articles/PMC3804902/) - Methods for quantifying precursor ions
- [Precursor Ion Selection](https://pmc.ncbi.nlm.nih.gov/articles/PMC5557716/) - Selection strategies for precursor ions

#### Data Aggregation Methods
- [Data Aggregation in Proteomics](https://pubmed.ncbi.nlm.nih.gov/39198030/) - Methods for data aggregation

#### Isotopologue Patterns
- [Isotope Patterns in MS](https://www.sciencedirect.com/science/article/pii/S2667145X24000282) - Understanding isotopic patterns
- [Isotopologue Analysis](https://www.thermofisher.com/blog/proteomics/the-isotopologue/) - Methods for analyzing isotopologue data
- [MS1 Quantification](https://pmc.ncbi.nlm.nih.gov/articles/PMC6849792/) - MS1-based quantification approaches

This step aggregates intensity measurements from multiple precursor ions (different charge states, isotopes, and/or modified forms) to the peptide level. This roll-up process is critical for accurate quantification as it combines the signal from all observed forms of the same peptide sequence, providing a more robust measurement of peptide abundance. The function applies statistical methods to appropriately combine these measurements while minimizing the effect of outliers or noisy precursors. The subsequent `updateProteinFiltering()` call tracks how this roll-up affects the number of unique proteins identified in your dataset.

```{r 11 Roll-up Precursor to Peptide}
peptide_normalized_tbl <- rollUpPrecursorToPeptide(search_srl_quant_cln)
rollup_protein_count <- peptide_normalized_tbl@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins after precursor roll-up to peptide level:", rollup_protein_count))

updateProteinFiltering(
  data = peptide_normalized_tbl@peptide_data,
  step_name = "3_peptidoform count",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Remove peptide based on the intensity threshold and the proportion of samples 
## below the threshold
### NB should be left to default unless you have specific experimental needs

### Further Reading
#### Intensity Thresholding
- [Signal-to-Noise in Proteomics](https://link.springer.com/protocol/10.1007/978-1-61779-573-2_4) - Understanding signal thresholds
- [Intensity-Based Filtering](https://pmc.ncbi.nlm.nih.gov/articles/PMC2842913/) - Methods for intensity-based filters
- [Dynamic Range in Proteomics](https://pubmed.ncbi.nlm.nih.gov/23307342/) - Dealing with wide dynamic ranges

#### Missing Values in Proteomics
- [Missing Value Patterns](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/pmic.202200092) - Understanding missing data in proteomics
- [Missing Value Handling Strategies](https://www.bioconductor.org/packages/devel/bioc/vignettes/DEP/inst/doc/MissingValues.html) - Approaches for missing value management
- [Missing Not At Random (MNAR)](https://www.nature.com/articles/s41598-017-19120-0) - Statistical treatment of different missing value types

#### Threshold Determination
- [Optimal Threshold Selection](https://www.mcponline.org/article/S1535-9476(20)33371-4/fulltext) - Methods for determining optimal thresholds
- [Data-Driven Thresholding](https://pubs.acs.org/doi/10.1021/acs.jproteome.8b00377) - Approaches to data-dependent filtering
- [Impact of Filtering on Statistical Power](https://www.mcponline.org/article/S1535-9476(20)31134-8/fulltext) - Effect of filtering on statistical analysis

This step removes peptides with low intensity values or those that are detected in too few samples. Low-intensity peptides often have poor quantification accuracy and can introduce noise into the dataset. By setting an intensity threshold and requiring peptides to be detected in a minimum number of samples, this filter improves the reliability of downstream quantification and statistical analysis. The step is crucial for reducing false discoveries that could result from spurious low-intensity signals. As with previous steps, the `updateProteinFiltering()` function tracks the effect of this filter on your dataset.

```{r 12 Filter Peptides on Intensity Threshold}
peptide_normalized_pif_cln <- peptideIntensityFiltering(
  theObject = peptide_normalized_tbl
)
pif_protein_count <- peptide_normalized_pif_cln@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after peptide intensity/proportion filtering:", pif_protein_count))

# Example: Interactively change intensity filtering thresholds and re-run this chunk
# peptide_normalized_tbl <- updateConfigParameter(
#   theObject      = peptide_normalized_tbl, # Use the input object from the previous step
#   function_name  = "peptideIntensityFiltering",
#   parameter_name = "peptides_intensity_cutoff_percentile", # Effect: Higher = stricter intensity threshold (removes more)
#   new_value      = NEW_VALUE_HERE
# )
# peptide_normalized_tbl <- updateConfigParameter(
#   theObject      = peptide_normalized_tbl,
#   function_name  = "peptideIntensityFiltering",
#   parameter_name = "peptides_proportion_of_samples_below_cutoff", # Effect: Higher = more relaxed filtering (keeps more)
#   new_value      = NEW_VALUE_HERE
# )
# peptide_normalized_pif_cln <- peptideIntensityFiltering(theObject = peptide_normalized_tbl) # Re-run after update

updateProteinFiltering(
  data = peptide_normalized_pif_cln@peptide_data,
  step_name = "4_peptideIntensityFiltering",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Keep the proteins only if they have two or more peptides mapping
### NB should be left to default unless you have specific experimental needs 
#### CHANGE IF YOU ARE INTERESTED IN QUANTIFYING SINGLE PEPTIDE PROTEIN MATCHES

### Further Reading
#### Multiple Peptide Rule
- [Two-Peptide Rule in Proteomics](https://pubmed.ncbi.nlm.nih.gov/19627159/) - Rationale for requiring multiple peptides
- [Protein Inference Best Practices](https://www.halolabs.com/blog/protein-identification-and-characterization-guide/) - Guidelines for protein identification

#### Special Cases
- [Short Proteins and Small ORFs]( https://jbiomedsci.biomedcentral.com/articles/10.1186/s12929-022-00802-5) - Approaches for small proteins with few peptides

This step filters proteins based on the number of unique peptides identified for each protein, keeping only those with at least two peptides. This "two-peptide rule" is a widely accepted standard in proteomics that greatly increases confidence in protein identifications and quantification reliability. While this filter may remove some true protein identifications (especially for small proteins or those with low sequence coverage), it significantly reduces false positive identifications. As the comment suggests, you may consider changing this threshold if you have specific interest in proteins that might only be identified by a single peptide, but be aware of the increased uncertainty associated with such identifications.

```{r 13 Filter Proteins on Peptide Number}
removed_proteins_with_less_than_two_peptides <- filterMinNumPeptidesPerProtein(
  theObject = peptide_normalized_pif_cln
)
twopep_protein_count <- removed_proteins_with_less_than_two_peptides@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins identified by >= 2 peptides:", twopep_protein_count))

# Example: Interactively change minimum peptide/peptidoform count and re-run this chunk
# peptide_normalized_pif_cln <- updateConfigParameter(
#   theObject      = peptide_normalized_pif_cln, # Use the input object from the previous step
#   function_name  = "filterMinNumPeptidesPerProtein",
#   parameter_name = "num_peptides_per_protein_thresh", # Effect: Higher = requires more unique peptides per protein
#   new_value      = NEW_VALUE_HERE
# )
# peptide_normalized_pif_cln <- updateConfigParameter(
#   theObject      = peptide_normalized_pif_cln,
#   function_name  = "filterMinNumPeptidesPerProtein",
#   parameter_name = "num_peptidoforms_per_protein_thresh", # Effect: Higher = requires more peptide forms per protein (more stringent)
#   new_value      = NEW_VALUE_HERE
# )
# removed_proteins_with_less_than_two_peptides <- filterMinNumPeptidesPerProtein(theObject = peptide_normalized_pif_cln) # Re-run after update

updateProteinFiltering(
  data = removed_proteins_with_less_than_two_peptides@peptide_data,
  step_name = "5_twopeptidesperprotein",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Remove samples with insufficient peptide counts
#### This section filters out samples that have fewer than a specified minimum 
#### number of peptides - ie poor sample performance

### Further Reading
#### Sample Quality Assessment
- [Sample Quality Metrics in Proteomics](https://www.creative-proteomics.com/resource/sample-quality-control-in-omics-research.htm) - Methods for assessing sample quality

#### Sample Exclusion Criteria
- [Outlier Detection in Proteomics](https://academic.oup.com/bib/article/25/3/bbae129/7638267) - Methods for detecting outlier samples

#### Technical Considerations
- [Sample Preparation Impact](https://pmc.ncbi.nlm.nih.gov/articles/PMC4817721/) - Effects of sample preparation on peptide recovery

This step identifies and removes samples with poor performance, indicated by an unusually low number of identified peptides. Such samples may be compromised due to issues in sample preparation, instrument performance, or other technical factors. Including these low-quality samples in downstream analysis could introduce bias and reduce the statistical power to detect true biological differences. The minimum peptide threshold ensures that all samples have sufficient protein coverage for reliable quantification, while the summary statistics at the end help evaluate which samples were removed and why.

```{r 14 Filter Samples on Peptide Number}
peptide_keep_samples_with_min_num_peptides <- filterMinNumPeptidesPerSample(
  theObject = removed_proteins_with_less_than_two_peptides
)
minsamp_protein_count <- peptide_keep_samples_with_min_num_peptides@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after removing low-peptide-count samples:", minsamp_protein_count))
message(paste(
  "Number of distinct runs remaining:",
  peptide_keep_samples_with_min_num_peptides@peptide_data |> distinct(Run) |> nrow()
))
message("Peptidoform counts per remaining run:")
print(
  peptide_keep_samples_with_min_num_peptides@peptide_data |>
    distinct(Run, Protein.Ids, Stripped.Sequence, peptidoform_count) |>
    group_by(Run) |>
    summarise(n = sum(peptidoform_count)) |>
    arrange(desc(n))
)

# Example: Interactively change minimum peptides per sample cutoff and re-run this chunk
#removed_proteins_with_less_than_two_peptides <- updateConfigParameter(
#   theObject      = removed_proteins_with_less_than_two_peptides, # Use the input object from the previous step
#   function_name  = "filterMinNumPeptidesPerSample",
#   parameter_name = "peptides_per_sample_cutoff", # Effect: Higher = stricter sample quality filter (removes more samples)
#   new_value      = new_value_here
#)
# peptide_keep_samples_with_min_num_peptides <- #filterMinNumPeptidesPerSample(theObject = #removed_proteins_with_less_than_two_peptides) # Re-run after update

updateProteinFiltering(
  data = peptide_keep_samples_with_min_num_peptides@peptide_data,
  step_name = "6_minpeppersample",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

```{r 15 RUV normalisation on peptides}
devtools::load_all("C:/Users/willk/OneDrive - Macquarie University/Projects/APAF Software/MultiScholaR")
loadDependencies()

# Calculate peptide matrix if not already done
peptide_matrix_obj <- calcPeptideMatrix(peptide_keep_samples_with_min_num_peptides)

# Step 2: Log2 transform (like IQ used to do automatically)
peptide_log2_obj <- log2TransformPeptideMatrix(peptide_matrix_obj)

plotRle(
  peptide_log2_obj,
  group = "group",
  yaxis_limit = c(-6, 6)
)

plotPca(peptide_log2_obj,
        grouping_variable = "group",
  label_column = "",
  shape_variable = "group",
  title = "",
  font_size = 8)



# Step 3: Normalization (now operates on log2 data - correct!)
peptide_normalised_obj <- normaliseBetweenSamples(
  peptide_log2_obj,
  normalisation_method = "cyclicloess"
)

plotRle(
  peptide_normalised_obj,
  group = "group",
  yaxis_limit = c(-6, 6)
)

plotPca(peptide_normalised_obj,
        grouping_variable = "group",
  label_column = "",
  shape_variable = "group",
  title = "",
  font_size = 8)


# Step 2: Find optimal percentage with automatic optimization (NOW FAST!)
optimal_results_peptides <- findBestNegCtrlPercentagePeptides(
  peptide_normalised_obj,
  percentage_range = seq(1, 30, by = 1),  # Can test wider range for peptides
  num_components_to_impute = 5,
  ruv_grouping_variable = "group",
  separation_metric = "max_difference",
  k_penalty_weight = 0.5,
  adaptive_k_penalty = TRUE,
  verbose = TRUE
)

# Step 3: Extract results
best_percentage <- optimal_results_peptides$best_percentage
best_k <- optimal_results_peptides$best_k
control_peptides <- optimal_results_peptides$best_control_genes_index

# Step 4: Apply RUV normalization with optimal parameters
peptide_ruv_normalised_results_temp_obj <- ruvIII_C_Varying(
  peptide_normalised_obj,
  ruv_grouping_variable = "group",
  ruv_number_k = best_k,
  ctrl = control_peptides
)

plotRle(
  peptide_ruv_normalised_results_temp_obj,
  group = "group",
  yaxis_limit = c(-6, 6)
)

plotPca(peptide_ruv_normalised_results_temp_obj,
        grouping_variable = "group",
  label_column = "",
  shape_variable = "group",
  title = "",
  font_size = 8)
optimal_results_peptides$best_cancor_plot

# Limpa DPC Imputation
peptide_ruv_normalised_imputed_obj <- peptideMissingValueImputationLimpa(
  peptide_ruv_normalised_results_temp_obj,
  imputed_value_column = "Peptide.Imputed.Limpa",
  use_log2_transform = FALSE,  # Recommended for limpa
  verbose = TRUE
)


plotRle(
  peptide_ruv_normalised_imputed_obj,
  group = "group",
  yaxis_limit = c(-6, 6)
)

plotPca(peptide_ruv_normalised_imputed_obj,
        grouping_variable = "group",
  label_column = "",
  shape_variable = "group",
  title = "",
  font_size = 8)

# Initialize QC composite figure
QC_composite_figure <- InitialiseGrid()

# Add the canonical correlation plot from optimization
QC_composite_figure@cancor_plots$cancor_plot_peptides_ruvIIIc_group <- optimal_results_peptides$best_cancor_plot
QC_composite_figure@cancor_titles$cancor_plot_peptides_ruvIIIc_group <- paste("Peptide RUV Optimization (", best_percentage, "%, k=", best_k, ")", sep="")

# Count remaining peptides after filtering
ruvfilt_peptide_count <- nrow(peptide_ruv_normalised_results_temp_obj@peptide_matrix)
message(paste("Number of peptides remaining after RUV normalization and filtering:", ruvfilt_peptide_count))

# Add QC plots to the composite figure
QC_composite_figure@rle_plots$peptide_rle_plot_after_ruvIIIc_group <- plotRle(
  peptide_ruv_normalised_results_temp_obj,
  group = "group",
  yaxis_limit = c(-6, 6)
)

QC_composite_figure@pca_plots$peptide_pca_plot_after_ruvIIIc_group <- plotPca(
  peptide_ruv_normalised_results_temp_obj,
  grouping_variable = "group",
  label_column = "",
  shape_variable = "group",
  title = "",
  font_size = 8
)

QC_composite_figure@density_plots$density_plot_after_ruvIIIc_group <- plotDensity(
  peptide_ruv_normalised_results_temp_obj,
  grouping_variable = "group"
)

QC_composite_figure@pearson_plots$pearson_correlation_pair_after_ruvIIIc_group <- plotPearson(
  peptide_ruv_normalised_results_temp_obj,
  tech_rep_remove_regex = "pool",
  correlation_group = "group"
)

# Save individual plots
savePlot(
  QC_composite_figure@rle_plots$peptide_rle_plot_after_ruvIIIc_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$peptide_qc_dir,
  "peptide_rle_plot_after_ruvIIIc_by_group"
)
savePlot(
  QC_composite_figure@pca_plots$peptide_pca_plot_after_ruvIIIc_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$peptide_qc_dir,
  "peptide_pca_plot_after_ruvIIIc"
)
savePlot(
  QC_composite_figure@density_plots$density_plot_after_ruvIIIc_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$peptide_qc_dir,
  "peptide_density_plot_after_ruvIIIc_by_group"
)
savePlot(
  QC_composite_figure@pearson_plots$pearson_correlation_pair_after_ruvIIIc_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$peptide_qc_dir,
  "peptide_pearson_correlation_pair_after_ruvIIIc_group"
)

# Display the composite plot
QC_composite_figure 

updateProteinFiltering(
  data = peptide_ruv_normalised_results_temp_obj@peptide_data,
  step_name = "7_RUVnormalisedpeptides",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)

# Limpa DPC Imputation
peptide_ruv_normalised_imputed_obj <- peptideMissingValueImputationLimpa(
  peptide_ruv_normalised_results_temp_obj,
  imputed_value_column = "Peptide.Imputed.Limpa",
  use_log2_transform = TRUE,  # Recommended for limpa
  verbose = TRUE
)

```


```{r 16 Impute missing values with limpa}
# Generate limpa DPC diagnostic plots - FIXED VERSION
if (!is.null(peptide_ruv_normalised_imputed_obj@args$limpa_dpc_results)) {
  dpc_results <- peptide_ruv_normalised_imputed_obj@args$limpa_dpc_results
  
  cat("\n=== limpa DPC Imputation Results ===\n")
  cat("DPC Method:", ifelse(is.null(dpc_results$dpc_method), "limpa_dpc", dpc_results$dpc_method), "\n")
  cat("Beta0 (intercept):", round(dpc_results$dpc_parameters[1], 4), "\n")
  cat("Beta1 (slope):", round(dpc_results$dpc_parameters[2], 4), "\n")
  cat("Missing value mechanism:", dpc_results$slope_interpretation, "\n")
  cat("Missing percentage before imputation:", dpc_results$missing_percentage_before, "%\n")
  
  # Calculate missing percentage after imputation directly
  missing_pct_after <- round(100 * mean(is.na(peptide_ruv_normalised_imputed_obj@peptide_matrix)), 1)
  cat("Missing percentage after imputation:", missing_pct_after, "%\n")
  cat("=====================================\n")
  
  # 1. Plot the Detection Probability Curve
  tryCatch({
    # Recreate the DPC fit from stored data for plotting
    if (!is.null(dpc_results$y_peptide_for_dpc)) {
      cat("Recreating DPC fit for plotting...\n")
      dpcfit_for_plot <- limpa::dpc(dpc_results$y_peptide_for_dpc)
      
      # Generate DPC plot
      dpc_curve_plot <- limpa::plotDPC(dpcfit_for_plot) +
        ggplot2::ggtitle(paste("Detection Probability Curve\n", 
                               "Slope:", round(dpc_results$dpc_parameters[2], 3),
                               "- Mechanism:", dpc_results$slope_interpretation)) +
        ggplot2::theme_bw()
      
      # Save DPC plot
      savePlot(
        dpc_curve_plot,
        project_dirs[[paste0(omic_type, "_", experiment_label)]]$peptide_qc_dir,
        "limpa_detection_probability_curve"
      )
      
      cat("DPC plot generated successfully!\n")
      
    } else {
      message("Original y_peptide data not available for DPC plotting")
      dpc_curve_plot <- NULL
    }
    
  }, error = function(e) {
    message("Could not generate DPC plot: ", e$message)
    dpc_curve_plot <- NULL
  })
  
  # 2. Missing Value Pattern Comparison
  tryCatch({
    # Before imputation (use the pre-imputation object)
    before_matrix <- peptide_ruv_normalised_results_temp_obj@peptide_matrix
    after_matrix <- peptide_ruv_normalised_imputed_obj@peptide_matrix
    
    # Check matrices are valid
    if (is.null(before_matrix) || is.null(after_matrix)) {
      stop("One or both matrices are NULL")
    }
    
    # Ensure matrices have same dimensions for comparison
    if (!identical(dim(before_matrix), dim(after_matrix))) {
      warning("Before and after matrices have different dimensions")
      # Use only common samples/peptides
      common_samples <- intersect(colnames(before_matrix), colnames(after_matrix))
      common_peptides <- intersect(rownames(before_matrix), rownames(after_matrix))
      before_matrix <- before_matrix[common_peptides, common_samples]
      after_matrix <- after_matrix[common_peptides, common_samples]
    }
    
    missing_before <- is.na(before_matrix)
    missing_after <- is.na(after_matrix)
    
    missing_pattern_before <- data.frame(
      Sample = rep(colnames(before_matrix), each = nrow(before_matrix)),
      Peptide = rep(1:nrow(before_matrix), ncol(before_matrix)),
      Missing = as.vector(missing_before),
      Stage = "Before limpa Imputation"
    )
    
    missing_pattern_after <- data.frame(
      Sample = rep(colnames(after_matrix), each = nrow(after_matrix)),
      Peptide = rep(1:nrow(after_matrix), ncol(after_matrix)),
      Missing = as.vector(missing_after),
      Stage = "After limpa Imputation"
    )
    
    # Combine patterns
    missing_comparison <- rbind(missing_pattern_before, missing_pattern_after)
    
    # Missing values per sample comparison
    missing_per_sample_plot <- missing_comparison |>
      dplyr::group_by(Sample, Stage) |>
      dplyr::summarise(Missing_Count = sum(Missing), .groups = "drop") |>
      ggplot2::ggplot(ggplot2::aes(x = Sample, y = Missing_Count, fill = Stage)) +
      ggplot2::geom_col(position = "dodge") +
      ggplot2::theme_bw() +
      ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1)) +
      ggplot2::labs(
        title = "Missing Values per Sample: Before vs After limpa Imputation",
        y = "Number of Missing Peptides",
        x = "Sample"
      ) +
      ggplot2::scale_fill_manual(values = c("Before limpa Imputation" = "red", 
                                           "After limpa Imputation" = "blue"))
    
    cat("Missing value comparison plot generated!\n")
    
  }, error = function(e) {
    message("Could not generate missing value comparison: ", e$message)
    missing_per_sample_plot <- ggplot2::ggplot() + 
      ggplot2::annotate("text", x = 0.5, y = 0.5, label = "Missing value comparison unavailable") +
      ggplot2::theme_void()
  })
  
  # 3. Intensity Distribution Comparison
  tryCatch({
    # Get data for comparison (data is already log2 transformed)
    before_data <- as.vector(before_matrix)
    before_data <- before_data[!is.na(before_data) & is.finite(before_data)]
    
    after_data <- as.vector(after_matrix)
    after_data <- after_data[!is.na(after_data) & is.finite(after_data)]
    
    if (length(before_data) > 0 && length(after_data) > 0) {
      # Create comparison data frame
      intensity_comparison <- data.frame(
        Intensity = c(before_data, after_data),
        Stage = c(rep("Before limpa Imputation", length(before_data)),
                  rep("After limpa Imputation", length(after_data)))
      )
      
      # Intensity distribution plot (data already log2 transformed)
      intensity_distribution_plot <- intensity_comparison |>
        ggplot2::ggplot(ggplot2::aes(x = Intensity, fill = Stage)) +
        ggplot2::geom_density(alpha = 0.6) +
        ggplot2::theme_bw() +
        ggplot2::labs(
          title = "Peptide Intensity Distribution: Before vs After limpa Imputation",
          x = "log2 Intensity",
          y = "Density"
        ) +
        ggplot2::scale_fill_manual(values = c("Before limpa Imputation" = "red", 
                                             "After limpa Imputation" = "blue"))
      
      cat("Intensity distribution plot generated!\n")
    } else {
      intensity_distribution_plot <- ggplot2::ggplot() + 
        ggplot2::annotate("text", x = 0.5, y = 0.5, label = "Intensity distribution unavailable") +
        ggplot2::theme_void()
    }
    
  }, error = function(e) {
    message("Could not generate intensity distribution: ", e$message)
    intensity_distribution_plot <- ggplot2::ggplot() + 
      ggplot2::annotate("text", x = 0.5, y = 0.5, label = "Intensity distribution unavailable") +
      ggplot2::theme_void()
  })
  
  # 4. Summary text plot
  summary_text_plot <- ggplot2::ggplot() +
    ggplot2::annotate("text", x = 0.5, y = 0.8, 
                     label = paste("limpa DPC Imputation Summary\n",
                                   "Method:", ifelse(is.null(dpc_results$dpc_method), "limpa_dpc", dpc_results$dpc_method), "\n",
                                   "DPC Slope (1):", round(dpc_results$dpc_parameters[2], 3), "\n",
                                   "DPC Intercept (0):", round(dpc_results$dpc_parameters[1], 3), "\n",
                                   "Missing Mechanism:", dpc_results$slope_interpretation, "\n",
                                   "Missing % Before:", dpc_results$missing_percentage_before, "%\n",
                                   "Missing % After:", missing_pct_after, "%"),
                     size = 4, hjust = 0.5) +
    ggplot2::theme_void() +
    ggplot2::ggtitle("limpa Imputation Results Summary")
  
  # Save all diagnostic plots
  tryCatch({
    savePlot(
      missing_per_sample_plot,
      project_dirs[[paste0(omic_type, "_", experiment_label)]]$peptide_qc_dir,
      "limpa_missing_values_comparison"
    )
    
    savePlot(
      intensity_distribution_plot,
      project_dirs[[paste0(omic_type, "_", experiment_label)]]$peptide_qc_dir,
      "limpa_intensity_distribution_comparison"
    )
    
    savePlot(
      summary_text_plot,
      project_dirs[[paste0(omic_type, "_", experiment_label)]]$peptide_qc_dir,
      "limpa_imputation_summary"
    )
    
    cat("All limpa diagnostic plots saved successfully!\n")
    
  }, error = function(e) {
    message("Error saving plots: ", e$message)
  })
  
  # Add the DPC curve to your composite figure if it was created successfully
  if (exists("dpc_curve_plot") && !is.null(dpc_curve_plot)) {
    QC_composite_figure@cancor_plots$limpa_dpc_curve <- dpc_curve_plot
    QC_composite_figure@cancor_titles$limpa_dpc_curve <- "limpa Detection Probability Curve"
    cat("DPC curve added to composite figure!\n")
  }
  
} else {
  message("No limpa DPC results found in object.")
}
```

```{r}
# Simpler missing pattern analysis
y_peptide_stored <- dpc_results$y_peptide_for_dpc

# Create a simple detected/missing summary
intensity_values <- as.vector(y_peptide_stored)
detected_data <- data.frame(
  intensity = intensity_values[!is.na(intensity_values)],
  status = "Detected"
)

# For missing values, we can't plot their actual intensities (they're missing!)
# So let's just show the detection pattern across the intensity range
intensity_range <- seq(min(intensity_values, na.rm = TRUE), 
                      max(intensity_values, na.rm = TRUE), 
                      length.out = 100)

# Calculate detection probability using the DPC curve
# P(detection) = plogis(beta0 + beta1 * intensity)
beta0 <- -1.3209555
beta1 <- 0.0734459
detection_prob <- plogis(beta0 + beta1 * intensity_range)

# Create a clean DPC curve plot
dpc_clean_plot <- data.frame(
  intensity = intensity_range,
  detection_probability = detection_prob
) |>
  ggplot2::ggplot(ggplot2::aes(x = intensity, y = detection_probability)) +
  ggplot2::geom_line(color = "blue", size = 1.2) +
  ggplot2::geom_rug(data = detected_data, 
                   ggplot2::aes(x = intensity, y = NULL), 
                   sides = "b", alpha = 0.3, color = "black") +
  ggplot2::labs(
    title = paste("Detection Probability Curve\n0 =", round(beta0, 3), 
                 ", 1 =", round(beta1, 3), "(nearly random missing)"),
    x = "log2 Intensity",
    y = "Detection Probability"
  ) +
  ggplot2::theme_bw() +
  ggplot2::ylim(0, 1)

print(dpc_clean_plot)

# Summary statistics
cat("\n=== Missing Pattern Analysis ===\n")
cat("Total data points:", length(intensity_values), "\n")
cat("Detected points:", sum(!is.na(intensity_values)), "\n") 
cat("Missing points:", sum(is.na(intensity_values)), "\n")
cat("Overall missing %:", round(100 * mean(is.na(intensity_values)), 1), "%\n")

# Detection probability at different intensities
low_intensity <- quantile(intensity_values, 0.1, na.rm = TRUE)
med_intensity <- quantile(intensity_values, 0.5, na.rm = TRUE) 
high_intensity <- quantile(intensity_values, 0.9, na.rm = TRUE)

cat("\nDetection probability by intensity:\n")
cat("At low intensity (", round(low_intensity, 1), "):", 
    round(plogis(beta0 + beta1 * low_intensity), 3), "\n")
cat("At medium intensity (", round(med_intensity, 1), "):", 
    round(plogis(beta0 + beta1 * med_intensity), 3), "\n")
cat("At high intensity (", round(high_intensity, 1), "):", 
    round(plogis(beta0 + beta1 * high_intensity), 3), "\n")

cat("\nConclusion: Missing values are nearly intensity-independent!")
cat("\nThis is IDEAL for limpa imputation! \n")
```

## Read in the fasta organism specific fasta file to extract details on the protein sequences
### There should be no need to change this chunk, ever

## This section aggregates the long-format peptide-level intensity values into protein-level quantification:
### It uses the IQ tool (https://github.com/tvpham/iq), which implements the same algorithm as DIA-NN's maxLFQ but runs faster (written in C++) 
### Unless your experiment has specific requirements or you wish to use IQ differently, it is recommended to leave default settings here

### Further Reading

#### IQ Tool
- [IQ GitHub Repository](https://github.com/tvpham/iq) - Source code and documentation for the IQ tool

This section aggregates peptide-level measurements to obtain protein-level quantification, a critical step in the proteomics workflow. The IQ tool implements the widely-used MaxLFQ algorithm, which provides accurate relative quantification of proteins across samples while accounting for peptide-level variability. The algorithm uses a graph-based approach to derive the most consistent protein ratios, even when different peptides are detected across samples. The resulting protein quantification values are logged (log2), which helps normalize the data and makes it suitable for statistical analysis. The step concludes by checking that the number of proteins matches expectations, ensuring no data is lost during the rollup process.

```{r 17 Peptide to Protein Rollup}
iq::process_long_format(
  peptide_values_imputed_file,
  output_filename = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "iq_output_file.txt"),
  sample_id = "Run",
  primary_id = "Protein.Ids",
  secondary_id = "Stripped.Sequence",
  intensity_col = "Peptide.Imputed",
  filter_double_less = c("Q.Value" = "0.01", "PG.Q.Value" = "0.01")
  ## very important for this workflow that you do NOT perform normalization here
  , normalization = "none"
)

## Read in the IQ output file (which outputs a file, not an object)
protein_log2_quant <- vroom::vroom(
  file.path(file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "iq_output_file.txt")))
```

## Create Protein Quantitative Data Object
### Unless you have changed the column identifiers or the object names leave defaults

### Further Reading
#### S4 Object-Oriented Programming
- [Object-Oriented Design](https://www.biostat.jhsph.edu/~rpeng/docs/R-classes-scope.pdf) - Principles of object-oriented design in R

#### Proteomics Data Structures
- [Data Representation in Proteomics](https://pmc.ncbi.nlm.nih.gov/articles/PMC4457114/) - Standards for proteomics data representation
- [ExpressionSet in Bioconductor](https://www.bioconductor.org/packages/release/bioc/vignettes/Biobase/inst/doc/ExpressionSetIntroduction.pdf) - Classical data structure for expression data
- [SummarizedExperiment Class](https://bioconductor.org/packages/release/bioc/vignettes/SummarizedExperiment/inst/doc/SummarizedExperiment.html) - Modern standard for omics data

#### Experimental Design Integration
- [Metadata in Proteomics](https://pmc.ncbi.nlm.nih.gov/articles/PMC7116434/) - Handling sample metadata in proteomics
- [Reproducible Analysis Frameworks](https://www.nature.com/articles/s41467-022-32155-w) - Frameworks for reproducible analysis

This step creates a structured S4 object to organize the protein quantification data alongside experimental metadata. This object-oriented approach encapsulates all the protein quantification values with their associated sample information and experimental design, making subsequent analysis more robust and reproducible. The object provides methods for data access and manipulation that maintain the integrity of the relationship between quantitative data and experimental design. This structured approach is particularly important for complex proteomics experiments where keeping track of sample groupings, technical replicates, and experimental conditions is crucial for valid statistical analysis.

```{r 18 Protein Data S4 Object Creation}
protein_obj <- ProteinQuantitativeData(
  # Protein Data Matrix Information
  protein_quant_table = protein_log2_quant,
  protein_id_column = "Protein.Ids",
  protein_id_table = protein_log2_quant |> distinct(Protein.Ids), 
  # Design Matrix Information
  design_matrix = peptide_values_imputed@design_matrix,
  sample_id = "Run",
  group_id = "group",
  technical_replicate_id = "replicates",
  args = peptide_values_imputed@args
)
```

## Arrange the protein ID's list to opt for the best accession in the list to be placed first
## Skips this section if you have supplied your own Uniprot accession searches

### Further Reading
#### Protein Accession Standards

#### Handling Multiple Accessions
- [Protein Inference](https://www.sciencedirect.com/science/article/pii/S1535947620341219) - Statistical approaches to protein inference
- [Ambiguous Protein Assignment](https://www.sciencedirect.com/science/article/pii/S1535947620335040) - Dealing with ambiguous peptide-to-protein mapping

#### Database Curation
- [Protein Database Quality](https://bigomics.ch/blog/guide-to-top-proteomics-databases-and-how-to-access-them/) - Impact of database quality on proteomics
- [Swiss-Prot vs TrEMBL](https://www.uniprot.org/help/uniprotkb_sections) - Differences between reviewed and unreviewed entries
- [Proteogenomics Approaches](https://pmc.ncbi.nlm.nih.gov/articles/PMC4991544/) - Integrating proteomics with genomics

This step ensures that protein accessions are standardized and prioritized according to relevant biological criteria. In many cases, peptides may map to multiple protein accessions, and this function helps choose the most informative or canonical accession to represent each protein. The priority is typically given to reviewed UniProt entries (Swiss-Prot), conserved entries, or entries with more complete annotation. This standardization is essential for downstream analyses like pathway analysis and functional annotation. The `updateProteinFiltering()` function shows how many proteins remain after this cleanup step.

```{r 19 Most Likely Protein Accession Rollup}
if (is.null(uniprot_search_results)) {
  aa_seq_tbl_final <- aa_seq_tbl_final |>
    dplyr::rename(uniprot_acc = database_id)

  protein_log2_quant_cln <- chooseBestProteinAccession(
    theObject = protein_obj,
    delim = ";",
    seqinr_obj = aa_seq_tbl_final,
    seqinr_accession_column = "uniprot_acc",
    replace_zero_with_na = TRUE,
    aggregation_method = "mean"
  )
  annotclean_protein_count <- protein_log2_quant_cln@protein_quant_table |>
    distinct(Protein.Ids) |>
    nrow()
  message(paste("Number of distinct proteins after choosing best protein accession:", annotclean_protein_count))

  updateProteinFiltering(
    data = protein_log2_quant_cln@protein_quant_table,
    step_name = "8_annotation_cleanup",
    omic_type = omic_type, # e.g., "proteomics"
    experiment_label = experiment_label, # e.g., "workshop_data"
    return_grid = TRUE,
    overwrite = TRUE
  )
} else {
  protein_log2_quant_cln <- protein_obj
}
```



## Pre-normalisation data QC
### RLE plot 
### PCA plot
### Pearson correlation
### Spearman correlation

### Further Reading
#### Quality Control Visualization
- [Interactive Visualization Tools](https://pmc.ncbi.nlm.nih.gov/articles/PMC4510819/) - Modern approaches to interactive data visualization

#### Principal Component Analysis
- [PCA in Proteomics](https://pmc.ncbi.nlm.nih.gov/articles/PMC4676806/) - Applications of PCA in proteomics
- [Understanding PCA](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) - Step by step explanation of PCA
- [Interpreting PCA Results](https://bioturing.medium.com/how-to-read-pca-biplots-and-scree-plots-186246aae063) - How to interpret PCA plots correctly

#### Correlation Analysis
- [Correlation Measures in Omics](https://www.nature.com/articles/s43588-023-00436-z) - Different correlation metrics for biological data
- [Sample Correlation Assessment](https://www.metwarebio.com/sample-correlation-analysis/) - Using correlation to assess sample quality

#### Relative Log Expression (RLE) Plots
- [RLE for Normalization Assessment]( https://biocellgen-public.svi.edu.au/mig_2019_scrnaseq-workshop/normalization-confounders-and-batch-correction.html) - Using RLE to evaluate normalization
```{r 24 Pre-normalisation QC}
QC_composite_figure <- InitialiseGrid()

QC_composite_figure@rle_plots$rle_plot_before_cyclic_loess <- plotRle(
  protein_log2_quant_cln,
  "group",
  yaxis_limit = c(-6, 6)
)

QC_composite_figure@pca_plots$pca_plot_before_cyclic_loess_group <- plotPca(
  protein_log2_quant_cln,
  grouping_variable = "group",
  label_column = "",
  shape_variable = "group",
  title = "",
  font_size = 8
)

QC_composite_figure@density_plots$density_plot_before_cyclic_loess_group <- plotDensity(
  QC_composite_figure@pca_plots$pca_plot_before_cyclic_loess_group,
  grouping_variable = "group"
)

pca_mixomics_before_cyclic_loess <- getPcaMatrix(protein_log2_quant_cln)

QC_composite_figure@pearson_plots$pearson_correlation_pair_before_cyclic_loess <-
  plotPearson(
    protein_log2_quant_cln,
    tech_rep_remove_regex = "pool",
    correlation_group = "group"
  )

summarizeQCPlot(QC_composite_figure)

savePlot(
  QC_composite_figure@rle_plots$rle_plot_before_cyclic_loess,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "rle_plot_before_cyclic_loess"
)
savePlot(
  QC_composite_figure@pca_plots$pca_plot_before_cyclic_loess_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "pca_plot_before_cyclic_loess"
)
savePlot(
  QC_composite_figure@density_plots$density_plot_before_cyclic_loess_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "density_plot_before_cyclic_loess"
)
savePlot(
  QC_composite_figure@pearson_plots$pearson_correlation_pair_before_cyclic_loess,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "pearson_correlation_pair_before_cyclic_loess"
)

frozen_protein_matrix_tech_rep <- proteinTechRepCorrelation(
  protein_log2_quant_cln,
  tech_rep_num_column = "group",
  tech_rep_remove_regex = "pool"
)

## change if you wish to filter on pearson or spearman
frozen_protein_matrix_tech_rep |>
  dplyr::filter(pearson > 0.8) |>
  nrow()
# frozen_protein_matrix_tech_rep |>
#  dplyr::filter(spearman > 0.8) |>
#  nrow()
```


## Sample Pearson Correlation Filtering

### Further Reading
#### Correlation-Based Sample Filtering
- [Pearson vs Spearman Correlation](https://pmc.ncbi.nlm.nih.gov/articles/PMC7779167/) - Choosing the appropriate correlation measure

#### Replicate Quality Control
- [Replicate Agreement Assessment](https://pubmed.ncbi.nlm.nih.gov/22462118/) - Methods for assessing agreement between replicates
- [Technical vs Biological Variation](https://www.nature.com/documents/Biological_and_technical_replicates_guidelines.pdf) - Distinguishing sources of variation

#### Impact on Statistical Power
- [Sample Filtering Effects on Power](https://academic.oup.com/bjaed/article/16/5/159/2389876) - How filtering affects statistical power
- [Outlier Influence on Differential Expression](https://cardinalscholar.bsu.edu/server/api/core/bitstreams/37a09705-8632-480a-9771-9b4cdafad608/content) - How outliers affect differential expression results

This section filters samples based on their Pearson correlation with other samples in the same experimental group. Low correlation between samples that should be similar (e.g., biological replicates) often indicates technical issues or sample quality problems. By implementing a correlation threshold (here set at 0.5), this step removes samples that don't correlate well with their group, ensuring that only high-quality, consistent samples are used for downstream statistical analysis. The `updateProteinFiltering()` function tracks the effect of this filtering step on the number of proteins in the dataset. The final data is saved in both TSV and RDS formats for transparency and to facilitate downstream analysis. This correlation-based filtering is the final quality control step before differential expression analysis, ensuring that the data used for biological interpretation is of the highest quality.

```{r 28 Sample Pearson Correlation Filtering}
ruv_correlation_vec <- pearsonCorForSamplePairs(
  ruv_normalised_results_cln_obj,
  tech_rep_remove_regex = "pool",
  correlation_group = "group"
)

ruv_normalised_filtered_results_obj <- filterSamplesByProteinCorrelationThreshold(
  ruv_normalised_results_cln_obj,
  pearson_correlation_per_pair = ruv_correlation_vec,
  min_pearson_correlation_threshold = 0.5
)

corfilt_protein_count <- ruv_normalised_filtered_results_obj@protein_quant_table |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after removing low-correlation samples:", corfilt_protein_count))

updateProteinFiltering(
  data = ruv_normalised_filtered_results_obj@protein_quant_table,
  step_name = "12_correlation_filtered",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)

vroom::vroom_write(
  ruv_normalised_filtered_results_obj@protein_quant_table,
  file.path(
    project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
    "ruv_normalised_results_cln_with_replicates.tsv"
  )
)

saveRDS(
  ruv_normalised_filtered_results_obj,
  file.path(
    project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
    "ruv_normalised_results_cln_with_replicates.RDS"
  )
)

saveRDS(
  ruv_normalised_filtered_results_obj, 
  file.path(
    project_dirs[[paste0("integration_", experiment_label)]]$multiomic_inputs_dir, 
    "ruv_normalised_results_cln_with_replicates.RDS" 
  )
)

ruv_normalised_for_de_analysis_obj <- ruv_normalised_filtered_results_obj

ruv_normalised_for_de_analysis <-
  ruv_normalised_for_de_analysis_obj@protein_quant_table |>
  pivot_longer(
    cols = !matches("Protein.Ids"),
    names_to = "replicates",
    values_to = "Log2.Protein.Imputed"
  ) |>
  dplyr::select(Protein.Ids, replicates, Log2.Protein.Imputed) |>
  mutate(Protein.Imputed = 2^Log2.Protein.Imputed) |>
  mutate(Protein.Imputed = ifelse(is.na(Protein.Imputed), NA, Protein.Imputed)) |>
  pivot_wider(
    id_cols = Protein.Ids,
    names_from = replicates,
    values_from = Protein.Imputed
  ) |>
  dplyr::rename(uniprot_acc = "Protein.Ids")

vroom::vroom_write(
  ruv_normalised_for_de_analysis,
  file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "ruv_normalised_results.tsv")
)

vroom::vroom_write(
  ruv_normalised_for_de_analysis |>
    dplyr::mutate(across(!matches("uniprot_acc"), log2)),
  file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "ruv_normalised_results_log.tsv")
)

vroom::vroom_write(
  design_matrix |>
    distinct(replicates, group) |>
    dplyr::rename(Run = replicates),
  file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "design_matrix_avrg.tsv")
)

ruv_normalised_for_de_analysis_mat <- ruv_normalised_for_de_analysis |>
  column_to_rownames("uniprot_acc") |>
  as.matrix()
```


## Composite QC Figure Generation

### Further Reading
#### Publication-Ready Visualizations
- [Data Visualization Best Practices](https://www.sciencedirect.com/science/article/pii/S2666389920301896) - Principles for effective scientific visualizations
- [Communicating with Data](https://journals.sagepub.com/doi/10.1177/15291006211051956) - How to communicate findings through visualization
- [Figure Design in Scientific Publications](https://www.nature.com/documents/natrev-artworkguide_PS.pdf) - Guidelines for creating impactful scientific figures

#### Multi-panel Figures
- [Composite Figures in R](https://cran.r-project.org/web/packages/patchwork/vignettes/patchwork.html) - Creating multi-panel figures with patchwork
- [Complex Layouts in ggplot2](https://ggplot2-book.org/arranging-plots.html) - Arranging multiple plots in ggplot2
- [Visual Hierarchy in Scientific Figures](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003833) - Establishing visual hierarchy in complex figures

#### Consistency in Data Presentation
- [Color Use in Scientific Visualization](https://www.molecularecologist.com/2020/04/23/simple-tools-for-mastering-color-in-scientific-figures/) - Appropriate use of color in scientific figures
- [Accessible Scientific Figures](https://www.a11y-collective.com/blog/accessible-charts/) - Creating visualizations accessible to all readers

This section creates a comprehensive multi-panel quality control figure that combines all the QC visualizations generated throughout the workflow into a single publication-ready composite image. This composite figure provides a holistic view of the data quality at different stages of processing, allowing for easy comparison between pre- and post-normalization states. The function organizes PCA plots, density plots, RLE plots, and correlation plots into a structured grid with consistent formatting and appropriate labels (a, b, c, etc.) for easy reference in publications. This single visualization serves as a powerful summary of the entire quality control process, making it easier to present and interpret the extensive QC work performed. The saved high-resolution figure can be directly included in publications or presentations to demonstrate the thoroughness of the data quality assessment.

```{r 30 Composite QC Figure Generation}
pca_ruv_rle_correlation_merged <- createGridQC(
  QC_composite_figure,
  pca_titles = c("a)", "b)", "c)"),
  density_titles = c("d)", "e)", "f)"),
  rle_titles = c("g)", "h)", "i)"),
  pearson_titles = c("j)", "k)", "l)"),
  save_path = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir),
  file_name = "composite_QC_figure"
)
pca_ruv_rle_correlation_merged
```

# Add up to date annotation data from Uniprot
## Will take some time to complete if  you are doing this for first time
## Good spot to grab a coffee :)

### Further Reading

#### Protein Functional Annotation
- [Gene Ontology Resources](https://geneontology.org/docs/go-annotations/) - Understanding Gene Ontology annotations
- [Protein Function Prediction](https://academic.oup.com/bib/article/25/4/bbae289/7696515) - Methods for protein function prediction
- [Evidence Codes in Annotation](https://geneontology.org/docs/guide-go-evidence-codes/) - Understanding evidence codes in annotations

#### Bioconductor Tools for Annotation
- [UniProt.ws Package](https://bioconductor.org/packages/release/bioc/html/UniProt.ws.html) - Bioconductor interface to UniProt
- [Annotation Workflows](https://www.bioconductor.org/packages/release/workflows/) - Complete annotation workflows in Bioconductor
- [Programmatic Access to Biological Data](https://pmc.ncbi.nlm.nih.gov/articles/PMC9815577/) - Best practices for accessing biological databases

This section retrieves and integrates up-to-date protein annotation data from UniProt, the leading universal protein resource database. The annotation process uses the UniProt.ws Bioconductor package to query the UniProt database for comprehensive information about each protein identified in your dataset. This information includes metadata like protein existence evidence, annotation score, review status, gene names, protein names, sequence length, cross-references to Ensembl, Gene Ontology (GO) terms, and keywords. By caching the results in an RDS file, the workflow avoids redundant queries in subsequent runs, significantly speeding up the process while still ensuring data freshness. This annotation step is crucial for biological interpretation of your results, as it provides the functional context needed to understand the biological significance of differentially expressed proteins.

```{r 31 Uniprot Data Annotation}
uniprot_dat_cln <- getUniprotAnnotations(
  input_tbl = ruv_normalised_for_de_analysis_obj@protein_quant_table,
  cache_dir = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$uniprot_annotation_dir),
  taxon_id = taxon_id
)
```

## Read in Previously Assigned Contrasts (optional)

### Further Reading
#### Experimental Contrasts
- [Experimental Design for Differential Analysis](https://www.nature.com/articles/s41467-024-47899-w) - Designing experiments for differential analysis
- [Contrast Specification in R](https://www.bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf) - Guide to specifying contrasts in limma
- [Pairwise Comparisons](https://pmc.ncbi.nlm.nih.gov/articles/PMC3047155/) - Strategies for pairwise comparisons in proteomics

#### Contrast Formulation
- [Linear Models for Proteomics](https://pnnl-comp-mass-spec.github.io/proteomics-data-analysis-tutorial/linear-reg.html) - Linear modeling approaches in proteomics
- [Multiple Testing Considerations](https://pmc.ncbi.nlm.nih.gov/articles/PMC5506159/) - Managing multiple comparisons in omics data

This section reads in predefined contrast specifications from a file, allowing for consistent and reproducible differential expression analysis. Contrasts define the comparisons between experimental groups (e.g., treatment vs. control) that will be tested for protein abundance differences. By defining these contrasts in a separate file, the workflow becomes more flexible and allows for complex experimental designs. This approach also makes it easy to update the analysis with new comparisons without changing the main workflow code. The contrast strings follow a specific format that will be interpreted by the differential expression analysis function to create the appropriate statistical tests.

```{r 32 Read in Previously Assigned Contrasts (optional)}
contrasts_tbl <- file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$source_dir, "contrast_strings.tab") |>
  readLines() |>
  {
    \(x) x[!grepl("^contrasts", x)]
  }() |>
  as_tibble() |>
  dplyr::rename(contrasts = value)
```

## DE Analysis Generation

### Further Reading
#### Differential Expression Analysis
- [Limma-Based Analysis](https://www.bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf) - Detailed guide to limma for differential expression

#### Moderated Statistics
- [Moderated t-tests](https://stanford.edu/group/wonglab/doc/NewGeneScore-5.13.07.pdf) - Understanding moderated t-statistics
- [Empirical Bayes Methods](https://pmc.ncbi.nlm.nih.gov/articles/PMC2759080/) - Empirical Bayes approaches in proteomics
- [TREAT Method](https://pmc.ncbi.nlm.nih.gov/articles/PMC2654802/) - Testing fold changes with thresholds

#### Interpreting DE Results
- [Volcano Plot Interpretation](https://pnnl-comp-mass-spec.github.io/proteomics-data-analysis-tutorial/volcano-plots.html) - Understanding volcano plots in proteomics
- [Statistical Power in omics](https://pmc.ncbi.nlm.nih.gov/articles/PMC4287952/) - Understanding statistical power in differential expression

This section performs differential expression analysis to identify proteins with statistically significant abundance differences between experimental conditions. The analysis uses limma, a well-established statistical framework that applies linear models and empirical Bayes methods to improve statistical power - particularly beneficial for proteomics data where sample sizes are often limited. For each contrast defined in the previous step, the code runs a complete differential expression analysis, including moderated t-tests with robust estimation and trend-based variance modeling. The TREAT method can optionally be applied to test for minimum fold-change thresholds in addition to statistical significance. The results are stored in a named list where each element contains the complete differential expression statistics for one contrast, ready for downstream visualization and biological interpretation.

```{r 33 DE Analysis Generation}
# Debug - if regex readin from config doesnt work
# NB here temporarily until config readin logic fixed for this pattern
config_list$deAnalysisParameters$args_group_pattern <- "(\\d+)"

# Create contrast names first
contrast_names <- contrasts_tbl |>
  dplyr::pull(contrasts) |>
  stringr::str_extract("^[^=]+") |> # Extract everything before the = sign
  stringr::str_replace_all("\\.", "_") # Replace dots with underscores

# Run DE analysis and explicitly set names
de_analysis_results_list <- seq_len(nrow(contrasts_tbl)) |>
  purrr::set_names(contrast_names) |> # This ensures the list will be named
  purrr::map(\(contrast_idx) {
    deAnalysisWrapperFunction(
      ruv_normalised_for_de_analysis_obj,
      contrasts_tbl |> dplyr::slice(contrast_idx),
      formula_string = config_list$deAnalysisParameters$formula_string,
      de_q_val_thresh = config_list$deAnalysisParameters$de_q_val_thresh,
      treat_lfc_cutoff = config_list$deAnalysisParameters$treat_lfc_cutoff,
      eBayes_trend = config_list$deAnalysisParameters$eBayes_trend,
      eBayes_robust = config_list$deAnalysisParameters$eBayes_robust,
      args_group_pattern = config_list$deAnalysisParameters$args_group_pattern,
      args_row_id = ruv_normalised_for_de_analysis_obj@protein_id_column
    )
  })

# Verify we have names
print(names(de_analysis_results_list))
```



## Output the results of the DE analysis

### Further Reading
#### Results Visualization
- [Customizing R Graphics](https://www.springer.com/gp/book/9780387981413) - Guide to customizing R graphics for publication

#### Data Export for Publication
- [Report Generation in R](https://bookdown.org/yihui/rmarkdown/) - Comprehensive guide to report generation
- [ProteomeXchange Submission](http://www.proteomexchange.org/submission) - Protocol for submitting data to repositories

#### Error Handling in R
- [Error Handling Best Practices](https://adv-r.hadley.nz/conditions.html) - Advanced R guide to error handling
- [Parallel Processing Errors](https://cran.r-project.org/web/packages/future/vignettes/future-4-issues.html) - Managing errors in parallel processing

This section outputs the differential expression analysis results in various formats for interpretation and publication. For each contrast, the code generates comprehensive result tables and visualizations, including volcano plots that highlight significantly changing proteins. The function takes advantage of the annotations retrieved from UniProt to enrich the results with biological context, including gene names and protein descriptions. Error handling with tryCatch ensures that the workflow continues even if individual contrasts encounter issues. Multiple output files are generated for each contrast, including TSV files for downstream analysis in other tools and publication-quality figures in various formats. These outputs provide both the raw statistical results and their biological interpretation, facilitating the transition from statistical significance to biological insight.

```{r 34 Output DE Analysis Results}
# Modified output approach with error handling
names(de_analysis_results_list) |>
  purrr::walk(\(contrast_name) {
    tryCatch(
      {
        message(paste("Processing contrast:", contrast_name))

        # Check if the result exists and has content
        result <- de_analysis_results_list[[contrast_name]]
        if (is.null(result) || length(result) == 0) {
          message(paste("Skipping empty result for:", contrast_name))
          return(NULL)
        }

        outputDeAnalysisResults(
          result,
          ruv_normalised_for_de_analysis_obj,
          uniprot_dat_cln,
          file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$de_output_dir),
          file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$publication_graphs_dir),
          file_prefix = paste0("de_proteins_", contrast_name),
          plots_format = config_list$deAnalysisParameters$plots_format,
          args_row_id = ruv_normalised_for_de_analysis_obj@protein_id_column,
          de_q_val_thresh = 0.05,
          gene_names_column = "gene_names"
        )
      },
      error = function(e) {
        message(paste("Error processing contrast:", contrast_name))
        message(paste("Error message:", e$message))
      }
    )
  })
```

## Enrichment Analysis
## This section performs enrichment analysis on the DE results
## If your taxon id is on the list supported by gprofiler, that will be used to perform the enrichment analysis
## Otherwise, the enrichment analysis will be performed using the GO annotations provided by UniProt in clusterProfiler

### Further Reading
#### Functional Enrichment Analysis
- [Gene Set Enrichment Methods](https://academic.oup.com/bib/article/22/1/545/5722384?login=true) - Overview of enrichment analysis methods
- [Pathway Analysis in Proteomics](https://www.sciencedirect.com/science/article/pii/S002251931400304X) - Approaches to pathway analysis
- [Multiple Testing in Enrichment](https://link.springer.com/article/10.1186/s13059-019-1716-1) - Controlling false discovery in enrichment

#### Gene Ontology Analysis
- [GO Enrichment Interpretation](https://www.nature.com/articles/nprot.2008.211) - Guide to interpreting GO enrichment
- [Semantic Similarity in GO](https://geneontology.org/docs/go-enrichment-analysis/) - Understanding semantic relationships in GO

#### Visualization of Enrichment Results
- [Enrichment Map Visualization](https://www.nature.com/articles/s41596-018-0103-9) - Creating enrichment map networks
- [GOplot Package](https://wencke.github.io/) - Visualizing functional analysis results

This section performs functional enrichment analysis to identify biological processes, molecular functions, and cellular components overrepresented in your differentially expressed proteins. The workflow is flexible, first attempting to use g:Profiler for enrichment if your organism's taxonomy ID is supported, then falling back to clusterProfiler with GO annotations from UniProt if needed. The analysis is performed separately for upregulated and downregulated protein sets from each contrast, providing a comprehensive view of the biological processes affected in different conditions. The enrichment results include statistical measures like p-values and enrichment ratios, along with visualizations like enrichment maps and GO term networks. This step transforms your list of differential proteins into meaningful biological insights, helping you understand the functional implications of the observed protein abundance changes.

```{r 35 Enrichment Analysis}
# Create S4 object of the DE results for enrichment analysis
de_results_for_enrichment <- createDEResultsForEnrichment(
  contrasts_tbl = contrasts_tbl,
  design_matrix = design_matrix,
  de_output_dir = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$de_output_dir)
)

# Run enrichment analysis
enrichment_results <- processEnrichments(
  de_results_for_enrichment # Your S4 object with DE results
  , taxon_id = taxon_id # your organism
  , up_cutoff = 0 # FC filtering
  , down_cutoff = 0 # FC filtering
  , q_cutoff = 0.05 # FDR threshold
  , pathway_dir = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$pathway_dir) # Output directory
  , go_annotations = uniprot_dat_cln # Annotation data
  , exclude_iea = FALSE,
  protein_id_column = ruv_normalised_for_de_analysis_obj@protein_id_column,
  contrast_names = (names(de_analysis_results_list))
)
```

## Save Workflow Arguments and Study Summary

### Further Reading
#### Workflow Reproducibility
- [Reproducible Workflows in R](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006561) - Best practices for reproducible workflows

#### Configuration Management
- [Parameter Management in R](https://bookdown.org/rdpeng/rprogdatascience/control-structures.html) - Strategies for parameter management
- [Version Control for Data Analysis](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004947) - Using version control with data analysis

This section saves all the parameters, configurations, and settings used throughout the workflow, creating a comprehensive record that enables reproduction of the analysis. The `createWorkflowArgsFromConfig()` function captures all the configuration parameters used in the analysis, including normalization settings, filtering thresholds, and statistical parameters. This approach to parameter tracking is essential for reproducible research, as it allows future users (including your future self) to understand exactly how the analysis was performed and to replicate the results if needed. This record serves as both documentation and a practical tool for reproducing or modifying the analysis in the future.

```{r 36 Save Workflow Arguments and Study SUmmary} 
# Create workflow args with config and git info
workflow_args <- createWorkflowArgsFromConfig(
  workflow_name = experiment_label
  , description = "Full protein analysis workflow with config parameters"
  , source_dir_path = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$source_dir)
) 

# Show the workflow arguments
workflow_args
```

# Copy Files to Publication Directory

### Further Reading
#### Data Organization for Publication
- [FAIR Data Principles](https://www.nature.com/articles/sdata201618) - Findable, Accessible, Interoperable, and Reusable data principles
- [Publication-Ready Outputs](https://www.nature.com/articles/s41592-019-0470-2) - Standards for publication-ready outputs

#### Data Archiving
- [Data Archiving Strategies](https://www.nature.com/articles/s41597-020-0524-5) - Approaches to long-term data archiving

This section copies key result files to a designated publication directory, creating a clean, organized set of outputs that can be easily shared with collaborators or included in publications. By centralizing the most important results and visualizations, this step simplifies the process of finding and using the outputs that matter most for interpretation and communication. The publication directory serves as a curated collection of the analysis results, containing only the final, polished outputs rather than intermediate files or raw data. This organization strategy aligns with best practices for research data management and makes it easier to prepare your results for sharing or publication.

```{r 37 Copy Files to Publication Directory}
copyToResultsSummary(
  omic_type = omic_type, # or "metabolomics", etc.
  experiment_label = experiment_label, # Your specific label
  force = FALSE
)
```

# Copy all study parameters to a Github repo for audit trail

### Further Reading
#### Version Control in Research
- [Git for Data Analysis](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004668) - Using Git for data analysis projects


This final step creates a complete GitHub repository containing all analysis parameters, configuration files, and essential results, establishing a permanent, version-controlled record of your analysis. Using Git for version control provides numerous benefits: it creates an immutable audit trail of your analysis, facilitates collaboration with team members, enables easy tracking of changes over time, and serves as a backup of your work. The repository can be shared with collaborators or cited in publications, allowing others to understand your methods in detail or even reproduce your analysis. This practice aligns with modern standards for computational reproducibility and transparent reporting in scientific research.

```{r 38 Copy Output to Github}
options(
  github_org = "your org",
  github_user_email = "your email",
  github_user_name = "your username"
)

pushProjectToGithub(
  base_dir = base_dir,
  source_dir = source_dir,
  project_id = "your project"
)
```

# Render the report
## This will create a report in the report directory
### The report contains all the information from the workflow including results, parameters, methods and figures
```{r 39 Render Report}
RenderReport(omic_type = omic_type
                         , experiment_label = experiment_label
                         , rmd_filename = "DIANN_report.rmd")
```