---
title: "DIANN Analysis for xyz"
version: "1.0"
author: "Your_fancy_self"
output:
  html_document:
    code_folding: true
    self_contained: true
    toc: false
    warning: false
    message: false
---
# Initial R environment setup  
## Checks your R environment for the required packages to run MultiScholaR, and installs them if they are not.

### Further Reading
#### Understanding Package Management in R
- [CRAN (The Comprehensive R Archive Network)](https://cran.r-project.org/) - The main repository for R packages
- [Bioconductor](https://bioconductor.org/) - Repository specialized in bioinformatics packages
- [R Package Installation Guide](https://www.datacamp.com/community/tutorials/r-packages-guide) - A comprehensive guide to installing packages in R
- [devtools Documentation](https://devtools.r-lib.org/) - Learn about the devtools package for installing from GitHub

#### Important Packages Used in this Workflow
- [clusterProfiler](https://bioconductor.org/packages/release/bioc/html/clusterProfiler.html) - Used for gene ontology and pathway enrichment analysis
- [GO.db](https://bioconductor.org/packages/release/data/annotation/html/GO.db.html) - Gene Ontology database annotations
- [UniProt.ws](https://bioconductor.org/packages/release/bioc/html/UniProt.ws.html) - Interface to the UniProt web services
- [mixOmics](https://www.bioconductor.org/packages/release/bioc/html/mixOmics.html) - Methods for multivariate analysis of biological data

#### About MultiScholaR
- [MultiScholaR GitHub](https://github.com/APAF-BIOINFORMATICS/MultiScholaR) - The main repository for the package
- [APAF Bioinformatics](https://www.mq.edu.au/research/research-centres-groups-and-facilities/facilities/macquarie-analytical-and-fabrication-facility/australian-proteome-analysis-facility) - Australian Proteome Analysis Facility bioinformatics resources

This function checks for and installs all the required packages for the MultiScholaR workflow. It installs packages from CRAN, Bioconductor, and GitHub as needed. The function is designed to make setup easy for users who may not be familiar with R package management.

## IF THIS IS YOUR FIRST INSTALL, THIS WILL TAKE SOME TIME AS THERE ARE A NUMBER OF DEPENDENCIES TO INSTALL :)
## GOOD POINT TO GRAB A COFFEE!
```{r 1 MultiScholaR FIRST INSTALL, message=TRUE, warning=TRUE}
installMultiScholaR <- function(verbose = TRUE) {
    # Install devtools if missing
    if (!requireNamespace("devtools", quietly = TRUE)) {
        install.packages("devtools")
    }

    # Detach if loaded
    if ("package:MultiScholaR" %in% search()) {
        try(detach("package:MultiScholaR", unload = TRUE, force = TRUE), silent = TRUE)
    }

    # Unload namespace
    try(unloadNamespace("MultiScholaR"), silent = TRUE)


    devtools::install_github(
        "APAF-BIOINFORMATICS/MultiScholaR",
        ref = "main", # Main branch
        dependencies = TRUE,
        upgrade = "never",
        force = TRUE
    )

    # Load it
    library(MultiScholaR)
}

installMultiScholaR()
loadDependencies()
```

# START HERE if you already have MultiScholaR installed

### Further Reading
#### R Library Management
- [Introduction to R Libraries](https://www.datacamp.com/community/tutorials/r-packages-guide) - Understanding how R libraries work
- [R Package Documentation](https://www.rdocumentation.org/) - Search for documentation of R packages
- [The R Packages Book](https://r-pkgs.org/) - In-depth explanation of R packages by Hadley Wickham and Jennifer Bryan

#### Dependency Management in R
- [Understanding R Package Dependencies](https://www.r-bloggers.com/2023/05/dependency-management/) - Best practices for managing dependencies
- [Packrat and renv](https://rstudio.github.io/renv/articles/renv.html) - Tools for reproducible environments in R

#### Reproducible Research
- [Reproducible Research with R](https://cran.r-project.org/web/views/ReproducibleResearch.html) - CRAN task view on reproducible research
- [R Markdown](https://rmarkdown.rstudio.com/) - Framework for reproducible reports in R

When you already have MultiScholaR installed (and you don't want to check for updates), you can start from this point. The code below loads the package and all its dependencies to prepare your environment for analysis.
```{r 2 Load MultiScholaR}
library(MultiScholaR)
loadDependencies()
```

# Set up your environment and project directory

### Further Reading
#### Project Organization & File Management
- [Project Management in R](https://swcarpentry.github.io/r-novice-gapminder/02-project-intro.html) - Best practices for organizing project files
- [here package](https://here.r-lib.org/) - A popular R package for project-relative file paths
- [File Organization in R Projects](https://www.tidyverse.org/blog/2017/12/workflow-vs-script/) - Workflow vs script-based approaches
- [renv for Reproducible Environments](https://rstudio.github.io/renv/) - Package for project isolation and reproducibility

#### Data Organization Principles
- [Tidy Data Principles](https://www.jstatsoft.org/article/view/v059i10) - Hadley Wickham's paper on tidy data structure
- [Good enough practices in scientific computing](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510) - Practical recommendations for data management

#### Proteomics Project Structure
- [Guidelines for Reporting Mass Spectrometry Data](https://www.mcponline.org/article/S1535-9476(20)31288-3/fulltext) - Standards for proteomics project organization
- [ProteomeXchange](http://www.proteomexchange.org/) - Repository standards for proteomics data

This section establishes the directory structure for your project. Setting up a consistent directory structure is crucial for managing the various files generated throughout the analysis, including input data, intermediate results, and final outputs. MultiScholaR creates a standardized structure to help organize your work.
```{r 3 Project Environment Management}
# Directory Management
## Set up the project directory structure
## This section sets up the project directory structure for MultiScholaR
## Directory management can be challenging, particularly when managing objects
## across multiple chunks within a single R Markdown document.
experiment_label <- "your_analysis"
omic_type <- "proteomics" # Set this to the type of analysis you are doing eg "proteomics", "metabolomics", "transcriptomics"
# Setup for the central pillars of molecular biology
project_dirs <- setupDirectories(
    #omic_types = "metabolomics"
    # Or: 
    omic_types = c("proteomics", "metabolomics", "transcriptomics", "lipidomics", "integration"),
    , label = experiment_label,
    force = FALSE # Set to TRUE to skip prompts if dirs exist
)
```

# At this step, please copy your data, fasta file and other data necessary into 
# the appropriate directories

### Further Reading
#### Proteomics Data Formats
- [DIA-NN Documentation](https://github.com/vdemichev/DiaNN) - The software that generates the report.tsv input file
- [FASTA Format Description](https://www.ncbi.nlm.nih.gov/genbank/fastaformat/) - Explanation of the FASTA file format
- [UniProt Database](https://www.uniprot.org/) - Source for protein sequence databases and FASTA files
- [Proteomics Standards Initiative](https://www.psidev.info/) - Standards for proteomics data formats

#### Data Preprocessing in Proteomics
- [mzTab Format](https://www.psidev.info/mztab) - Standard format for reporting MS proteomics results
- [Introduction to Mass Spectrometry Data Analysis](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/lcms-dataprocessing/tutorial.html) - Overview of MS data processing
- [DIA Data Analysis Overview](https://www.sciencedirect.com/science/article/pii/S1535947624000902) - Review of DIA analysis methods

#### Configuration Files in R
- [INI Files in R](https://cran.r-project.org/web/packages/ini/index.html) - Working with INI configuration files
- [configr Package](https://cran.r-project.org/web/packages/configr/vignettes/configr.html) - Advanced configuration management

#### Finding Taxonomy IDs
- [NCBI Taxonomy Browser](https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi) - Look up taxonomic IDs for different organisms
- [UniProt Taxonomy](https://www.uniprot.org/taxonomy) - Alternative source for organism taxonomy information

#### UniProt ID Mapping
- [UniProt ID Mapping](https://www.uniprot.org/id-mapping) - UniProt ID mapping service

This section handles importing your DIA-NN results and FASTA file, and setting up essential parameters like taxonomy ID that will be used throughout the analysis. Make sure to download your organism's FASTA file from UniProt and find its correct taxonomy ID before proceeding.
```{r 4 Data Management}
## Input Parameters for Quality Control
## Parameters in this section are experiment-specific. Their default parameters
## are intended as a guide only - every source of variance is different just as
## every set of proteins going through a mass spectrometer is different!
## One size does not fit all and you *will* most likely need to fine tune these
## to get the most out of your data.
config_list <- readConfigFile(file = file.path(project_dirs$proteomics$base_dir, "config.ini"))

# Annotation Management
## Please download the organism fasta file from UniProt. If UniProt is not
## available, the program will extract the relevant identifiers from the fasta
## provided and attempt to match them to user supplied UniProt / UniParc
## conversions
## Please set the name of your fasta file here in the root directory if you
## already have it
DIANN_filename <- "report.tsv" ## copy to /data/proteomics in your project directory
fasta_filename <- "fasta.fasta" ## copy to /data/UniProt in your project directory
uniprot_search_results <- NULL ## copy to /data/UniProt in your project directory eg "idmapping.tsv"
uniparc_search_results <- NULL ## copy to /data/UniProt in your project directory eg "idmapping.tsv"
## Please supply your organism's taxon ID here
taxon_id <-  # You can find this at the links above.
## Please supply your organism's name here
organism_name <- "" # If you don't know this by now, you have bigger problems than this workflow!

data_tbl <- vroom::vroom(file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$data_dir, DIANN_filename))
fasta_file_path <-file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$uniprot_annotation_dir, fasta_filename)

config_list[["globalParameters"]][["fasta_file"]] <- fasta_filename
config_list[["globalParameters"]][["peptides_input_file"]] <- DIANN_filename

# Load search results if files exist
if (!is.null(uniprot_search_results) && !is.null(uniparc_search_results)) {
  uniprot_search_results <- vroom::vroom(
    file.path(project_dirs$proteomics$uniprot_annotation_dir, uniprot_search_results)
  )
  uniparc_search_results <- vroom::vroom(
    file.path(project_dirs$proteomics$uniprot_annotation_dir, uniparc_search_results)
  )
  }
```


# Set your design matrix (for the first time)

### Further Reading
#### Experimental Design in Proteomics
- [Experimental Design for Mass Spectrometry](https://www.nature.com/articles/s41596-021-00566-6) - Guidelines for designing MS experiments
- [Statistical Considerations in Proteomics](https://link.springer.com/protocol/10.1007/978-1-60761-987-1_16) - Statistical aspects of proteomics experimental design
- [DIA Experimental Design](https://pubs.rsc.org/en/content/articlehtml/2021/mo/d0mo00072h) - Special considerations for DIA-MS studies

#### Design Matrices in R
- [Design Matrices Explained]( https://www.r-bloggers.com/2023/11/understanding-matrices-in-r-programming/) - Understanding R design matrices
- [R for Data Science: Model Basics](https://r4ds.had.co.nz/model-basics.html) - Explanation of model matrices in R
- [limma User Guide](https://bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf) - Details on design matrices for differential expression

#### Sample Organization 
- [Batch Effects in Omics Data](https://www.nature.com/articles/nrg2825) - Understanding and addressing batch effects


This section helps you create a design matrix that links your samples to experimental conditions. The design matrix is a fundamental component that defines how your samples relate to each other and which comparisons will be possible in your analysis. MultiScholaR provides an interactive applet to help you build this matrix easily.
```{r 5 Design Matrix Setup} 
if (exists("design_matrix", envir = .GlobalEnv)) {
  print("Design matrix already set :) No need to run app again!")
} else {
      RunApplet(applet_type = "designMatrix"
                , omic_type = "proteomics"
                , experiment_label = experiment_label
                , force = FALSE)
}
# Comment in if you wish to run manually
# RunApplet(applet_type = "designMatrix"
                #, omic_type = "proteomics"
                #, experiment_label = experiment_label
                #, force = FALSE)
```

# If you have the design matrix stored from a previous run, you can read it in here, otherwise skip

### Further Reading
#### Data Import in R
- [Data Import with R](https://r4ds.had.co.nz/data-import.html) - Comprehensive guide to importing data
- [readr Package](https://readr.tidyverse.org/) - Modern data import functions
- [vroom Package](https://vroom.r-lib.org/) - Fast reading of delimited files

#### File Path Management
- [File Paths in R](https://www.r4epi.com/file-paths/) - Best practices for handling file paths
- [Using file.path() Function](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/file.path) - Platform-independent path construction

#### Working with Tables in R
- [R Data Import/Export](https://cran.r-project.org/doc/manuals/r-release/R-data.html) - Official R documentation on data import
- [Data Table Formats](https://www.datacamp.com/community/tutorials/r-data-import-tutorial) - Overview of various table formats in R

This optional section allows you to read in a previously created design matrix / cleaned data matrix rather than creating one from scratch. This is useful if you're re-running the analysis or if you've prepared your design matrix in a different way.

```{r 6 Design Matrix Read In (optional)}
design_matrix <- read.table(
  file = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$source_dir, "design_matrix.tab"),
  sep = "\t",
  header = TRUE,
  stringsAsFactors = FALSE
)

data_cln <- vroom::vroom(
  file = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$source_dir, "data_cln.tab"),
  delim = "\t",
  col_types = cols(.default = col_guess()),
  na = c("", "NA")
)
```

# Convert the protein identifiers to Uniprot or Uniparc accessions if those annotations are available 
## Otherwise makes use of pre-supplied uniprot fasta annotations

### Further Reading
#### Protein Identifiers and Databases
- [UniProt Knowledgebase](https://www.uniprot.org/help/uniprotkb) - Comprehensive resource for protein sequence and annotation data
- [UniParc](https://www.uniprot.org/help/uniparc) - UniProt Archive for non-redundant sequence database
- [Protein Accession Numbers Explained](https://www.uniprot.org/help/accession_numbers) - Understanding different protein accession formats

#### Protein Sequence Analysis
- [Sequence Analysis Tools](https://www.ebi.ac.uk/Tools/sss/) - Tools for sequence similarity searches
- [Bioconductor for Proteomics](https://bioconductor.org/packages/release/BiocViews.html#___Proteomics) - R/Bioconductor tools for proteomics
- [Introduction to Biostrings](https://bioconductor.org/packages/release/bioc/vignettes/Biostrings/inst/doc/BiostringsQuickOverview.pdf) - Working with biological sequences in R

#### FASTA File Processing
- [seqinr Package](https://cran.r-project.org/web/packages/seqinr/index.html) - Biological sequences retrieval and analysis
- [Parsing FASTA Files in R](https://bioinformaticschool.com/fasta-files-bioinformatics-guide/) - Different approaches to reading FASTA files
- [rentrez Package](https://cran.r-project.org/web/packages/rentrez/vignettes/rentrez_tutorial.html) - Accessing NCBI resources programmatically

This section converts protein identifiers in your dataset to standardized UniProt or UniParc accessions, which are essential for accurate protein identification and downstream functional analysis. This step ensures consistent annotation and allows for better integration with protein databases and pathways.
```{r 7 Protein ID Conversion}
fasta_meta_file <- "parsed_fasta_data.rds"
aa_seq_tbl_final <- processFastaFile(
  fasta_file_path,
  uniprot_search_results,
  uniparc_search_results,
  fasta_meta_file,
  organism_name
)
data_cln <- updateProteinIDs(data_cln, aa_seq_tbl_final)
```

## Create the PeptideQuantitativeData object
### This section initializes a PeptideQuantitativeData object with peptide-level 
### quantitative data and experimental design information.
### It specifies the column names for various data attributes and sets up the 
### design matrix for the experiment.

### Further Reading
#### S4 Objects in R
- [Introduction to S4 Objects](https://bioconductor.org/help/course-materials/2017/Zurich/S4-classes-and-methods.html) - Understanding S4 object-oriented programming in R
- [S4 Classes in Bioconductor](https://bioconductor.org/packages/release/bioc/vignettes/S4Vectors/inst/doc/S4QuickOverview.pdf) - Overview of S4 vectors in Bioconductor
- [Methods and Classes in R](https://adv-r.hadley.nz/s4.html) - Advanced R guide to S4 system

#### Proteomics Data Structures
- [MSnbase: MS Data Structures](https://www.bioconductor.org/packages/release/bioc/vignettes/MSnbase/inst/doc/MSnbase-development.html) - Data structures for MS proteomics
- [Tidy Proteomics](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-023-05360-7) - Modern approaches to proteomics data in R
- [ProteomicsAnnotationHubData](https://github.com/lgatto/ProteomicsAnnotationHubData) - Resources for proteomics annotation

#### Peptide Quantification in DIA
- [MaxLFQ Algorithm](https://www.sciencedirect.com/science/article/pii/S1535947620333107) - Label-free quantification in proteomics
- [Proteotypic Peptides](https://pubmed.ncbi.nlm.nih.gov/27975286/) - Understanding proteotypic peptides in proteomics

This section creates a structured S4 object to hold your peptide data with all the necessary metadata. By organizing your data in this object, ProteomeScholaR can easily access and manipulate the data in subsequent analysis steps, while maintaining the relationship between peptides, proteins, and experimental conditions.

```{r 8 Peptide Data S4 Object Creation}
peptide_data <- new(
  "PeptideQuantitativeData"

  # Protein vs Sample quantitative data
  ,
  peptide_data = data_cln,
  protein_id_column = "Protein.Ids",
  peptide_sequence_column = "Stripped.Sequence",
  q_value_column = "Q.Value",
  global_q_value_column = "Global.Q.Value",
  proteotypic_peptide_sequence_column = "Proteotypic",
  raw_quantity_column = "Precursor.Quantity",
  norm_quantity_column = "Precursor.Normalised",
  is_logged_data = FALSE

  # Design Matrix Information
  , design_matrix = design_matrix,
  sample_id = "Run",
  group_id = "group",
  technical_replicate_id = "replicates",
  args = config_list
)
```

# Raw Data QC

### Further Reading
#### Quality Control in Proteomics
- [QC in Proteomics Analysis](https://pubs.acs.org/doi/10.1021/acs.jproteome.4c00363) - Comprehensive overview of QC in proteomics
- [Quality Metrics for LC-MS](https://www.sciencedirect.com/science/article/pii/S1535947620337956) - Metrics for evaluating LC-MS performance

#### Data Visualization for QC
- [ggplot2 Documentation](https://ggplot2.tidyverse.org/) - Tool used for visualization in this workflow
- [Data Visualization in R](https://socviz.co/) - Comprehensive guide to data visualization
- [Interactive Visualizations with plotly](https://plotly.com/r/) - Creating interactive plots for data exploration

#### Filtering Strategies
- [Proteomics Data Filtering]( https://www.r-bloggers.com/2018/08/proteomics-data-analysis-2-3-data-filtering-and-missing-value-imputation/) - Approaches to filtering proteomics data
- [DIA Filtering Considerations](https://www.nature.com/articles/s41592-022-01639-4) - Special considerations for DIA data filtering

#### QC Metrics Tracking
- [Longitudinal QC Monitoring](https://pubmed.ncbi.nlm.nih.gov/28483925/) - Tracking metrics across experiments

This section begins the quality control process by examining your raw data and establishes a baseline for subsequent filtering steps. Each `updateProteinFiltering()` call throughout the workflow creates a checkpoint that allows you to track the effects of each filtering step on your dataset. This systematic approach lets you monitor how many peptides and proteins remain after each filter, helping you understand the impact of quality thresholds and optimize your data processing strategy. The visualization tools create plots showing the effect of each filtering step, providing documentation for your methods section and helping identify potential issues in your analysis pipeline.

```{r 9 Raw Data QC}
updateProteinFiltering( # Please note that these images don't render properly in RStudio, please see the publication_graphs_dir for the images
  data = data_cln,
  step_name = "1_Raw Data",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Filter peptides based on q-value and proteotypic peptide match
### NB should be left to default unless you have specific experimental needs

### Further Reading
#### False Discovery Rate and q-values
- [Understanding q-values and FDR](https://totallab.com/resources/p-values-fdr-q-values/) - Statistical basis for q-values in proteomics
- [FDR Control in Proteomics](https://www.bioinfor.com/fdr-tutorial/) - Methods for controlling false discovery rate
- [Target-Decoy Strategy](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2922680/) - The basis for FDR estimation in proteomics

#### Peptide-to-Protein Inference
- [Protein Inference Challenges](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3118629/) - Challenges in mapping peptides to proteins
- [Inference Algorithms](https://www.mcponline.org/article/S1535-9476(20)33526-9/fulltext) - Methods for peptide-to-protein mapping
- [Shared Peptides](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5891066/) - Handling shared peptides across proteins

This step filters your dataset based on the statistical confidence of peptide identifications (q-value) and ensures that only peptides uniquely mapping to a specific protein (proteotypic) are retained. This is crucial for reliable protein quantification, as it reduces false positives and ambiguous peptide assignments that could lead to inaccurate protein abundance estimates. The `updateProteinFiltering()` function at the end visualizes the effect of this filtering step compared to the raw data.

```{r 10 Filter Peptides on q Value and Proteotypic Peptide Match}
search_srl_quant_cln <- srlQvalueProteotypicPeptideClean(theObject = peptide_data)
qval_protein_count <- search_srl_quant_cln@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after q-value and proteotypic filtering:", qval_protein_count))

# Example: Interactively change q-value thresholds and re-run this chunk
# peptide_data <- updateConfigParameter(
#   theObject      = peptide_data, # Use the input object from the previous step
#   function_name  = "srlQvalueProteotypicPeptideClean",
#   parameter_name = "qvalue_threshold",               # Effect: Lower = stricter peptide ID confidence
#   new_value      = NEW_VALUE_HERE
# )
# peptide_data <- updateConfigParameter(
#   theObject      = peptide_data,
#   function_name  = "srlQvalueProteotypicPeptideClean",
#   parameter_name = "global_qvalue_threshold",        # Effect: Lower = stricter protein group ID confidence
#   new_value      = NEW_VALUE_HERE
# )
# peptide_data <- updateConfigParameter(
#   theObject      = peptide_data,
#   function_name  = "srlQvalueProteotypicPeptideClean",
#   parameter_name = "choose_only_proteotypic_peptide",# Effect: 1 = Keep only unique peptides, 0 = Keep shared peptides
#   new_value      = NEW_VALUE_HERE
# )
# search_srl_quant_cln <- srlQvalueProteotypicPeptideClean(theObject = peptide_data) # Re-run after update

updateProteinFiltering(
  data = search_srl_quant_cln@peptide_data,
  step_name = "2_qval Filtered",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Roll-up of precursor ions to peptide level intensity value quantitation

### Further Reading
#### Precursor-to-Peptide Aggregation
- [Precursor Ion Quantification](https://pmc.ncbi.nlm.nih.gov/articles/PMC3804902/) - Methods for quantifying precursor ions
- [Precursor Ion Selection](https://pmc.ncbi.nlm.nih.gov/articles/PMC5557716/) - Selection strategies for precursor ions

#### Data Aggregation Methods
- [Data Aggregation in Proteomics](https://pubmed.ncbi.nlm.nih.gov/39198030/) - Methods for data aggregation

#### Isotopologue Patterns
- [Isotope Patterns in MS](https://www.sciencedirect.com/science/article/pii/S2667145X24000282) - Understanding isotopic patterns
- [Isotopologue Analysis](https://www.thermofisher.com/blog/proteomics/the-isotopologue/) - Methods for analyzing isotopologue data
- [MS1 Quantification](https://pmc.ncbi.nlm.nih.gov/articles/PMC6849792/) - MS1-based quantification approaches

This step aggregates intensity measurements from multiple precursor ions (different charge states, isotopes, and/or modified forms) to the peptide level. This roll-up process is critical for accurate quantification as it combines the signal from all observed forms of the same peptide sequence, providing a more robust measurement of peptide abundance. The function applies statistical methods to appropriately combine these measurements while minimizing the effect of outliers or noisy precursors. The subsequent `updateProteinFiltering()` call tracks how this roll-up affects the number of unique proteins identified in your dataset.

```{r 11 Roll-up Precursor to Peptide}
peptide_normalized_tbl <- rollUpPrecursorToPeptide(search_srl_quant_cln)
rollup_protein_count <- peptide_normalized_tbl@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins after precursor roll-up to peptide level:", rollup_protein_count))

updateProteinFiltering(
  data = peptide_normalized_tbl@peptide_data,
  step_name = "3_peptidoform count",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Remove peptide based on the intensity threshold and the proportion of samples 
## below the threshold
### NB should be left to default unless you have specific experimental needs

### Further Reading
#### Intensity Thresholding
- [Signal-to-Noise in Proteomics](https://link.springer.com/protocol/10.1007/978-1-61779-573-2_4) - Understanding signal thresholds
- [Intensity-Based Filtering](https://pmc.ncbi.nlm.nih.gov/articles/PMC2842913/) - Methods for intensity-based filters
- [Dynamic Range in Proteomics](https://pubmed.ncbi.nlm.nih.gov/23307342/) - Dealing with wide dynamic ranges

#### Missing Values in Proteomics
- [Missing Value Patterns](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/pmic.202200092) - Understanding missing data in proteomics
- [Missing Value Handling Strategies](https://www.bioconductor.org/packages/devel/bioc/vignettes/DEP/inst/doc/MissingValues.html) - Approaches for missing value management
- [Missing Not At Random (MNAR)](https://www.nature.com/articles/s41598-017-19120-0) - Statistical treatment of different missing value types

#### Threshold Determination
- [Optimal Threshold Selection](https://www.mcponline.org/article/S1535-9476(20)33371-4/fulltext) - Methods for determining optimal thresholds
- [Data-Driven Thresholding](https://pubs.acs.org/doi/10.1021/acs.jproteome.8b00377) - Approaches to data-dependent filtering
- [Impact of Filtering on Statistical Power](https://www.mcponline.org/article/S1535-9476(20)31134-8/fulltext) - Effect of filtering on statistical analysis

This step removes peptides with low intensity values or those that are detected in too few samples. Low-intensity peptides often have poor quantification accuracy and can introduce noise into the dataset. By setting an intensity threshold and requiring peptides to be detected in a minimum number of samples, this filter improves the reliability of downstream quantification and statistical analysis. The step is crucial for reducing false discoveries that could result from spurious low-intensity signals. As with previous steps, the `updateProteinFiltering()` function tracks the effect of this filter on your dataset.

```{r 12 Filter Peptides on Intensity Threshold}
peptide_normalized_pif_cln <- peptideIntensityFiltering(
  theObject = peptide_normalized_tbl
)
pif_protein_count <- peptide_normalized_pif_cln@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after peptide intensity/proportion filtering:", pif_protein_count))

# Example: Interactively change intensity filtering thresholds and re-run this chunk
# peptide_normalized_tbl <- updateConfigParameter(
#   theObject      = peptide_normalized_tbl, # Use the input object from the previous step
#   function_name  = "peptideIntensityFiltering",
#   parameter_name = "peptides_intensity_cutoff_percentile", # Effect: Higher = stricter intensity threshold (removes more)
#   new_value      = NEW_VALUE_HERE
# )
# peptide_normalized_tbl <- updateConfigParameter(
#   theObject      = peptide_normalized_tbl,
#   function_name  = "peptideIntensityFiltering",
#   parameter_name = "peptides_proportion_of_samples_below_cutoff", # Effect: Higher = more relaxed filtering (keeps more)
#   new_value      = NEW_VALUE_HERE
# )
# peptide_normalized_pif_cln <- peptideIntensityFiltering(theObject = peptide_normalized_tbl) # Re-run after update

updateProteinFiltering(
  data = peptide_normalized_pif_cln@peptide_data,
  step_name = "4_peptideIntensityFiltering",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Keep the proteins only if they have two or more peptides mapping
### NB should be left to default unless you have specific experimental needs 
#### CHANGE IF YOU ARE INTERESTED IN QUANTIFYING SINGLE PEPTIDE PROTEIN MATCHES

### Further Reading
#### Multiple Peptide Rule
- [Two-Peptide Rule in Proteomics](https://pubmed.ncbi.nlm.nih.gov/19627159/) - Rationale for requiring multiple peptides
- [Protein Inference Best Practices](https://www.halolabs.com/blog/protein-identification-and-characterization-guide/) - Guidelines for protein identification

#### Special Cases
- [Short Proteins and Small ORFs]( https://jbiomedsci.biomedcentral.com/articles/10.1186/s12929-022-00802-5) - Approaches for small proteins with few peptides

This step filters proteins based on the number of unique peptides identified for each protein, keeping only those with at least two peptides. This "two-peptide rule" is a widely accepted standard in proteomics that greatly increases confidence in protein identifications and quantification reliability. While this filter may remove some true protein identifications (especially for small proteins or those with low sequence coverage), it significantly reduces false positive identifications. As the comment suggests, you may consider changing this threshold if you have specific interest in proteins that might only be identified by a single peptide, but be aware of the increased uncertainty associated with such identifications.

```{r 13 Filter Proteins on Peptide Number}
removed_proteins_with_less_than_two_peptides <- filterMinNumPeptidesPerProtein(
  theObject = peptide_normalized_pif_cln
)
twopep_protein_count <- removed_proteins_with_less_than_two_peptides@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins identified by >= 2 peptides:", twopep_protein_count))

# Example: Interactively change minimum peptide/peptidoform count and re-run this chunk
# peptide_normalized_pif_cln <- updateConfigParameter(
#   theObject      = peptide_normalized_pif_cln, # Use the input object from the previous step
#   function_name  = "filterMinNumPeptidesPerProtein",
#   parameter_name = "num_peptides_per_protein_thresh", # Effect: Higher = requires more unique peptides per protein
#   new_value      = NEW_VALUE_HERE
# )
# peptide_normalized_pif_cln <- updateConfigParameter(
#   theObject      = peptide_normalized_pif_cln,
#   function_name  = "filterMinNumPeptidesPerProtein",
#   parameter_name = "num_peptidoforms_per_protein_thresh", # Effect: Higher = requires more peptide forms per protein (more stringent)
#   new_value      = NEW_VALUE_HERE
# )
# removed_proteins_with_less_than_two_peptides <- filterMinNumPeptidesPerProtein(theObject = peptide_normalized_pif_cln) # Re-run after update

updateProteinFiltering(
  data = removed_proteins_with_less_than_two_peptides@peptide_data,
  step_name = "5_twopeptidesperprotein",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Remove samples with insufficient peptide counts
#### This section filters out samples that have fewer than a specified minimum 
#### number of peptides - ie poor sample performance

### Further Reading
#### Sample Quality Assessment
- [Sample Quality Metrics in Proteomics](https://www.creative-proteomics.com/resource/sample-quality-control-in-omics-research.htm) - Methods for assessing sample quality

#### Sample Exclusion Criteria
- [Outlier Detection in Proteomics](https://academic.oup.com/bib/article/25/3/bbae129/7638267) - Methods for detecting outlier samples

#### Technical Considerations
- [Sample Preparation Impact](https://pmc.ncbi.nlm.nih.gov/articles/PMC4817721/) - Effects of sample preparation on peptide recovery

This step identifies and removes samples with poor performance, indicated by an unusually low number of identified peptides. Such samples may be compromised due to issues in sample preparation, instrument performance, or other technical factors. Including these low-quality samples in downstream analysis could introduce bias and reduce the statistical power to detect true biological differences. The minimum peptide threshold ensures that all samples have sufficient protein coverage for reliable quantification, while the summary statistics at the end help evaluate which samples were removed and why.

```{r 14 Filter Samples on Peptide Number}
peptide_keep_samples_with_min_num_peptides <- filterMinNumPeptidesPerSample(
  theObject = removed_proteins_with_less_than_two_peptides
)
minsamp_protein_count <- peptide_keep_samples_with_min_num_peptides@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after removing low-peptide-count samples:", minsamp_protein_count))
message(paste(
  "Number of distinct runs remaining:",
  peptide_keep_samples_with_min_num_peptides@peptide_data |> distinct(Run) |> nrow()
))
message("Peptidoform counts per remaining run:")
print(
  peptide_keep_samples_with_min_num_peptides@peptide_data |>
    distinct(Run, Protein.Ids, Stripped.Sequence, peptidoform_count) |>
    group_by(Run) |>
    summarise(n = sum(peptidoform_count)) |>
    arrange(desc(n))
)

# Example: Interactively change minimum peptides per sample cutoff and re-run this chunk
#removed_proteins_with_less_than_two_peptides <- updateConfigParameter(
#   theObject      = removed_proteins_with_less_than_two_peptides, # Use the input object from the previous step
#   function_name  = "filterMinNumPeptidesPerSample",
#   parameter_name = "peptides_per_sample_cutoff", # Effect: Higher = stricter sample quality filter (removes more samples)
#   new_value      = new_value_here
#)
# peptide_keep_samples_with_min_num_peptides <- #filterMinNumPeptidesPerSample(theObject = #removed_proteins_with_less_than_two_peptides) # Re-run after update

updateProteinFiltering(
  data = peptide_keep_samples_with_min_num_peptides@peptide_data,
  step_name = "6_minpeppersample",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Remove peptides with only one replicate in the data set
### This section filters out peptides that appear in only one replicate across all groups.
### Ensures that the analysis is based on peptides with consistent detection across multiple replicates.

### Further Reading
#### Reproducibility in Peptide Detection
- [Reproducibility in Proteomics](https://www.tandfonline.com/doi/full/10.1080/14789450.2024.2320166) - Factors affecting reproducible detection

#### Replicate Analysis
- [Technical Replicates in Proteomics](https://www.tandfonline.com/doi/full/10.1080/14789450.2024.2320166) - Value of technical replicates

#### Data Quality Enhancement
- [Improving Data Reliability](https://www.sciencedirect.com/science/article/pii/S1535947620348787) - Methods for enhancing data reliability
- [Balancing Sensitivity and Reliability](https://pubmed.ncbi.nlm.nih.gov/20101609/ ) - Tradeoffs in proteomics data analysis

This step removes peptides that appear in only one replicate, focusing the analysis on peptides that demonstrate consistent detectability across multiple samples. Peptides detected in just a single replicate often represent random noise, contamination, or false identifications. Removing these inconsistently detected peptides improves the reliability of quantification and increases confidence in biological conclusions. This filter strikes a balance between maintaining sufficient peptide coverage and ensuring reproducible measurements across replicates.

```{r 15 Filter Peptides on Replicate Number}
removed_peptides_with_only_one_replicate <- removePeptidesWithOnlyOneReplicate(
  peptide_keep_samples_with_min_num_peptides,
  replicate_group_column = peptide_keep_samples_with_min_num_peptides@technical_replicate_id
)
onerep_protein_count <- removed_peptides_with_only_one_replicate@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after removing single-replicate peptides:", onerep_protein_count))

# Example: Interactively change replicate filtering and re-run this chunk
# peptide_keep_samples_with_min_num_peptides <- updateConfigParameter(
#   theObject      = peptide_keep_samples_with_min_num_peptides, # Use the input object from the previous step
#   function_name  = "removePeptidesWithOnlyOneReplicate",
#   parameter_name = "replicate_group_column", # Effect: Change the column used for replicate filtering
#   new_value      = NEW_VALUE_HERE
# )
# peptide_keep_samples_with_min_num_peptides <- removePeptidesWithOnlyOneReplicate(peptide_keep_samples_with_min_num_peptides, replicate_group_column = NEW_VALUE_HERE) # Re-run after update


updateProteinFiltering(
  data = removed_peptides_with_only_one_replicate@peptide_data,
  step_name = "7_peptidesonerep",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Missing values "imputation" using technical replicates
### This section imputes missing values using the average of technical replicate samples.
### The imputation is performed if there's a high proportion of technical replicate samples with non-missing values.
### By default this section is commented out. Comment back in if technical replicates are included in the experimental data.

This step addresses missing values in the dataset by using information from technical replicates. When a value is missing in one replicate but present in others from the same sample, this approach imputes the missing value using the average of the available measurements. This strategy is preferable to more aggressive imputation methods because it relies on actual measurements from the same biological sample rather than theoretical models. The approach is conservative, only imputing when there's sufficient evidence from technical replicates, which helps maintain data integrity while reducing the negative impact of missing values on statistical analysis.

```{r 16 Missing Value Imputation}
peptide_values_imputed <- peptideMissingValueImputation(
  theObject = removed_peptides_with_only_one_replicate
)
impute_protein_count <- peptide_values_imputed@peptide_data |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins after peptide-level imputation:", impute_protein_count))

# Example: Interactively change imputation threshold and re-run this chunk
# removed_peptides_with_only_one_replicate <- updateConfigParameter(
#   theObject      = removed_peptides_with_only_one_replicate, # Use the input object from the previous step
#   function_name  = "peptideMissingValueImputation",
#   parameter_name = "proportion_missing_values", # Effect: Lower = imputes more readily (less stringent)
#   new_value      = NEW_VALUE_HERE
# )
# peptide_values_imputed <- peptideMissingValueImputation(theObject = removed_peptides_with_only_one_replicate) # Re-run after update


peptide_values_imputed_file <- file.path(
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$peptide_qc_dir,
  "peptide_values_imputed.tsv"
)

vroom::vroom_write(
  peptide_values_imputed@peptide_data |>
    mutate(
      Q.Value = 0.0009,
      PG.Q.Value = 0.009
    ) |>
    mutate(
      Peptide.Imputed = ifelse(is.na(Peptide.Imputed), 0, Peptide.Imputed)
    ),
  peptide_values_imputed_file
)
```

## Read in the fasta organism specific fasta file to extract details on the protein sequences
### There should be no need to change this chunk, ever

## This section aggregates the long-format peptide-level intensity values into protein-level quantification:
### It uses the IQ tool (https://github.com/tvpham/iq), which implements the same algorithm as DIA-NN's maxLFQ but runs faster (written in C++) 
### Unless your experiment has specific requirements or you wish to use IQ differently, it is recommended to leave default settings here

### Further Reading

#### IQ Tool
- [IQ GitHub Repository](https://github.com/tvpham/iq) - Source code and documentation for the IQ tool

This section aggregates peptide-level measurements to obtain protein-level quantification, a critical step in the proteomics workflow. The IQ tool implements the widely-used MaxLFQ algorithm, which provides accurate relative quantification of proteins across samples while accounting for peptide-level variability. The algorithm uses a graph-based approach to derive the most consistent protein ratios, even when different peptides are detected across samples. The resulting protein quantification values are logged (log2), which helps normalize the data and makes it suitable for statistical analysis. The step concludes by checking that the number of proteins matches expectations, ensuring no data is lost during the rollup process.

```{r 17 Peptide to Protein Rollup}
iq::process_long_format(
  peptide_values_imputed_file,
  output_filename = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "iq_output_file.txt"),
  sample_id = "Run",
  primary_id = "Protein.Ids",
  secondary_id = "Stripped.Sequence",
  intensity_col = "Peptide.Imputed",
  filter_double_less = c("Q.Value" = "0.01", "PG.Q.Value" = "0.01")
  ## very important for this workflow that you do NOT perform normalization here
  , normalization = "none"
)

## Read in the IQ output file (which outputs a file, not an object)
protein_log2_quant <- vroom::vroom(
  file.path(file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "iq_output_file.txt")))
```

## Create Protein Quantitative Data Object
### Unless you have changed the column identifiers or the object names leave defaults

### Further Reading
#### S4 Object-Oriented Programming
- [Object-Oriented Design](https://www.biostat.jhsph.edu/~rpeng/docs/R-classes-scope.pdf) - Principles of object-oriented design in R

#### Proteomics Data Structures
- [Data Representation in Proteomics](https://pmc.ncbi.nlm.nih.gov/articles/PMC4457114/) - Standards for proteomics data representation
- [ExpressionSet in Bioconductor](https://www.bioconductor.org/packages/release/bioc/vignettes/Biobase/inst/doc/ExpressionSetIntroduction.pdf) - Classical data structure for expression data
- [SummarizedExperiment Class](https://bioconductor.org/packages/release/bioc/vignettes/SummarizedExperiment/inst/doc/SummarizedExperiment.html) - Modern standard for omics data

#### Experimental Design Integration
- [Metadata in Proteomics](https://pmc.ncbi.nlm.nih.gov/articles/PMC7116434/) - Handling sample metadata in proteomics
- [Reproducible Analysis Frameworks](https://www.nature.com/articles/s41467-022-32155-w) - Frameworks for reproducible analysis

This step creates a structured S4 object to organize the protein quantification data alongside experimental metadata. This object-oriented approach encapsulates all the protein quantification values with their associated sample information and experimental design, making subsequent analysis more robust and reproducible. The object provides methods for data access and manipulation that maintain the integrity of the relationship between quantitative data and experimental design. This structured approach is particularly important for complex proteomics experiments where keeping track of sample groupings, technical replicates, and experimental conditions is crucial for valid statistical analysis.

```{r 18 Protein Data S4 Object Creation}
protein_obj <- ProteinQuantitativeData(
  # Protein Data Matrix Information
  protein_quant_table = protein_log2_quant,
  protein_id_column = "Protein.Ids",
  protein_id_table = protein_log2_quant |> distinct(Protein.Ids), 
  # Design Matrix Information
  design_matrix = peptide_values_imputed@design_matrix,
  sample_id = "Run",
  group_id = "group",
  technical_replicate_id = "replicates",
  args = peptide_values_imputed@args
)
```

## Arrange the protein ID's list to opt for the best accession in the list to be placed first
## Skips this section if you have supplied your own Uniprot accession searches

### Further Reading
#### Protein Accession Standards

#### Handling Multiple Accessions
- [Protein Inference](https://www.sciencedirect.com/science/article/pii/S1535947620341219) - Statistical approaches to protein inference
- [Ambiguous Protein Assignment](https://www.sciencedirect.com/science/article/pii/S1535947620335040) - Dealing with ambiguous peptide-to-protein mapping

#### Database Curation
- [Protein Database Quality](https://bigomics.ch/blog/guide-to-top-proteomics-databases-and-how-to-access-them/) - Impact of database quality on proteomics
- [Swiss-Prot vs TrEMBL](https://www.uniprot.org/help/uniprotkb_sections) - Differences between reviewed and unreviewed entries
- [Proteogenomics Approaches](https://pmc.ncbi.nlm.nih.gov/articles/PMC4991544/) - Integrating proteomics with genomics

This step ensures that protein accessions are standardized and prioritized according to relevant biological criteria. In many cases, peptides may map to multiple protein accessions, and this function helps choose the most informative or canonical accession to represent each protein. The priority is typically given to reviewed UniProt entries (Swiss-Prot), conserved entries, or entries with more complete annotation. This standardization is essential for downstream analyses like pathway analysis and functional annotation. The `updateProteinFiltering()` function shows how many proteins remain after this cleanup step.

```{r 19 Most Likely Protein Accession Rollup}
if (is.null(uniprot_search_results)) {
  aa_seq_tbl_final <- aa_seq_tbl_final |>
    dplyr::rename(uniprot_acc = database_id)

  protein_log2_quant_cln <- chooseBestProteinAccession(
    theObject = protein_obj,
    delim = ";",
    seqinr_obj = aa_seq_tbl_final,
    seqinr_accession_column = "uniprot_acc",
    replace_zero_with_na = TRUE,
    aggregation_method = "mean"
  )
  annotclean_protein_count <- protein_log2_quant_cln@protein_quant_table |>
    distinct(Protein.Ids) |>
    nrow()
  message(paste("Number of distinct proteins after choosing best protein accession:", annotclean_protein_count))

  updateProteinFiltering(
    data = protein_log2_quant_cln@protein_quant_table,
    step_name = "8_annotation_cleanup",
    omic_type = omic_type, # e.g., "proteomics"
    experiment_label = experiment_label, # e.g., "workshop_data"
    return_grid = TRUE,
    overwrite = TRUE
  )
} else {
  protein_log2_quant_cln <- protein_obj
}
```

## Helper function to set your filtering parameters to allow for the minimum 
## number of values/groups to be present for quantitation

### Further Reading

#### Filtering Strategies
- [Sample Coverage Thresholds](https://www.mcponline.org/article/S1535-9476(20)31141-5/fulltext) - Setting thresholds for sample coverage
- [Group-Based Filtering](https://www.sciencedirect.com/science/article/pii/S1535947620313979) - Filtering based on experimental groups
- [Balancing Sensitivity and Reproducibility](https://www.nature.com/articles/s41592-019-0365-3) - Trade-offs in filtering decisions

#### Statistical Considerations
- [Statistical Power and Missing Data](https://academic.oup.com/bioinformatics/article/32/15/2383/1743989) - Effect of missing data on statistical power
- [Sensitivity Analysis for Missing Values](https://www.sciencedirect.com/science/article/pii/S1874391913005940) - Testing sensitivity to missing value handling
- [Group Size Considerations](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3638073/) - How group size affects missing value impacts

This step configures the parameters for handling missing values in your dataset. The function sets thresholds for the minimum number of valid measurements required within each experimental group and the minimum number of groups that must have these measurements. These parameters are crucial for balancing data completeness with proteome coverage - stricter thresholds provide more reliable quantification but can reduce the number of proteins analyzed. The settings are typically determined based on your experimental design, with larger groups often allowing for more stringent filtering. These parameters will be used in subsequent filtering steps to remove proteins with excessive missing values.

```{r 20 Set Missing Value Filter Parameters}
config_list <- updateMissingValueParameters(
  design_matrix,
  config_list,
  min_reps_per_group = 2,
  min_groups = 2
)
```

## Remove protein based on the intensity threshold and the proportion of samples below the threshold
### The threshold is determined by the 1% quantile of the protein intensity values
### This helps to ensure that only reliably quantified proteins are retained for further analysis

This step removes proteins with consistently low intensity values or those detected in too few samples across the experimental groups. The 1% quantile threshold helps identify a data-driven cutoff for removing unreliably quantified proteins that are at or near the noise level of the instrument. By using the `removeRowsWithMissingValuesPercent()` function, the workflow removes proteins that fall below this threshold in too many samples, making the dataset more robust for downstream statistical analysis. The `updateProteinFiltering()` function at the end tracks the effect of this filter on protein numbers and provides visualization.

```{r 21 Filter Proteins on Intensity Threshold}
protein_normalized_pif_cln <- removeRowsWithMissingValuesPercent(protein_log2_quant_cln
)
protmiss_protein_count <- protein_normalized_pif_cln@protein_quant_table |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after protein intensity/proportion filtering:", protmiss_protein_count))

# Example: Interactively change protein missing value filtering and re-run this chunk
# protein_log2_quant_cln <- updateConfigParameter(
#   theObject      = protein_log2_quant_cln, # Use the input object from the previous step
#   function_name  = "removeRowsWithMissingValuesPercent",
#   parameter_name = "groupwise_percentage_cutoff", # Effect: Lower = stricter filtering within groups (removes more)
#   new_value      = NEW_VALUE_HERE
# )
# protein_log2_quant_cln <- updateConfigParameter(
#   theObject      = protein_log2_quant_cln,
#   function_name  = "removeRowsWithMissingValuesPercent",
#   parameter_name = "max_groups_percentage_cutoff", # Effect: Lower = stricter filtering across groups (removes more)
#   new_value      = NEW_VALUE_HERE
# )
# protein_log2_quant_cln <- updateConfigParameter(
#   theObject      = protein_log2_quant_cln,
#   function_name  = "removeRowsWithMissingValuesPercent",
#   parameter_name = "proteins_intensity_cutoff_percentile", # Effect: Higher = stricter intensity threshold (removes more)
#   new_value      = NEW_VALUE_HERE
# )
# protein_normalized_pif_cln <- removeRowsWithMissingValuesPercent(protein_log2_quant_cln) # Re-run after update


updateProteinFiltering(
  data = protein_normalized_pif_cln@protein_quant_table,
  step_name = "9_protein_missingvals_percent",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Summarize data from duplicate proteins
### Calculate mean across matching duplicate proteins and populate the new identifier with a single value
### Leave as default unless you wish to perform another form of duplication handling
### Or if you have an exotic experiment that looks at very identical proteins, modifications etc you may wish to skip this.

### Further Reading
#### Handling Duplicate Identifiers
- [Protein Naming in Proteomics](https://www.ncbi.nlm.nih.gov/genbank/internatprot_nomenguide/) - Standards for protein naming
- [Dealing with Redundancy](https://www.uniprot.org/help/proteome_redundancy) - Approaches to redundant protein entries
- [Entry Consolidation Methods](https://pmc.ncbi.nlm.nih.gov/articles/PMC11966240/) - Statistical approaches to merging duplicates

#### Protein Groups and Isoforms
- [Isoform Quantification](https://pmc.ncbi.nlm.nih.gov/articles/PMC3800748/) - Challenges in quantifying protein isoforms 
- [Proteoform Analysis](https://pmc.ncbi.nlm.nih.gov/articles/PMC6602557/) - Approaches to distinguish proteoforms 

This step identifies and handles duplicate protein entries in the dataset by aggregating their quantitative values. Duplicate entries can arise from various sources, including database redundancy, protein isoforms, or ambiguous peptide-to-protein mapping. The approach taken here uses the mean value across duplicates, which is a robust way to combine measurements while preserving the overall quantitative relationship between samples. After aggregation, each protein is represented by a single entry, simplifying downstream analysis while maintaining data integrity. The final count confirms the removal of duplicates while retaining all unique proteins.

```{r 22 Remove Duplicate Proteins}
# Identify duplicates
duplicates <- protein_normalized_pif_cln@protein_quant_table |>
  dplyr::group_by(Protein.Ids) |>
  dplyr::filter(n() > 1) |>
  dplyr::select(Protein.Ids) |>
  dplyr::distinct() |>
  dplyr::pull(Protein.Ids)

duplicates

# Clean duplicates
protein_normalized_pif_cln@protein_quant_table <-
  protein_normalized_pif_cln@protein_quant_table |>
  dplyr::group_by(Protein.Ids) |>
  dplyr::summarise(
    dplyr::across(matches("\\d+"), ~ mean(.x, na.rm = TRUE))
  ) |>
  dplyr::ungroup()

protein_normalized_pif_cln@protein_quant_table |>
  dplyr::distinct(Protein.Ids) |>
  nrow()
```

## Remove proteins with only one replicate in the data set
### No need to change this unless you wish to include single replicate proteins

### Further Reading


#### Parallel Processing in R
- [Parallelization with future](https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html) - Modern parallel processing in R
- [Multicore Processing](https://www.r-bloggers.com/2018/06/parallel-processing-with-r/) - Approaches to multicore processing
- [Cluster Computing in R](https://www.rdocumentation.org/packages/parallel/versions/3.6.2/topics/makeCluster) - Creating and using clusters in R

#### Statistical Robustness
- [Power Analysis in Proteomics](https://pubmed.ncbi.nlm.nih.gov/21591257/) - Statistical power considerations 

This step removes proteins that appear in only one replicate across the experimental groups, ensuring that all proteins in the final dataset have been reproducibly detected in at least two replicates. The parallel processing approach (using multiple CPU cores) helps speed up this computation-intensive task. Filtering out single-replicate proteins is critical for statistical reliability, as proteins detected in just one replicate could represent random noise or false identifications. The output is written to a file for reference, and the `updateProteinFiltering()` function visualizes how this filter affects the number of proteins in the dataset.

```{r 23 Filter Proteins on Replicate Number}
core_utilisation <- new_cluster(4) # note to change this in the below function to cores from config_list
remove_proteins_with_only_one_rep <- removeProteinsWithOnlyOneReplicate(
  protein_normalized_pif_cln,
  core_utilisation,
  grouping_variable = "group"
)
protonerep_protein_count <- remove_proteins_with_only_one_rep@protein_quant_table |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after removing single-replicate proteins:", protonerep_protein_count))
vroom::vroom_write(
  remove_proteins_with_only_one_rep@protein_quant_table,
  file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "remove_proteins_with_only_one_rep.tsv")
)

updateProteinFiltering(
  data = remove_proteins_with_only_one_rep@protein_quant_table,
  step_name = "10_proteins_onerep_filter",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)
```

## Pre-normalisation data QC
### RLE plot 
### PCA plot
### Pearson correlation
### Spearman correlation

### Further Reading
#### Quality Control Visualization
- [Interactive Visualization Tools](https://pmc.ncbi.nlm.nih.gov/articles/PMC4510819/) - Modern approaches to interactive data visualization

#### Principal Component Analysis
- [PCA in Proteomics](https://pmc.ncbi.nlm.nih.gov/articles/PMC4676806/) - Applications of PCA in proteomics
- [Understanding PCA](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) - Step by step explanation of PCA
- [Interpreting PCA Results](https://bioturing.medium.com/how-to-read-pca-biplots-and-scree-plots-186246aae063) - How to interpret PCA plots correctly

#### Correlation Analysis
- [Correlation Measures in Omics](https://www.nature.com/articles/s43588-023-00436-z) - Different correlation metrics for biological data
- [Sample Correlation Assessment](https://www.metwarebio.com/sample-correlation-analysis/) - Using correlation to assess sample quality

#### Relative Log Expression (RLE) Plots
- [RLE for Normalization Assessment]( https://biocellgen-public.svi.edu.au/mig_2019_scrnaseq-workshop/normalization-confounders-and-batch-correction.html) - Using RLE to evaluate normalization
```{r 24 Pre-normalisation QC}
QC_composite_figure <- InitialiseGrid()

QC_composite_figure@rle_plots$rle_plot_before_cyclic_loess <- plotRle(
  remove_proteins_with_only_one_rep,
  "group",
  yaxis_limit = c(-6, 6)
)

QC_composite_figure@pca_plots$pca_plot_before_cyclic_loess_group <- plotPca(
  remove_proteins_with_only_one_rep,
  grouping_variable = "group",
  label_column = "",
  shape_variable = "group",
  title = "",
  font_size = 8
)

QC_composite_figure@density_plots$density_plot_before_cyclic_loess_group <- plotDensity(
  QC_composite_figure@pca_plots$pca_plot_before_cyclic_loess_group,
  grouping_variable = "group"
)

pca_mixomics_before_cyclic_loess <- getPcaMatrix(remove_proteins_with_only_one_rep)

QC_composite_figure@pearson_plots$pearson_correlation_pair_before_cyclic_loess <-
  plotPearson(
    remove_proteins_with_only_one_rep,
    tech_rep_remove_regex = "pool",
    correlation_group = "group"
  )

summarizeQCPlot(QC_composite_figure)

savePlot(
  QC_composite_figure@rle_plots$rle_plot_before_cyclic_loess,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "rle_plot_before_cyclic_loess"
)
savePlot(
  QC_composite_figure@pca_plots$pca_plot_before_cyclic_loess_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "pca_plot_before_cyclic_loess"
)
savePlot(
  QC_composite_figure@density_plots$density_plot_before_cyclic_loess_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "density_plot_before_cyclic_loess"
)
savePlot(
  QC_composite_figure@pearson_plots$pearson_correlation_pair_before_cyclic_loess,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "pearson_correlation_pair_before_cyclic_loess"
)

frozen_protein_matrix_tech_rep <- proteinTechRepCorrelation(
  remove_proteins_with_only_one_rep,
  tech_rep_num_column = "group",
  tech_rep_remove_regex = "pool"
)

## change if you wish to filter on pearson or spearman
frozen_protein_matrix_tech_rep |>
  dplyr::filter(pearson > 0.8) |>
  nrow()
# frozen_protein_matrix_tech_rep |>
#  dplyr::filter(spearman > 0.8) |>
#  nrow()
```

## Cyclic loess normalisation and QC

### Further Reading
#### Normalization Methods in Proteomics
- [Normalization Strategies](https://academic.oup.com/bib/article/19/1/1/2562889) - Comprehensive review of normalization methods
- [Impact of Normalization on Results](https://pmc.ncbi.nlm.nih.gov/articles/PMC2758752/) - How normalization affects biological conclusions

#### Cyclic Loess Method
- [Loess Normalization in Proteomics](https://pmc.ncbi.nlm.nih.gov/articles/PMC10318553/) - Application of loess in proteomics
- [Statistical Foundations of Loess](https://www.itl.nist.gov/div898/handbook/pmd/section1/pmd144.htm) - Mathematical background of loess smoothing

#### Batch Effect Correction
- [Batch Effect Detection and Correction](https://www.sciencedirect.com/science/article/pii/S2001037022003567) - Methods for identifying and correcting batch effects
- [Technical Variation Removal](https://pubmed.ncbi.nlm.nih.gov/14625853/) - Approaches to remove unwanted technical variation
- [Integrated Normalization Workflows](https://pmc.ncbi.nlm.nih.gov/articles/PMC9450154/) - Complete workflows for data normalization

This section applies cyclic loess normalization to your protein data and evaluates its effectiveness through various quality control visualizations. Cyclic loess is a non-linear normalization method that adjusts for intensity-dependent biases in the data by iteratively applying local regression to pairwise sample comparisons. This approach is particularly effective for proteomics data, where global scaling methods may be insufficient due to complex biases. The QC visualizations following normalization (RLE plots, PCA, density plots, and correlation analysis) help evaluate whether the normalization successfully reduced technical variation while preserving biological differences. The comparison with pre-normalization plots allows you to assess the improvement in data quality and identify any remaining batch effects or outliers that might need further attention.

```{r 25 Cyclic Loess Normalisation and QC}
normalised_frozen_protein_matrix_obj <- normaliseBetweenSamples(
  remove_proteins_with_only_one_rep,
  normalisation_method = "cyclicloess"
)

QC_composite_figure@rle_plots$rle_plot_before_ruvIIIc_group <- plotRle(
  normalised_frozen_protein_matrix_obj,
  "group",
  yaxis_limit = c(-6, 6)
)

QC_composite_figure@pca_plots$pca_plot_before_ruvIIIc_group <- plotPca(
  normalised_frozen_protein_matrix_obj,
  grouping_variable = "group",
  label_column = "",
  shape_variable = "group",
  title = "",
  font_size = 8
)

QC_composite_figure@density_plots$density_plot_before_ruvIIIc_group <- plotDensity(
  QC_composite_figure@pca_plots$pca_plot_before_ruvIIIc_group,
  grouping_variable = "group"
)

pca_mixomics_before_ruvIIIc <- getPcaMatrix(normalised_frozen_protein_matrix_obj)

QC_composite_figure@pearson_plots$pca_plot_before_ruvIIIc_group <- plotPearson(
  normalised_frozen_protein_matrix_obj,
  tech_rep_remove_regex = "pool",
  correlation_group = "group"
)

summarizeQCPlot(QC_composite_figure)

savePlot(
  QC_composite_figure@rle_plots$rle_plot_before_ruvIIIc_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "rle_plot_before_ruvIIIc_by_group"
)
savePlot(
  QC_composite_figure@pca_plots$pca_plot_before_ruvIIIc_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "pca_plot_before_ruvIIIc_by_group"
)
savePlot(
  QC_composite_figure@density_plots$density_plot_before_ruvIIIc_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "density_plot_before_ruvIIIc_by_group"
)
savePlot(
  QC_composite_figure@pearson_plots$pearson_correlation_pair_before_ruvIIIc,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "pearson_correlation_pair_before_ruvIIIc"
)
```

## RUVIII-C Canonical Correlation Plot

### Further Reading
#### Remove Unwanted Variation (RUV) Methods
- [RUV in High-Dimensional Data](https://cran.r-project.org/web/packages/ruv/ruv.pdf) - Original paper describing RUV methodology
- [RUV Applications in Proteomics](https://www.sciencedirect.com/science/article/pii/S0303264722000533) - Adaptations of RUV for proteomics data

#### Canonical Correlation Analysis
- [Canonical Correlation in Omics](https://pmc.ncbi.nlm.nih.gov/articles/PMC10237647/) - Applications in multi-omics data integration
- [Multivariate Analysis Methods](https://www.sciencedirect.com/science/article/abs/pii/B9780123944467000194) - Comparison of multivariate techniques
- [Statistical Foundations of CCA](https://online.stat.psu.edu/stat505/book/export/html/682) - Mathematical background of canonical correlation

#### Parameter Selection in RUV
- [Determining Optimal k Parameter](https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb) - Methods for selecting the number of factors to remove
- [Cross-Validation Approaches](https://medium.com/@fraidoonomarzai99/hyperparameters-tunning-and-cross-validation-in-depth-d0918b62d986) - Cross-validation for parameter optimization

This section implements the RUV-III (Remove Unwanted Variation) method with canonical correlation analysis to further normalize your data by removing unwanted technical variation while preserving biological signal. The approach uses proteins least likely to be differentially expressed (identified by ANOVA) as "negative controls" to estimate and remove sources of unwanted variation. 

**NEW: Automated Optimization**: Instead of manually selecting the percentage of proteins to use as negative controls, this workflow now includes an automated optimization function (`findBestNegCtrlPercentage`) that tests different percentages (1% to 20%) and finds the one that gives the best separation between "All" and "Control" groups in the canonical correlation plot. This removes the subjectivity from parameter selection and ensures optimal results for your specific dataset.

The canonical correlation plot helps determine the optimal number of factors (k) to remove - too few factors may leave unwanted variation, while too many might remove biological signal. The plot shows correlations between canonical variables derived from the data and the experimental design, with the inflection point or "elbow" in the plot suggesting the optimal k value. This data-driven approach to selecting both the percentage of negative controls and k ensures that normalization is appropriately tuned to your specific dataset.

```{r 26A Automated Optimization of Negative Control Percentage}
# AUTOMATED APPROACH: Find the optimal percentage of proteins to use as negative controls
# This function tests different percentages and finds the one with best separation quality
# while considering the resulting k value to avoid over-correction
# NEW: Now uses ADAPTIVE penalty by default - automatically adjusts thresholds based on sample size

# Check sample size and adjust percentage range accordingly
sample_size <- ncol(normalised_frozen_protein_matrix_obj@protein_quant_table) - 1
if (sample_size < 15) {
  cat("Small dataset detected (n =", sample_size, "samples). Using extended percentage range for better optimization.\n")
  percentage_range_to_use <- seq(1, 50, by = 2)  # Extended range for small datasets
} else {
  percentage_range_to_use <- seq(1, 20, by = 1)  # Standard range for larger datasets
}

optimization_result <- findBestNegCtrlPercentage(
  normalised_frozen_protein_matrix_obj,
  percentage_range = percentage_range_to_use,  # Adaptive percentage range based on sample size
  separation_metric = "max_difference",         # Use maximum difference as quality metric
  k_penalty_weight = 0.5,                      # Weight for penalizing high k values (0-1)
  # adaptive_k_penalty = TRUE,                 # Default: automatically sets max_acceptable_k based on sample size
  verbose = TRUE                               # Show progress messages
)

# Extract results from optimization
percentage_as_neg_ctrl <- optimization_result$best_percentage
control_genes_index <- optimization_result$best_control_genes_index
cancorplot_r1 <- optimization_result$best_cancor_plot
best_k <- optimization_result$best_k  # No need to call findBestK() again!

# Update config with optimal percentage
config_list$ruvParameters$percentage_as_neg_ctrl <- percentage_as_neg_ctrl

# Display optimization results
cat("Optimization Results:\n")
cat("  Sample size:", optimization_result$sample_size, "samples\n")
cat("  Adaptive max_k threshold:", optimization_result$max_acceptable_k, "\n")
cat("  Best percentage:", percentage_as_neg_ctrl, "%\n")
cat("  Best k value:", best_k, "\n")
cat("  Separation score:", round(optimization_result$best_separation_score, 4), "\n")
cat("  Composite score:", round(optimization_result$best_composite_score, 4), "\n")
cat("  Number of control genes:", sum(control_genes_index, na.rm = TRUE), "\n")

# View all tested percentages and their scores
print(optimization_result$optimization_results)

# Add vertical line for best_k
cancorplot_r1 <- cancorplot_r1 +
  geom_vline(
    xintercept = best_k,
    color = "blue",
    linetype = "dashed",
    size = 1
  ) +
  annotate(
    "text",
    x = best_k + 0.5,
    y = max(layer_scales(cancorplot_r1)$y$range$range),
    label = paste("Best k =", best_k),
    hjust = 0
  ) +
  xlim(1, ncol(normalised_frozen_protein_matrix_obj@protein_quant_table) - 1)

# Save and display the plot
savePlot(cancorplot_r1, project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "canonical_correlation_plot")
cancorplot_r1
```

```{r 26A-Advanced Custom Parameters (Optional)}
# ADVANCED: Custom parameter tuning (only needed for special cases)
# The default adaptive penalty should work well for most datasets

# Example: More conservative approach (smaller datasets or regulatory requirements)
# optimization_result_conservative <- findBestNegCtrlPercentage(
#   normalised_frozen_protein_matrix_obj,
#   k_penalty_weight = 0.7,              # Higher penalty for k values
#   adaptive_k_penalty = FALSE,          # Use fixed penalty if needed
#   max_acceptable_k = 2,                # Very conservative threshold
#   verbose = TRUE
# )

# Example: More permissive approach (large datasets with complex designs)  
# optimization_result_permissive <- findBestNegCtrlPercentage(
#   normalised_frozen_protein_matrix_obj,
#   k_penalty_weight = 0.3,              # Lower penalty for k values
#   percentage_range = seq(1, 30, by = 1), # Wider percentage range
#   verbose = TRUE
# )

# For most users: stick with the default adaptive approach above!
```

```{r 26B Manual Approach (Alternative)}
# MANUAL APPROACH: Choose the % of proteins manually (use this if you want to override the automated approach)
# percentage_as_neg_ctrl <- 5
# config_list$ruvParameters$percentage_as_neg_ctrl <- percentage_as_neg_ctrl
# control_genes_index <- getNegCtrlProtAnova(
#   normalised_frozen_protein_matrix_obj,
#   percentage_as_neg_ctrl = percentage_as_neg_ctrl
# )
# cancorplot_r1 <- ruvCancor(
#   normalised_frozen_protein_matrix_obj,
#   ctrl = control_genes_index,
#   num_components_to_impute = 5,
#   ruv_grouping_variable = "group"
# )
# 
# # Find the best k
# best_k <- findBestK(cancorplot_r1)
# 
# # Add vertical line for best_k
# cancorplot_r1 <- cancorplot_r1 +
#   geom_vline(
#     xintercept = best_k,
#     color = "blue",
#     linetype = "dashed",
#     size = 1
#   ) +
#   annotate(
#     "text",
#     x = best_k + 0.5,
#     y = max(layer_scales(cancorplot_r1)$y$range$range),
#     label = paste("Best k =", best_k),
#     hjust = 0
#   ) +
#   xlim(1, ncol(normalised_frozen_protein_matrix_obj@protein_quant_table) - 1)
# 
# # Save and display the plot
# savePlot(cancorplot_r1, project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "canonical_correlation_plot")
# cancorplot_r1
```

## RUVIII-C Normalisation and QC

### Further Reading
#### RUV Implementation
- [RUV-III Algorithm](https://pmc.ncbi.nlm.nih.gov/articles/PMC9458465/) - Details of the RUV-III algorithm implementation

#### Evaluating Normalization Success
- [QC Metrics for Normalization](https://www.mcponline.org/article/S1535-9476(20)34962-1/fulltext) - Metrics to evaluate normalization success
- [Normalization Benchmarking](https://www.nature.com/articles/s41598-024-57670-2) - Comparing effectiveness of normalization methods

#### Handling Missing Values Post-Normalization
- [Missing Value Patterns After Normalization](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6314938/) - Dealing with missing values introduced by normalization
- [Filtering Strategies After RUV](https://www.bioconductor.org/packages//2.11/bioc/vignettes/limma/inst/doc/usersguide.pdf) - Approaches for post-normalization filtering

This section applies the RUV-III-C normalization using the optimal k value determined from the canonical correlation analysis and then evaluates its effectiveness through quality control visualizations. RUV-III-C removes unwanted sources of variation by estimating factors of unwanted variation from negative control proteins and then regressing these factors out of the data. After normalization, the code filters out proteins that may have excessive missing values introduced by the RUV procedure, ensuring data completeness for downstream analysis. The comprehensive quality control assessment (RLE plots, PCA, density plots, and correlation analysis) allows you to evaluate whether RUV-III-C successfully reduced technical variation while preserving biological differences. Comparing these plots with those from earlier normalization steps helps quantify the improvement in data quality achieved by the multistep normalization strategy.

```{r 27 RUVIII-C Normalisation and QC}
ruv_normalised_results_temp_obj <- ruvIII_C_Varying(
  normalised_frozen_protein_matrix_obj,
  ruv_grouping_variable = "group",
  ruv_number_k = best_k,
  ctrl = control_genes_index
)

config_list <- updateRuvParameters(
  config_list,
  best_k,
  control_genes_index,
  percentage_as_neg_ctrl
)

## Sometimes RUV will blank out some of the values, so we need to remove proteins
## if too many values are blanked out
ruv_normalised_results_cln_obj <- removeRowsWithMissingValuesPercent(
  theObject = ruv_normalised_results_temp_obj
)
ruvfilt_protein_count <- ruv_normalised_results_cln_obj@protein_quant_table |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after RUV normalization and filtering:", ruvfilt_protein_count))

updateProteinFiltering(
  data = ruv_normalised_results_cln_obj@protein_quant_table,
  step_name = "11_RUV_filtered",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)

QC_composite_figure@rle_plots$rle_plot_after_ruvIIIc_group <- plotRle(
  ruv_normalised_results_cln_obj,
  group = "group",
  yaxis_limit = c(-6, 6)
)

QC_composite_figure@pca_plots$pca_plot_after_ruvIIIc_group <- plotPca(
  ruv_normalised_results_cln_obj,
  grouping_variable = "group",
  label_column = "",
  shape_variable = "group",
  title = "",
  font_size = 8
)

QC_composite_figure@density_plots$density_plot_after_ruvIIIc_group <- plotDensity(
  QC_composite_figure@pca_plots$pca_plot_after_ruvIIIc_group,
  grouping_variable = "group"
)

pca_mixomics_after_ruvIIIc <- getPcaMatrix(ruv_normalised_results_cln_obj)

QC_composite_figure@pearson_plots$pearson_correlation_pair_after_ruvIIIc_group <-
  plotPearson(
    ruv_normalised_results_cln_obj,
    tech_rep_remove_regex = "pool",
    correlation_group = "group"
  )

savePlot(
  QC_composite_figure@rle_plots$rle_plot_after_ruvIIIc_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "rle_plot_after_ruvIIIc_by_group"
)
savePlot(
  QC_composite_figure@pca_plots$pca_plot_after_ruvIIIc_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "pca_plot_after_ruvIIIc"
)
savePlot(
  QC_composite_figure@density_plots$density_plot_after_ruvIIIc_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "density_plot_after_ruvIIIc_by_group"
)
savePlot(
  QC_composite_figure@pearson_plots$pearson_correlation_pair_after_ruvIIIc_group,
  project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
  "pearson_correlation_pair_after_ruvIIIc_group"
)

summarizeQCPlot(QC_composite_figure)
```

## Sample Pearson Correlation Filtering

### Further Reading
#### Correlation-Based Sample Filtering
- [Pearson vs Spearman Correlation](https://pmc.ncbi.nlm.nih.gov/articles/PMC7779167/) - Choosing the appropriate correlation measure

#### Replicate Quality Control
- [Replicate Agreement Assessment](https://pubmed.ncbi.nlm.nih.gov/22462118/) - Methods for assessing agreement between replicates
- [Technical vs Biological Variation](https://www.nature.com/documents/Biological_and_technical_replicates_guidelines.pdf) - Distinguishing sources of variation

#### Impact on Statistical Power
- [Sample Filtering Effects on Power](https://academic.oup.com/bjaed/article/16/5/159/2389876) - How filtering affects statistical power
- [Outlier Influence on Differential Expression](https://cardinalscholar.bsu.edu/server/api/core/bitstreams/37a09705-8632-480a-9771-9b4cdafad608/content) - How outliers affect differential expression results

This section filters samples based on their Pearson correlation with other samples in the same experimental group. Low correlation between samples that should be similar (e.g., biological replicates) often indicates technical issues or sample quality problems. By implementing a correlation threshold (here set at 0.5), this step removes samples that don't correlate well with their group, ensuring that only high-quality, consistent samples are used for downstream statistical analysis. The `updateProteinFiltering()` function tracks the effect of this filtering step on the number of proteins in the dataset. The final data is saved in both TSV and RDS formats for transparency and to facilitate downstream analysis. This correlation-based filtering is the final quality control step before differential expression analysis, ensuring that the data used for biological interpretation is of the highest quality.

```{r 28 Sample Pearson Correlation Filtering}
ruv_correlation_vec <- pearsonCorForSamplePairs(
  ruv_normalised_results_cln_obj,
  tech_rep_remove_regex = "pool",
  correlation_group = "group"
)

ruv_normalised_filtered_results_obj <- filterSamplesByProteinCorrelationThreshold(
  ruv_normalised_results_cln_obj,
  pearson_correlation_per_pair = ruv_correlation_vec,
  min_pearson_correlation_threshold = 0.5
)

corfilt_protein_count <- ruv_normalised_filtered_results_obj@protein_quant_table |>
  distinct(Protein.Ids) |>
  nrow()
message(paste("Number of distinct proteins remaining after removing low-correlation samples:", corfilt_protein_count))

updateProteinFiltering(
  data = ruv_normalised_filtered_results_obj@protein_quant_table,
  step_name = "12_correlation_filtered",
  omic_type = omic_type, # e.g., "proteomics"
  experiment_label = experiment_label, # e.g., "workshop_data"
  return_grid = TRUE,
  overwrite = TRUE
)

vroom::vroom_write(
  ruv_normalised_filtered_results_obj@protein_quant_table,
  file.path(
    project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
    "ruv_normalised_results_cln_with_replicates.tsv"
  )
)

saveRDS(
  ruv_normalised_filtered_results_obj,
  file.path(
    project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir,
    "ruv_normalised_results_cln_with_replicates.RDS"
  )
)

saveRDS(
  ruv_normalised_filtered_results_obj, 
  file.path(
    project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, 
    "ruv_normalised_results_cln_with_replicates.RDS" 
  )
)

ruv_normalised_for_de_analysis_obj <- ruv_normalised_filtered_results_obj
```

# Output Files For Audit Trail

### Further Reading

#### Reproducibility Practices
- [Data Provenance Tracking](https://pmc.ncbi.nlm.nih.gov/articles/PMC8056256/) - Methods for tracking data provenance

#### File Format Conversions
- [Data Serialization in R](https://www.r-bloggers.com/2016/12/remember-to-use-the-serializable-formats/) - Understanding serialization formats like RDS
- [Binary vs Text Formats](https://bookdown.org/rdpeng/rprogdatascience/using-textual-and-binary-formats-for-storing-data.html) - Choosing between binary and text formats
- [Long vs Wide Format](https://medium.com/@rgr5882/100-days-of-data-science-day-35-reshaping-data-pivoting-and-melting-db7a0570bb74) - Data reshaping considerations for different analyses

This section creates multiple output files from the final normalized dataset to ensure data accessibility and facilitate different downstream analysis needs. The code transforms the normalized protein data from its internal S4 object structure into standard formats (TSV files) in both original and log2-transformed versions, making it easily accessible for other analysis tools and researchers. The wide-format data tables (proteins as rows, samples as columns) are created for traditional statistical analyses, while design information is separately exported to document the experimental design. Both text-based formats (TSV) and binary formats (RDS) are provided - the former for human readability and software compatibility, the latter for preserving exact R objects with their metadata. These diverse output formats create a comprehensive audit trail that supports research reproducibility and facilitates data sharing.

```{r 29 Output Files For Audit Trail}
ruv_normalised_for_de_analysis <-
  ruv_normalised_for_de_analysis_obj@protein_quant_table |>
  pivot_longer(
    cols = !matches("Protein.Ids"),
    names_to = "replicates",
    values_to = "Log2.Protein.Imputed"
  ) |>
  dplyr::select(Protein.Ids, replicates, Log2.Protein.Imputed) |>
  mutate(Protein.Imputed = 2^Log2.Protein.Imputed) |>
  mutate(Protein.Imputed = ifelse(is.na(Protein.Imputed), NA, Protein.Imputed)) |>
  pivot_wider(
    id_cols = Protein.Ids,
    names_from = replicates,
    values_from = Protein.Imputed
  ) |>
  dplyr::rename(uniprot_acc = "Protein.Ids")

vroom::vroom_write(
  ruv_normalised_for_de_analysis,
  file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "ruv_normalised_results.tsv")
)

vroom::vroom_write(
  ruv_normalised_for_de_analysis |>
    dplyr::mutate(across(!matches("uniprot_acc"), log2)),
  file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "ruv_normalised_results_log.tsv")
)

vroom::vroom_write(
  design_matrix |>
    distinct(replicates, group) |>
    dplyr::rename(Run = replicates),
  file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir, "design_matrix_avrg.tsv")
)

ruv_normalised_for_de_analysis_mat <- ruv_normalised_for_de_analysis |>
  column_to_rownames("uniprot_acc") |>
  as.matrix()
```

## Composite QC Figure Generation

### Further Reading
#### Publication-Ready Visualizations
- [Data Visualization Best Practices](https://www.sciencedirect.com/science/article/pii/S2666389920301896) - Principles for effective scientific visualizations
- [Communicating with Data](https://journals.sagepub.com/doi/10.1177/15291006211051956) - How to communicate findings through visualization
- [Figure Design in Scientific Publications](https://www.nature.com/documents/natrev-artworkguide_PS.pdf) - Guidelines for creating impactful scientific figures

#### Multi-panel Figures
- [Composite Figures in R](https://cran.r-project.org/web/packages/patchwork/vignettes/patchwork.html) - Creating multi-panel figures with patchwork
- [Complex Layouts in ggplot2](https://ggplot2-book.org/arranging-plots.html) - Arranging multiple plots in ggplot2
- [Visual Hierarchy in Scientific Figures](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003833) - Establishing visual hierarchy in complex figures

#### Consistency in Data Presentation
- [Color Use in Scientific Visualization](https://www.molecularecologist.com/2020/04/23/simple-tools-for-mastering-color-in-scientific-figures/) - Appropriate use of color in scientific figures
- [Accessible Scientific Figures](https://www.a11y-collective.com/blog/accessible-charts/) - Creating visualizations accessible to all readers

This section creates a comprehensive multi-panel quality control figure that combines all the QC visualizations generated throughout the workflow into a single publication-ready composite image. This composite figure provides a holistic view of the data quality at different stages of processing, allowing for easy comparison between pre- and post-normalization states. The function organizes PCA plots, density plots, RLE plots, and correlation plots into a structured grid with consistent formatting and appropriate labels (a, b, c, etc.) for easy reference in publications. This single visualization serves as a powerful summary of the entire quality control process, making it easier to present and interpret the extensive QC work performed. The saved high-resolution figure can be directly included in publications or presentations to demonstrate the thoroughness of the data quality assessment.

```{r 30 Composite QC Figure Generation}
pca_ruv_rle_correlation_merged <- createGridQC(
  QC_composite_figure,
  pca_titles = c("a)", "b)", "c)"),
  density_titles = c("d)", "e)", "f)"),
  rle_titles = c("g)", "h)", "i)"),
  pearson_titles = c("j)", "k)", "l)"),
  save_path = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$protein_qc_dir),
  file_name = "composite_QC_figure"
)
pca_ruv_rle_correlation_merged
```

# Add up to date annotation data from Uniprot
## Will take some time to complete if  you are doing this for first time
## Good spot to grab a coffee :)

### Further Reading

#### Protein Functional Annotation
- [Gene Ontology Resources](https://geneontology.org/docs/go-annotations/) - Understanding Gene Ontology annotations
- [Protein Function Prediction](https://academic.oup.com/bib/article/25/4/bbae289/7696515) - Methods for protein function prediction
- [Evidence Codes in Annotation](https://geneontology.org/docs/guide-go-evidence-codes/) - Understanding evidence codes in annotations

#### Bioconductor Tools for Annotation
- [UniProt.ws Package](https://bioconductor.org/packages/release/bioc/html/UniProt.ws.html) - Bioconductor interface to UniProt
- [Annotation Workflows](https://www.bioconductor.org/packages/release/workflows/) - Complete annotation workflows in Bioconductor
- [Programmatic Access to Biological Data](https://pmc.ncbi.nlm.nih.gov/articles/PMC9815577/) - Best practices for accessing biological databases

This section retrieves and integrates up-to-date protein annotation data from UniProt, the leading universal protein resource database. The annotation process uses the UniProt.ws Bioconductor package to query the UniProt database for comprehensive information about each protein identified in your dataset. This information includes metadata like protein existence evidence, annotation score, review status, gene names, protein names, sequence length, cross-references to Ensembl, Gene Ontology (GO) terms, and keywords. By caching the results in an RDS file, the workflow avoids redundant queries in subsequent runs, significantly speeding up the process while still ensuring data freshness. This annotation step is crucial for biological interpretation of your results, as it provides the functional context needed to understand the biological significance of differentially expressed proteins.

```{r 31 Uniprot Data Annotation}
uniprot_dat_cln <- getUniprotAnnotations(
  input_tbl = ruv_normalised_for_de_analysis_obj@protein_quant_table,
  cache_dir = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$uniprot_annotation_dir),
  taxon_id = taxon_id
)
```

## Read in Previously Assigned Contrasts (optional)

### Further Reading
#### Experimental Contrasts
- [Experimental Design for Differential Analysis](https://www.nature.com/articles/s41467-024-47899-w) - Designing experiments for differential analysis
- [Contrast Specification in R](https://www.bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf) - Guide to specifying contrasts in limma
- [Pairwise Comparisons](https://pmc.ncbi.nlm.nih.gov/articles/PMC3047155/) - Strategies for pairwise comparisons in proteomics

#### Contrast Formulation
- [Linear Models for Proteomics](https://pnnl-comp-mass-spec.github.io/proteomics-data-analysis-tutorial/linear-reg.html) - Linear modeling approaches in proteomics
- [Multiple Testing Considerations](https://pmc.ncbi.nlm.nih.gov/articles/PMC5506159/) - Managing multiple comparisons in omics data

This section reads in predefined contrast specifications from a file, allowing for consistent and reproducible differential expression analysis. Contrasts define the comparisons between experimental groups (e.g., treatment vs. control) that will be tested for protein abundance differences. By defining these contrasts in a separate file, the workflow becomes more flexible and allows for complex experimental designs. This approach also makes it easy to update the analysis with new comparisons without changing the main workflow code. The contrast strings follow a specific format that will be interpreted by the differential expression analysis function to create the appropriate statistical tests.

```{r 32 Read in Previously Assigned Contrasts (optional)}
contrasts_tbl <- file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$source_dir, "contrast_strings.tab") |>
  readLines() |>
  {
    \(x) x[!grepl("^contrasts", x)]
  }() |>
  as_tibble() |>
  dplyr::rename(contrasts = value)
```

## DE Analysis Generation

### Further Reading
#### Differential Expression Analysis
- [Limma-Based Analysis](https://www.bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf) - Detailed guide to limma for differential expression

#### Moderated Statistics
- [Moderated t-tests](https://stanford.edu/group/wonglab/doc/NewGeneScore-5.13.07.pdf) - Understanding moderated t-statistics
- [Empirical Bayes Methods](https://pmc.ncbi.nlm.nih.gov/articles/PMC2759080/) - Empirical Bayes approaches in proteomics
- [TREAT Method](https://pmc.ncbi.nlm.nih.gov/articles/PMC2654802/) - Testing fold changes with thresholds

#### Interpreting DE Results
- [Volcano Plot Interpretation](https://pnnl-comp-mass-spec.github.io/proteomics-data-analysis-tutorial/volcano-plots.html) - Understanding volcano plots in proteomics
- [Statistical Power in omics](https://pmc.ncbi.nlm.nih.gov/articles/PMC4287952/) - Understanding statistical power in differential expression

This section performs differential expression analysis to identify proteins with statistically significant abundance differences between experimental conditions. The analysis uses limma, a well-established statistical framework that applies linear models and empirical Bayes methods to improve statistical power - particularly beneficial for proteomics data where sample sizes are often limited. For each contrast defined in the previous step, the code runs a complete differential expression analysis, including moderated t-tests with robust estimation and trend-based variance modeling. The TREAT method can optionally be applied to test for minimum fold-change thresholds in addition to statistical significance. The results are stored in a named list where each element contains the complete differential expression statistics for one contrast, ready for downstream visualization and biological interpretation.

```{r 33 DE Analysis Generation}
# Debug - if regex readin from config doesnt work
# NB here temporarily until config readin logic fixed for this pattern
config_list$deAnalysisParameters$args_group_pattern <- "(\\d+)"

# Create contrast names first
contrast_names <- contrasts_tbl |>
  dplyr::pull(contrasts) |>
  stringr::str_extract("^[^=]+") |> # Extract everything before the = sign
  stringr::str_replace_all("\\.", "_") # Replace dots with underscores

# Run DE analysis and explicitly set names
de_analysis_results_list <- seq_len(nrow(contrasts_tbl)) |>
  purrr::set_names(contrast_names) |> # This ensures the list will be named
  purrr::map(\(contrast_idx) {
    deAnalysisWrapperFunction(
      ruv_normalised_for_de_analysis_obj,
      contrasts_tbl |> dplyr::slice(contrast_idx),
      formula_string = config_list$deAnalysisParameters$formula_string,
      de_q_val_thresh = config_list$deAnalysisParameters$de_q_val_thresh,
      treat_lfc_cutoff = config_list$deAnalysisParameters$treat_lfc_cutoff,
      eBayes_trend = config_list$deAnalysisParameters$eBayes_trend,
      eBayes_robust = config_list$deAnalysisParameters$eBayes_robust,
      args_group_pattern = config_list$deAnalysisParameters$args_group_pattern,
      args_row_id = ruv_normalised_for_de_analysis_obj@protein_id_column
    )
  })

# Verify we have names
print(names(de_analysis_results_list))
```



## Output the results of the DE analysis

### Further Reading
#### Results Visualization
- [Customizing R Graphics](https://www.springer.com/gp/book/9780387981413) - Guide to customizing R graphics for publication

#### Data Export for Publication
- [Report Generation in R](https://bookdown.org/yihui/rmarkdown/) - Comprehensive guide to report generation
- [ProteomeXchange Submission](http://www.proteomexchange.org/submission) - Protocol for submitting data to repositories

#### Error Handling in R
- [Error Handling Best Practices](https://adv-r.hadley.nz/conditions.html) - Advanced R guide to error handling
- [Parallel Processing Errors](https://cran.r-project.org/web/packages/future/vignettes/future-4-issues.html) - Managing errors in parallel processing

This section outputs the differential expression analysis results in various formats for interpretation and publication. For each contrast, the code generates comprehensive result tables and visualizations, including volcano plots that highlight significantly changing proteins. The function takes advantage of the annotations retrieved from UniProt to enrich the results with biological context, including gene names and protein descriptions. Error handling with tryCatch ensures that the workflow continues even if individual contrasts encounter issues. Multiple output files are generated for each contrast, including TSV files for downstream analysis in other tools and publication-quality figures in various formats. These outputs provide both the raw statistical results and their biological interpretation, facilitating the transition from statistical significance to biological insight.

```{r 34 Output DE Analysis Results}
# Modified output approach with error handling
names(de_analysis_results_list) |>
  purrr::walk(\(contrast_name) {
    tryCatch(
      {
        message(paste("Processing contrast:", contrast_name))

        # Check if the result exists and has content
        result <- de_analysis_results_list[[contrast_name]]
        if (is.null(result) || length(result) == 0) {
          message(paste("Skipping empty result for:", contrast_name))
          return(NULL)
        }

        outputDeAnalysisResults(
          result,
          ruv_normalised_for_de_analysis_obj,
          uniprot_dat_cln,
          file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$de_output_dir),
          file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$publication_graphs_dir),
          file_prefix = paste0("de_proteins_", contrast_name),
          plots_format = config_list$deAnalysisParameters$plots_format,
          args_row_id = ruv_normalised_for_de_analysis_obj@protein_id_column,
          de_q_val_thresh = 0.05,
          gene_names_column = "gene_names"
        )
      },
      error = function(e) {
        message(paste("Error processing contrast:", contrast_name))
        message(paste("Error message:", e$message))
      }
    )
  })
```

## Enrichment Analysis
## This section performs enrichment analysis on the DE results
## If your taxon id is on the list supported by gprofiler, that will be used to perform the enrichment analysis
## Otherwise, the enrichment analysis will be performed using the GO annotations provided by UniProt in clusterProfiler

### Further Reading
#### Functional Enrichment Analysis
- [Gene Set Enrichment Methods](https://academic.oup.com/bib/article/22/1/545/5722384?login=true) - Overview of enrichment analysis methods
- [Pathway Analysis in Proteomics](https://www.sciencedirect.com/science/article/pii/S002251931400304X) - Approaches to pathway analysis
- [Multiple Testing in Enrichment](https://link.springer.com/article/10.1186/s13059-019-1716-1) - Controlling false discovery in enrichment

#### Gene Ontology Analysis
- [GO Enrichment Interpretation](https://www.nature.com/articles/nprot.2008.211) - Guide to interpreting GO enrichment
- [Semantic Similarity in GO](https://geneontology.org/docs/go-enrichment-analysis/) - Understanding semantic relationships in GO

#### Visualization of Enrichment Results
- [Enrichment Map Visualization](https://www.nature.com/articles/s41596-018-0103-9) - Creating enrichment map networks
- [GOplot Package](https://wencke.github.io/) - Visualizing functional analysis results

This section performs functional enrichment analysis to identify biological processes, molecular functions, and cellular components overrepresented in your differentially expressed proteins. The workflow is flexible, first attempting to use g:Profiler for enrichment if your organism's taxonomy ID is supported, then falling back to clusterProfiler with GO annotations from UniProt if needed. The analysis is performed separately for upregulated and downregulated protein sets from each contrast, providing a comprehensive view of the biological processes affected in different conditions. The enrichment results include statistical measures like p-values and enrichment ratios, along with visualizations like enrichment maps and GO term networks. This step transforms your list of differential proteins into meaningful biological insights, helping you understand the functional implications of the observed protein abundance changes.

```{r 35 Enrichment Analysis}
# Create S4 object of the DE results for enrichment analysis
de_results_for_enrichment <- createDEResultsForEnrichment(
  contrasts_tbl = contrasts_tbl,
  design_matrix = design_matrix,
  de_output_dir = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$de_output_dir)
)

# Run enrichment analysis
enrichment_results <- processEnrichments(
  de_results_for_enrichment # Your S4 object with DE results
  , taxon_id = taxon_id # your organism
  , up_cutoff = 0 # FC filtering
  , down_cutoff = 0 # FC filtering
  , q_cutoff = 0.05 # FDR threshold
  , pathway_dir = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$pathway_dir) # Output directory
  , go_annotations = uniprot_dat_cln # Annotation data
  , exclude_iea = FALSE,
  protein_id_column = ruv_normalised_for_de_analysis_obj@protein_id_column,
  contrast_names = (names(de_analysis_results_list))
)
```

## Save Workflow Arguments and Study Summary

### Further Reading
#### Workflow Reproducibility
- [Reproducible Workflows in R](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006561) - Best practices for reproducible workflows

#### Configuration Management
- [Parameter Management in R](https://bookdown.org/rdpeng/rprogdatascience/control-structures.html) - Strategies for parameter management
- [Version Control for Data Analysis](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004947) - Using version control with data analysis

This section saves all the parameters, configurations, and settings used throughout the workflow, creating a comprehensive record that enables reproduction of the analysis. The `createWorkflowArgsFromConfig()` function captures all the configuration parameters used in the analysis, including normalization settings, filtering thresholds, and statistical parameters. This approach to parameter tracking is essential for reproducible research, as it allows future users (including your future self) to understand exactly how the analysis was performed and to replicate the results if needed. This record serves as both documentation and a practical tool for reproducing or modifying the analysis in the future.

```{r 36 Save Workflow Arguments and Study SUmmary} 
# Create workflow args with config and git info
workflow_args <- createWorkflowArgsFromConfig(
  workflow_name = experiment_label
  , description = "Full protein analysis workflow with config parameters"
  , source_dir_path = file.path(project_dirs[[paste0(omic_type, "_", experiment_label)]]$source_dir)
) 

# Show the workflow arguments
workflow_args
```

# Copy Files to Publication Directory

### Further Reading
#### Data Organization for Publication
- [FAIR Data Principles](https://www.nature.com/articles/sdata201618) - Findable, Accessible, Interoperable, and Reusable data principles
- [Publication-Ready Outputs](https://www.nature.com/articles/s41592-019-0470-2) - Standards for publication-ready outputs

#### Data Archiving
- [Data Archiving Strategies](https://www.nature.com/articles/s41597-020-0524-5) - Approaches to long-term data archiving

This section copies key result files to a designated publication directory, creating a clean, organized set of outputs that can be easily shared with collaborators or included in publications. By centralizing the most important results and visualizations, this step simplifies the process of finding and using the outputs that matter most for interpretation and communication. The publication directory serves as a curated collection of the analysis results, containing only the final, polished outputs rather than intermediate files or raw data. This organization strategy aligns with best practices for research data management and makes it easier to prepare your results for sharing or publication.

```{r 37 Copy Files to Publication Directory}
copyToResultsSummary(
  omic_type = omic_type, # or "metabolomics", etc.
  experiment_label = experiment_label, # Your specific label
  force = FALSE
)
```

# Copy all study parameters to a Github repo for audit trail

### Further Reading
#### Version Control in Research
- [Git for Data Analysis](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004668) - Using Git for data analysis projects


This final step creates a complete GitHub repository containing all analysis parameters, configuration files, and essential results, establishing a permanent, version-controlled record of your analysis. Using Git for version control provides numerous benefits: it creates an immutable audit trail of your analysis, facilitates collaboration with team members, enables easy tracking of changes over time, and serves as a backup of your work. The repository can be shared with collaborators or cited in publications, allowing others to understand your methods in detail or even reproduce your analysis. This practice aligns with modern standards for computational reproducibility and transparent reporting in scientific research.

```{r 38 Copy Output to Github}
options(
  github_org = "your org",
  github_user_email = "your email",
  github_user_name = "your username"
)

pushProjectToGithubFromDirs(
  project_dirs = project_dirs,
  omic_type = omic_type,
  experiment_label = experiment_label,
  project_id = "your project"
)
```

# Render the report
## This will create a report in the report directory
### The report contains all the information from the workflow including results, parameters, methods and figures
```{r 39 Render Report}
RenderReport(omic_type = omic_type
                         , experiment_label = experiment_label
                         , rmd_filename = "DIANN_report.rmd")
```