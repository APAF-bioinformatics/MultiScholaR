---
title: "DIANN analysis for xyz - report"
author: "Your fancy self"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output:
  word_document:
    fig_caption: true
params:
  omic_type: "proteomics" # Use omic_type instead of suffix
  experiment_label: "DEFAULT_LABEL" # Use experiment_label instead of suffix
mainfont: "Calibri"    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

```{r Project Setup, echo=FALSE, warning=FALSE}
# This chunk now sets up paths using the global project_dirs object
library(tidyverse)
library(purrr)
library(flextable)
library(here)
library(patchwork)
library(viridis)
library(gt)
library(plotly)
library(glue)
library(webshot2)
# Explicitly load key tidyverse sub-packages for knitr environment robustness
library(dplyr)
library(stringr)

# Ensure global project_dirs object exists
if (!exists("project_dirs", envir = .GlobalEnv) || !is.list(project_dirs)) {
  stop("Global object 'project_dirs' not found or is not a list. Run MultiScholaR::setupDirectories() first.")
}
# Construct the key for the current omic type and experiment label
current_omic_key <- paste0(params$omic_type, "_", params$experiment_label)
if (!current_omic_key %in% names(project_dirs)) {
  stop(glue::glue("Key '{current_omic_key}' (from params$omic_type='{params$omic_type}' & params$experiment_label='{params$experiment_label}') not found in global 'project_dirs'. Ensure setupDirectories() was run correctly."))
}
# Get the list of paths for the current context
current_project_paths <- project_dirs[[current_omic_key]]

# Define commonly used paths for this report for convenience
report_source_dir             <- current_project_paths$source_dir
report_results_summary_dir    <- current_project_paths$results_summary_dir
report_qc_figures_dir         <- current_project_paths$qc_figures_dir
report_publication_figures_dir<- current_project_paths$publication_figures_dir
report_publication_tables_dir <- current_project_paths$publication_tables_dir

# Basic validation (more can be added if needed)
if (!dir.exists(report_source_dir)) stop(glue::glue("Report source directory not found: {report_source_dir}"))
if (!dir.exists(report_results_summary_dir)) stop(glue::glue("Report results summary directory not found: {report_results_summary_dir}"))

# Define the safe default operator %||% (needed by restored ExtractStudyParameters)
`%||%` <- function(a, b) if (!is.null(a)) a else b
```

```{r Format Sample Details, echo=FALSE}
# Original function code provided by user - UNCHANGED
FormatSampleDetails <- function(numbers) {
  # Check if numbers object has required fields and set defaults if missing
  if (is.null(numbers$technical_replicates)) numbers$technical_replicates <- 0
  if (is.null(numbers$biological_samples)) numbers$biological_samples <- 0
  if (is.null(numbers$total_samples)) numbers$total_samples <- 0
  if (is.null(numbers$avg_tech_reps_per_sample)) 
    numbers$avg_tech_reps_per_sample <- 1
  
  # Handle different cases for technical replicates
  tech_rep_text <- if(numbers$technical_replicates > 0) {
    glue::glue("{numbers$biological_samples} biological samples with an average of 
    {round(numbers$avg_tech_reps_per_sample, 2)} technical replicates per sample, 
    and {numbers$technical_replicates} additional technical replicate measurements")
  } else {
    glue::glue("{numbers$biological_samples} biological samples with no technical 
    replicates")
  }
  
  glue::glue("
SAMPLE DETAILS  

There are {numbers$total_samples} samples in total which consists of:  
• {tech_rep_text}  
• Sample identification (eg client #, sample name, Salesforce ID etc)
• Sample conditions upon receipt  
• Comments about any noncompliance with sample conditions or suitability for 
  testing including client instruction to proceed/not proceed with work  ",
.trim = FALSE)
}
```

```{r Method Section, echo=FALSE}
# Original function code provided by user - UNCHANGED
FormatMethodDetails <- function(numbers) {
  # Set default values for missing parameters with robust error checking
  if (is.null(numbers$qvalue_threshold)) numbers$qvalue_threshold <- 0.01
  if (is.null(numbers$proteotypic_peptide_only)) 
    numbers$proteotypic_peptide_only <- TRUE
  if (is.null(numbers$proportion_samples_below_cutoff)) 
    numbers$proportion_samples_below_cutoff <- 0.5
  if (is.null(numbers$intensity_cutoff_percentile)) 
    numbers$intensity_cutoff_percentile <- 1
  if (is.null(numbers$min_peptidoforms_per_protein)) 
    numbers$min_peptidoforms_per_protein <- 2
  if (is.null(numbers$min_peptides_per_sample)) 
    numbers$min_peptides_per_sample <- 200
  if (is.null(numbers$missing_values_cutoff)) 
    numbers$missing_values_cutoff <- 33.3
  if (is.null(numbers$normalisation_method)) 
    numbers$normalisation_method <- "cyclicloess"
  if (is.null(numbers$num_neg_ctrl)) numbers$num_neg_ctrl <- 0
  if (is.null(numbers$best_k)) numbers$best_k <- 0
  
  # Safe value conversion for the proportion used in text
  proportion_pct <- tryCatch({
    as.numeric(numbers$proportion_samples_below_cutoff) * 100
  }, error = function(e) {
    50  # Default to 50% if conversion fails
  })
  
  # Safe option for proteotypic peptide text
  proteotypic_text <- tryCatch({
    if(isTRUE(numbers$proteotypic_peptide_only)) {
      'only those that are proteotypic'
    } else {
      'including non-proteotypic peptides'
    }
  }, error = function(e) {
    'only those that are proteotypic'  # Default if error
  })
  
  # Safe numeric conversions
  safe_round <- function(x, digits = 0) {
    if(is.numeric(x)) {
      round(x, digits)
    } else {
      tryCatch({
        round(as.numeric(x), digits)
      }, error = function(e) {
        0  # Default if conversion fails
      })
    }
  }
  
  text <- glue::glue("
METHOD DETAILS  

Data analyses were performed based on SOP xxx.
• SOP number(s) and name(s)  
• Brief summary of method(s):  

Data filtering and normalization
Peptide and protein quantification was performed using a series of filtering and 
aggregation steps to ensure high-confidence identifications and accurate 
quantification. Initially, low-confidence peptide identifications were removed 
using q-value and proteotypic peptide filtering, retaining only peptides with a 
q-value threshold of {numbers$qvalue_threshold} and 
{proteotypic_text}. Precursor ion intensities were then 
aggregated into peptide-level quantification values, combining all charge states 
of the same peptide sequence while keeping different modified peptides 
(peptidoforms) as separate entries.

Intensity threshold filtering was applied to remove peptides where a significant 
proportion of samples fell below a specified log-intensity threshold. By default, 
peptides were excluded if {proportion_pct}% or more 
of the samples were below the lowest {numbers$intensity_cutoff_percentile}st 
percentile of intensity values. Proteins were required to have a minimum of 
{numbers$min_peptidoforms_per_protein} peptides, which could include different 
modifications of the same sequence, to be considered for further analysis.

Samples with fewer than {numbers$min_peptides_per_sample} peptides were removed to 
ensure data quality. Peptides appearing in only one replicate across all groups 
were also excluded to maintain measurement reproducibility. Protein-level 
quantification was performed using the IQ algorithm, implementing the maxLFQ 
algorithm to generate protein abundance values. Peptide intensities were aggregated 
into protein-level quantification without normalization at this stage, resulting in 
a protein quantification matrix with log2-transformed intensity values for each 
protein across all samples.

Using the protein intensity matrix, zero values were replaced with NA denoting 
these are missing values. Proteins with {numbers$missing_values_cutoff} percent or 
more of the samples with missing values or values below the 
{numbers$intensity_cutoff_percentile}st percentile of intensity values were 
discarded from the analysis. The data matrix was then log (base 2) transformed and 
between-sample normalization were performed using the 
'{numbers$normalisation_method}' method from the 'limma' R package 
(Ritchie et al., 2015).

To remove the batch effects from biological data, the remove unwanted variation 
(RUVIII-C) R package (Molania et al., 2019; Poulos et al., 2020) and information on 
sample groupings were used to remove batch effects from the normalized data matrix. 
The method relies on having a set of endogenous negative control proteins, which 
are proteins with little or no changes in protein abundances between different 
samples. For this study, {safe_round(numbers$num_neg_ctrl)} empirical negative control 
proteins were identified from an initial ANOVA test comparing across the technical 
replicate groups. {numbers$best_k} unwanted components were selected for removal.

At each stage of the data normalization, the samples were checked for batch effects 
using principal component analysis (PCA) plot, density boxplots, relative 
log-expression (RLE) plots and the distribution of the Pearson correlation between 
replicate samples within groups.



• Comments on test specific conditions information (e.g. temperature), where 
  relevant  
• Comments on deviations, modifications, additions, or exclusions to the method(s) 
  including the acceptance from the client to proceed with them  
• Critical reagent details where relevant; comments on laboratory materials used 
  past their expiry date including the acceptance from the client to proceed with 
  them  ",
.trim = FALSE)

  return(text)
}
```

```{r Result Section, echo=FALSE}
# Original function code provided by user - UNCHANGED
# Note: This original function has a 'figures_dir' argument that seems unused
# and refers to a global 'results_dir' which is no longer set by the new setup.
# If this function *did* need a path, it would need updating, but based on the
# content, it seems focused on text generation from correlation_data only.
FormatResults <- function(correlation_data, 
                         figures_dir = file.path(results_dir, "protein_qc")) {
  glue::glue("
Quality Control  

Quality Control Analysis  
Figure 1 shows the QC metrics across three stages of data processing:  
• Log2 normalisation (leftmost column, a, d, g, j)  
• Cyclic loess normalisation (centre column, b, e, h, k)  
• RUVIII-C normalisation (rightmost column, c, f, i, l)  

Principal Component Analysis  
The PCA plots (Figures 1a-c) demonstrate the effectiveness of batch effect removal:  
• Initial log2 data showed notable batch effects  
• After cyclic loess normalization, batch effects remained in PC1 but were reduced 
  in PC2  
• Post RUVIII-C and cyclic loess normalization, batches merged effectively, 
  suggesting successful removal of unwanted variations  

Density Boxplots
The density boxplots (Figures 1d-f) show the distribution of the PC1 and PC2 values 
for each group.
• The density boxplots show that the distribution of the PC1 and PC2 values for 
  each group are similar.
• Successive rounds of normalisation have reduced the variability in the PC1 and 
  PC2 values for each group.

Relative Log Expression  
The RLE plots (Figures 1g-i) show progressive improvement in data quality:  
• Smaller interquartile ranges (IQR) after normalization indicate reduced technical 
  variation  
• Final normalized data shows consistent median values near zero, suggesting 
  successful bias removal  

Pearson Correlation
• The Pearson correlation matrix (Figures 1j-l) shows that the correlation between 
  biological samples was high and consistent across all normalisation methods ",
.trim = FALSE)
}
```

```{r Comment Section, echo=FALSE}
# Original function code provided by user - UNCHANGED
FormatComments <- function() {
  glue::glue("
COMMENTS  

Quality Control  
• All samples passed initial QC criteria  
• Normalization successfully reduced technical variation  
• Batch effects were substantially mitigated, though not completely eliminated  

Technical Performance  
• High technical reproducibility achieved for majority of proteins  
• Correlation metrics indicate reliable quantification  
• Sample processing met quality standards  

Limitations  
• Residual batch effects present but within acceptable ranges  
• Technical variation adequately controlled through normalization steps  ",
.trim = FALSE)
}
```

```{r Opinions/Interpretations Section, echo=FALSE}
# Original function code provided by user - UNCHANGED
FormatOpinions <- function(correlation_data) {
  # Default value
  high_corr_proteins <- "N/A"
  
  # Try to calculate if possible
  tryCatch({
    if (!is.null(correlation_data) && !is.null(correlation_data$protein_data)) {
      high_corr_proteins <- sum(correlation_data$protein_data$pearson >= 0.8, 
                               na.rm = TRUE)
    }
  }, error = function(e) {
    # Keep the default value
  })
  
  glue::glue("
OPINIONS AND INTERPRETATION  

Data Quality Assessment  
• The dataset demonstrates robust technical quality  
• Normalization strategy effectively reduced systematic biases  

Recommendations  
1. Proceed with downstream analysis using normalized dataset  

Technical Validation  
• Quality metrics support the reliability of the data  
• Technical reproducibility meets industry standards  
• Dataset is suitable for biological interpretation  ",
.trim = FALSE)
}
```

```{r Differential Expression Section, echo=FALSE}
# Original function code provided by user - UNCHANGED
FormatDEDetails <- function(numbers) {
  # Set default values for missing parameters
  if (is.null(numbers$treat_lfc_cutoff)) numbers$treat_lfc_cutoff <- 0
  if (is.null(numbers$ebayes_trend)) numbers$ebayes_trend <- TRUE
  if (is.null(numbers$ebayes_robust)) numbers$ebayes_robust <- TRUE
  if (is.null(numbers$de_qvalue_threshold)) numbers$de_qvalue_threshold <- 0.05
  if (is.null(numbers$formula_string)) numbers$formula_string <- "~ 0 + group"
  
  # Determine if treat was used with a threshold
  treat_desc <- if(numbers$treat_lfc_cutoff > 0) {
    glue::glue("The treat function was used to find proteins with high confidence 
    log fold-change, with a threshold of {numbers$treat_lfc_cutoff}.")
  } else {
    "The treat function was used to find proteins with high confidence 
    log fold-change, with no threshold set."
  }
  
  # Determine the trend and robust settings
  trend_robust <- c()
  if(numbers$ebayes_trend) trend_robust <- c(trend_robust, "trended")
  if(numbers$ebayes_robust) trend_robust <- c(trend_robust, "robust")
  
  trend_robust_text <- if(length(trend_robust) > 0) {
    paste(paste(trend_robust, collapse = " and "), "analysis were enabled.")
  } else {
    "Standard analysis was used (no trend or robust options)."
  }
  
  glue::glue("
DIFFERENTIAL ABUNDANCE ANALYSIS

Differential abundance analysis of proteins was performed using the adjusted 
abundance matrix for comparing each pair of consensus clusters. The 'limma' R 
package (Ritchie et al., 2015) was used, the linear model for comparing each pair 
of time points was fitted using the formula '{numbers$formula_string}' with the 
'lmFit' function, and the p-values calculated using the empirical Bayes method 
'eBayes' function. {trend_robust_text} The false discovery rate correction was 
applied to the moderated p-values by calculating the q-values (Storey, 2002). 
{treat_desc} Significant differentially expressed proteins were defined as those 
with q-values less than {numbers$de_qvalue_threshold}.

Volcano plots of differentially expressed proteins across all groups are shown 
in Figure 2, with a threshold of q-value < 0.05 and no log fold-change 
threshold. Full details of all proteins are provided in the the Supplementary
results table, DE_proteins_results.xlsx within the Publication_tables folder.",
.trim = FALSE)
}
```

```{r GO Enrichment Section, echo=FALSE}
# Original function code provided by user - UNCHANGED
FormatGOEnrichment <- function(numbers) {
  # Set default values if missing
  if (is.null(numbers$organism_name)) {
    organism_name <- "the species under study"
  } else {
    organism_name <- numbers$organism_name
  }
  
  if (is.null(numbers$taxon_id)) {
    taxon_info <- ""
    taxon_id <- NULL
  } else {
    taxon_info <- glue::glue(" (NCBI Taxonomy ID: {numbers$taxon_id})")
    taxon_id <- numbers$taxon_id
  }
  
  # Construct the background info based on available organism data
  background_info <- if (!is.null(numbers$organism_name) || !is.null(numbers$taxon_id)) {
    glue::glue("• The background protein set consisted of all identified proteins from 
  {organism_name}{taxon_info} that passed the quality control filtering steps")
  } else {
    "• The background protein set consisted of all identified proteins in the dataset 
  that passed the quality control filtering steps"
  }
  
  # List of taxon IDs supported by gprofiler2
  # Common model organisms
  gprofiler2_supported_taxa <- c(
    "9606",  # Homo sapiens
    "10090", # Mus musculus
    "10116", # Rattus norvegicus
    "7955",  # Danio rerio (zebrafish)
    "7227",  # Drosophila melanogaster
    "6239",  # Caenorhabditis elegans
    "4932",  # Saccharomyces cerevisiae
    "511145", # Escherichia coli
    "3702",  # Arabidopsis thaliana
    "9031",  # Gallus gallus (chicken)
    "9615",  # Canis lupus familiaris (dog)
    "9913",  # Bos taurus (cattle)
    "9823",  # Sus scrofa (pig)
    "9544",  # Macaca mulatta (rhesus macaque)
    "9986"   # Oryctolagus cuniculus (rabbit)
  )
  
  # Determine which tool to use based on taxon ID
  if (!is.null(taxon_id) && taxon_id %in% gprofiler2_supported_taxa) {
    # Use gprofiler2 description
    enrichment_tool_text <- glue::glue("The enrichment analysis was performed using 
the gprofiler2 R package (Kolberg et al., 2023) via the gost function with default 
parameters. For each GO category (Biological Process, Molecular Function, and 
Cellular Component), terms were considered significantly enriched if they had an 
adjusted p-value less than 0.05 after multiple testing correction using the g:SCS 
method, which is specifically tailored for the hierarchical structure of functional 
annotations. The results were visualized using standard gprofiler2 plots to 
highlight the relationships between enriched terms.")
  } else {
    # Use clusterProfiler description (original)
    enrichment_tool_text <- glue::glue("The enrichment analysis was performed using 
the clusterProfiler R package (Yu et al., 2012). For each GO category (Biological 
Process, Molecular Function, and Cellular Component), terms were considered 
significantly enriched if they had an adjusted p-value less than 0.05 after 
multiple testing correction using the Benjamini-Hochberg method. The results were 
visualized using dot plots and enrichment maps to highlight the relationships 
between enriched terms.")
  }
  
  glue::glue("
GO ENRICHMENT ANALYSIS

Gene Ontology (GO) enrichment analysis was performed on the differentially 
expressed proteins to identify significantly enriched biological processes, 
molecular functions, and cellular components. The analysis was conducted using the 
following parameters:
• No fold change cutoff was applied to filter differentially expressed proteins
• An adjusted FDR cutoff of 0.05 was used to identify significantly enriched GO 
  terms
{background_info}

{enrichment_tool_text}

Key findings from the GO enrichment analysis are summarized below for each 
comparison:

Biological Process:
• Upregulated processes included cellular stress response, metabolic regulation, 
  and protein folding pathways
• Downregulated processes were primarily related to cell adhesion, cytoskeletal 
  organization, and vesicular transport

Molecular Function:
• Proteins with binding functions (particularly nucleotide and protein binding) 
  were significantly enriched
• Catalytic activities, especially those involved in redox reactions, showed 
  significant enrichment patterns

Cellular Component:
• Significant enrichment was observed for proteins localized to membrane-bound 
  organelles, particularly mitochondria and endoplasmic reticulum
• Extracellular components showed differential regulation across conditions

The complete list of enriched GO terms with statistics is provided in the 
supplementary file 'Pathway_enrichment_results.xlsx' within the Publication_tables 
folder.",
.trim = FALSE)
}
```

```{r Sample Analysis Functions, echo=FALSE}
# Original function code provided by user - UNCHANGED
# This chunk uses the native pipe |> as provided in the original code
CalculateSampleCounts <- function(group_summary) {
  total_measurements <- sum(group_summary$total_measurements)
  biological_samples <- sum(group_summary$n_samples)
  technical_replicates <- max(0, total_measurements - biological_samples)
  list( total_samples = total_measurements, biological_samples = biological_samples, technical_replicates = technical_replicates )
}

CalculatePatientMetrics <- function(numbers, group_summary) {
  numbers$avg_tech_reps_per_sample <- numbers$total_samples / numbers$biological_samples
  numbers$samples_per_group <- numbers$biological_samples / n_distinct(group_summary$group)
  numbers
}

AnalyzeGroups <- function(design_df) {
  unique(design_df$group) |>
    purrr::map_dfr(function(g) {
      group_data <- design_df |> 
        filter(group == g)
      unique_samples <- n_distinct(group_data$Run)
      total_rows <- nrow(group_data)
      avg_reps <- if(unique_samples == total_rows) 1 else total_rows / unique_samples
      tibble( group = g, n_samples = unique_samples, total_measurements = total_rows, avg_replicates = avg_reps )
    })
}

ExtractStudyParameters <- function(study_params) {
  basic_params <- list(
    missing_values_cutoff = as.numeric(study_params$removeRowsWithMissingValuesPercent$`Groupwise Percentage Cutoff`),
    intensity_cutoff_percentile = as.numeric(study_params$peptideIntensityFiltering$`Peptides Intensity Cutoff Percentile`),
    normalisation_method = study_params$normaliseBetweenSamples$Method,
    num_neg_ctrl = as.numeric(study_params$ruvParameters$`Num Neg Ctrl`) * (as.numeric(study_params$ruvParameters$`Percentage as Neg Ctrl`)/100),
    best_k = as.numeric(study_params$ruvParameters$`Best k`)
  )
  organism_params <- list(organism_name = NA, taxon_id = NA)
  if ("Organism Information" %in% names(study_params)) {
    if ("Organism Name" %in% names(study_params$`Organism Information`)) { organism_params$organism_name <- study_params$`Organism Information`$`Organism Name` }
    if ("Taxon ID" %in% names(study_params$`Organism Information`)) { organism_params$taxon_id <- study_params$`Organism Information`$`Taxon ID` }
  }
  peptide_filtering_params <- list(
    qvalue_threshold = as.numeric(study_params$srlQvalueProteotypicPeptideClean$`Qvalue Threshold`),
    global_qvalue_threshold = as.numeric(study_params$srlQvalueProteotypicPeptideClean$`Global Qvalue Threshold`),
    proteotypic_peptide_only = as.logical(as.numeric(study_params$srlQvalueProteotypicPeptideClean$`Choose Only Proteotypic Peptide`)),
    intensity_column = study_params$srlQvalueProteotypicPeptideClean$`Input Matrix Column Ids`
  )
  intensity_filtering_params <- list(
    intensity_cutoff_percentile = as.numeric(study_params$peptideIntensityFiltering$`Peptides Intensity Cutoff Percentile`),
    proportion_samples_below_cutoff = as.numeric(study_params$peptideIntensityFiltering$`Peptides Proportion of Samples Below Cutoff`)
  )
  protein_peptide_params <- list(
    min_peptides_per_protein = as.numeric(study_params$filterMinNumPeptidesPerProtein$`Peptides per Protein Cutoff`),
    min_peptidoforms_per_protein = as.numeric(study_params$filterMinNumPeptidesPerProtein$`Peptidoforms per Protein Cutoff`)
  )
  sample_filtering_params <- list( min_peptides_per_sample = as.numeric(study_params$filterMinNumPeptidesPerSample$`Peptides per Sample Cutoff`) )
  missing_value_params <- list(
    imputed_value_column = study_params$peptideMissingValueImputation$`Imputed Value Column`,
    max_proportion_missing = as.numeric(study_params$peptideMissingValueImputation$`Proportion Missing Values`)
  )
  protein_missing_params <- list(
    groupwise_missing_cutoff = as.numeric(study_params$removeRowsWithMissingValuesPercent$`Groupwise Percentage Cutoff`),
    max_groups_missing_cutoff = as.numeric(study_params$removeRowsWithMissingValuesPercent$`Max Groups Percentage Cutoff`),
    proteins_intensity_cutoff = as.numeric(study_params$removeRowsWithMissingValuesPercent$`Proteins Intensity Cutoff Percentile`)
  )
  ruv_params <- list(
    ruv_grouping_variable = study_params$ruvIII_C_Varying$`Ruv Grouping Variable`,
    ruv_number_k = as.numeric(study_params$ruvIII_C_Varying$`Ruv Number k`),
    percentage_as_neg_ctrl = as.numeric(study_params$ruvParameters$`Percentage as Neg Ctrl`),
    best_k = as.numeric(study_params$ruvParameters$`Best k`),
    num_neg_ctrl = as.numeric(study_params$ruvParameters$`Num Neg Ctrl`)
  )
  de_params <- list(
    formula_string = study_params$deAnalysisParameters$`Formula String`,
    treat_lfc_cutoff = as.numeric(study_params$deAnalysisParameters$`Treat Lfc Cutoff`),
    ebayes_trend = as.logical(as.character(study_params$deAnalysisParameters$`eBayes Trend`)),
    ebayes_robust = as.logical(as.character(study_params$deAnalysisParameters$`eBayes Robust`)),
    de_qvalue_threshold = as.numeric(study_params$deAnalysisParameters$`De q Val Thresh`),
    lfc_cutoff = as.logical(as.character(study_params$deAnalysisParameters$`Lfc Cutoff`))
  )
  go_params <- list( use_fold_change_cutoff = FALSE, fdr_cutoff = 0.05 )
  all_params <- c(basic_params, organism_params, peptide_filtering_params, intensity_filtering_params, protein_peptide_params, sample_filtering_params, missing_value_params, protein_missing_params, ruv_params, de_params, go_params)
  return(all_params)
}

AnalyzeSamples <- function(design_matrix, study_params) {
  group_summary <- AnalyzeGroups(design_matrix)
  numbers <- CalculateSampleCounts(group_summary)
  numbers <- CalculatePatientMetrics(numbers, group_summary)
  params_extracted <- ExtractStudyParameters(study_params) # Use renamed variable
  numbers <- c(numbers, params_extracted) # Combine base counts/metrics with extracted params
  list( numbers = numbers, table = group_summary )
}
```

```{r Main Execution Function, echo=FALSE, warning=FALSE}
# This chunk reads inputs using the NEW directory variables, but calls the ORIGINAL helper functions

# ReadStudyParams definition (original)
ReadStudyParams <- function(filepath) {
  params_text <- readLines(filepath)
  params <- list()
  current_section <- NULL
  purrr::walk(params_text, function(line) {
    if(str_detect(line, ":$")) {
      current_section <<- str_trim(str_remove(line, ":$"))
      params[[current_section]] <<- list()
    } else if(str_detect(line, ":") && !is.null(current_section)) {
      kv <- str_split(line, ":", n = 2)[[1]]
      params[[current_section]][[str_trim(kv[1])]] <<- str_trim(kv[2])
    }
  })
  return(params)
}

# Main function definition (original internal logic, adapted pathing)
Main <- function() { # Removed suffix param
  # Paths are now globally available from the 'Project Setup' chunk
  
  # Read inputs with error checking using NEW path variables
  params_path <- file.path(report_source_dir, "study_parameters.txt")
  if (!file.exists(params_path)) stop(glue::glue("Study parameters file not found: {params_path}"))
  study_params_data <- tryCatch({ ReadStudyParams(params_path) }, error = function(e) { stop(glue::glue("Error reading study parameters: {e$message}")) })

  matrix_path <- file.path(report_source_dir, "design_matrix.tab")
  if (!file.exists(matrix_path)) stop(glue::glue("Design matrix file not found: {matrix_path}"))
  design_matrix_data <- tryCatch({ readr::read_tsv(matrix_path, show_col_types = FALSE) }, error = function(e) { stop(glue::glue("Error reading design matrix: {e$message}")) })
  
  # Use NEW path variable for correlation data
  corr_path <- file.path(report_publication_tables_dir, "ruv_normalised_results.RDS")
  correlation_data <- if (file.exists(corr_path)) {
    tryCatch({ readRDS(corr_path) }, error = function(e) { warning(glue::glue("Error reading RDS: {e$message}")); list() })
  } else { warning(glue::glue("File not found: {corr_path}")); list() }
  
  # Run analysis using ORIGINAL AnalyzeSamples
  analysis_results <- tryCatch({ AnalyzeSamples(design_matrix_data, study_params_data) }, error = function(e) { stop(glue::glue("Error in AnalyzeSamples: {e$message}")) })
  
  # Generate text sections using ORIGINAL Format* functions
  # Ensure all necessary 'numbers' fields are produced by the ORIGINAL AnalyzeSamples/ExtractStudyParameters
  report_content <- list(
    content = list(
      sample_details = FormatSampleDetails(analysis_results$numbers),
      method_details = FormatMethodDetails(analysis_results$numbers),
      results        = FormatResults(correlation_data), # Pass correlation data
      comments       = FormatComments(),
      opinions       = FormatOpinions(correlation_data), # Pass correlation data
      de_details     = FormatDEDetails(analysis_results$numbers),
      go_enrichment  = FormatGOEnrichment(analysis_results$numbers)
    ),
    table = analysis_results$table,
    numbers = analysis_results$numbers
  )
  return(report_content)
}

# Execute Main to get report data
results_data <- tryCatch({ Main() }, error = function(e) {
  list(content = list(sample_details = paste("Error running Main function:", e$message), method_details = "", results = "", comments = "", opinions = "", de_details = "", go_enrichment = ""), table = data.frame(), numbers = list())
})
```

# Sample Details

`r results_data$content$sample_details`

# Method Details

`r results_data$content$method_details`

# Results

`r results_data$content$results`

```{r QC_metrics, echo=FALSE, warning=FALSE, fig.cap="Figure 1: QC metrics across normalisation stages"}
# Use NEW path variable
composite_qc_fig_path <- file.path(report_qc_figures_dir, paste0(params$omic_type, "_composite_QC_figure.png"))
if(file.exists(composite_qc_fig_path)) {
  knitr::include_graphics(composite_qc_fig_path)
} else {
  cat(glue::glue("Warning: Composite QC figure not found at {composite_qc_fig_path}\\n")) # Added newline
}
```

# Differential Expression Analysis

`r results_data$content$de_details`

```{r grid_layout, echo=FALSE, out.width="49%", out.height="49%", fig.align='center', fig.ncol=2, fig.show='hold'}
# Use NEW path variable and grep method
volcano_plots_dir <- file.path(report_publication_figures_dir, "Volcano_Plots")
if(dir.exists(volcano_plots_dir)) {
  all_volcano_files <- list.files(volcano_plots_dir, full.names = TRUE, recursive = FALSE)
  volcano_plot_files <- sort(grep(pattern = "[.](png|jpg|jpeg)$", x = all_volcano_files, ignore.case = TRUE, value = TRUE))
  if(length(volcano_plot_files) > 0) {
    knitr::include_graphics(volcano_plot_files)
  } else {
    cat(glue::glue("Warning: No Volcano Plot images (.png, .jpg, .jpeg) found in {volcano_plots_dir} (after grep filtering). Raw count: {length(all_volcano_files)}\\n"))
  }
} else {
  cat(glue::glue("Warning: Volcano Plots directory not found at {volcano_plots_dir}\\n"))
}
```

```{r Volcano Plot Caption, echo=FALSE, results='asis'}
# Original code
cat("*Figure 2: Volcano plots of differentially expressed proteins in alphabetical order.*\n\n")
```

# Comments

`r results_data$content$comments`

# Opinions and Interpretation

`r results_data$content$opinions`

# GO Enrichment Analysis

`r results_data$content$go_enrichment`

```{r GO Enrichment Plots, echo=FALSE, out.width="49%", out.height="49%", fig.align='center', fig.ncol=2, fig.show='hold'}
# Use NEW path variable and grep method
enrichment_plots_dir <- file.path(report_publication_figures_dir, "Enrichment_Plots")
if(dir.exists(enrichment_plots_dir)) {
  all_enrichment_files <- list.files(enrichment_plots_dir, full.names = TRUE, recursive = FALSE)
  enrichment_plot_files <- sort(grep(pattern = "[.](png|jpg|jpeg)$", x = all_enrichment_files, ignore.case = TRUE, value = TRUE))
  if(length(enrichment_plot_files) > 0) {
    knitr::include_graphics(enrichment_plot_files)
  } else {
     cat(glue::glue("Warning: No Enrichment Plot images (.png, .jpg, .jpeg) found in {enrichment_plots_dir} (after grep filtering). Raw count: {length(all_enrichment_files)}\\n"))
  }
} else {
  cat(glue::glue("Warning: Enrichment Plots directory not found at {enrichment_plots_dir}\\n"))
}
```

```{r Go Enrichment Caption, echo=FALSE, results='asis'}
# Original code
cat("*Figure 3: GO enrichment analysis showing pathways enriched in differentially expressed proteins. Plots are ordered alphabetically by comparison and direction.*\n\n")
```

```